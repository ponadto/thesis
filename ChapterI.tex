% Co musi byc w Intro:
% 1. Paragraf o MD + losowosc + collective variable.
% 2. Ze wynik eksperymentu jest próbą statystyczną z pewnego rozkladu.
% 3.  Machine learning / Statistical learning method.
%  -> unsuppervised
%  -> suppervised
%  Ze unsuppervised==clustering==finding patterns in seemingly chaotic
% 5. Jakis ustep (ktory wychodzi z klasteringu) n.t. dynamicznych domen.

% Co (byc moze) powinno byc:
% 1. Metody optymalizacji poprzez alg. genetyczne.
% 2. Energia swobodna.

\chapter{Introduction}

The primary aim of this dissertation was the extraction of palpable observations from complex data describing (bio)molecular systems.
In Chapter II we present our novel method of discovering quasi-rigid parts in proteins, which relies on experimental data, or from a molecular dynamics trajectory.
Chapter III presents a very different problem of identifying parts of a molecular system that propel (or hinder) a given structural transition.
In both these cases we applied unsuppervised machine learning techniques (also known as \emph{clustering}), which is a methodology commonly used in finding patterns in seemingly chaotic data.

In this introductory chapter we give an overview of the molecular dynamics simulation scheme and the machine learning approach.
The discussion here is broad, and was intended to motivate the application of clustering techniques described in the remainder of the dissertation. % TODO: inaczej
We go into more details concerning particular molecular dynamics and machine learning algorithms in Chapters II and III. % TODO: inaczej


\section{Molecular dynamics}

Molecular dynamics (MD) simulations are a wide range of numerical methods designed for the study of molecular systems.
Typically, the MD approach assumes a classical potential energy function, $U$, also known as the \emph{force field}, which treats atoms as electrostatically charged points, and chemical bonds as springs and hinges.
The arguments of the function $U$ are configurations $\mathbf{q}$ of the system at hand, and the value $U(\mathbf{q})$ is a real number expressing the potential energy of the system at $\mathbf{q}$.
If we denote the set of all configurations of a given system by $\Omega$, then $U\colon\Omega\to\mathbb{R}$. % TODO: zobacz, czy to nie jest zbedne
The force acting on atom $i$ is then derived from $U$ by taking a gradient with respect to position $\mathbf{q}_i$ of that atom:
$$
\mathbf{F}_i(\mathbf{q}) = -\nabla_i U(\mathbf{q}),
$$
which requires that $U$ is differentiable. 

More accurate algorithms take into account quantum effects, making the simulation more reliable, but also immensely expensive in terms of computational time.
Although modelling intricate properties of molecular systems using a classical potential may seem an over-simplification, MD has achieved considerable success over the recent years, and is constantly improving.

Classical MD simulations are popular mainly because of their significantly lower computational cost.
This allows for estimation of free energy profiles of structural transitions, even for large, biologically relevant systems (such as enzymes, DNA, membrane systems, and other biomolecules).
Free energy translates into experimentally observable properties, for example, the affinity of a ligand to an enzyme, or the chance of transfering an ion through a membrane channel.

Feedback from experiments contributes to more accurate classical potential energy functions, thus improving MD simulations' reliability.
However, there is a separate aspect to MD methodology which greatly benefits from information gained from the experiment.
To run an MD simulations, one needs to choose the right force field, but also~--~and perhaps more importantly~--~the appropriate \emph{sampling technique}.
The underlying, well-established premise is that molecular systems in thermal equilibrium experience random impulses of momenta, which can be though of as a model of collisions with molecules of the outside world.
In other words, the total energy of a system in thermal equilibrium is not conserved, but rather fluctuates around a certain mean value.

Consequently, configurations of a system are characterized by a probability density, which in the case of thermal equilibrium at $T=const$ is the well-known \emph{Boltzmann distribution}:
\begin{equation}
 \rho_B(\mathbf{q})=Z^{-1} \exp[-U(\mathbf{q})/k_BT], \quad  Z=\int_\Omega \exp[-U(\mathbf{q})/k_BT] d\mathbf{q}
\end{equation}
where $k_B$ is the Boltzmann constant, and $Z$ is a normalizing factor.
From this probabilistic perspective we can view an MD simulation as a sampling procedure, during which we acquire configurations according to the Boltzmann distribution.
Macroscopic properties of the system (heat capacity, for example) are defined as expected values of appropriate microscopic quantities, or the probability density itself, as in the case of entropy, or the free energy.

\subsection{Sampling techniques and the Helmholtz free energy}

Underlying the simplistic, classical assumptions embedded in MD simulations are complex sampling techniques, used for estimating macroscopic properties of microscopic systems.

MD simulations can mimic time evolution of a system with the use of integration schemes such as the Langevin dynamics, which numerically realizes the Langevin equation:
\begin{equation}
 \mathbb{M}\ddot{\mathbf{q}} = -\nabla U(\mathbf{q}) -\gamma \mathbb{M}\dot{\mathbf{q}} + \sqrt{2\gamma k_BT\mathbb{M}}\hspace{0.1cm	}\mathbf{R}(t)
\end{equation}

where $\mathbb{M}$ is a diagonal matrix of atom masses, and $\mathbf{R}(t)$ is a stationary Gaussian process with zero mean, and auto-correlation expressed by the delta function: $\langle \mathbf{R}(t)\mathbf{R}(t') \rangle = \delta(t-t')$. 

The second point is especially important, because all algorithms designed for sampling the configurational space in the classical MD set-up are naturally transferable to quantum-based methods.

\subsubsection{Potential of Mean Force}



\section{Unsupervised machine learning}

Data mining / machine learning ... (powinienes jakos zgrabnie opisac machine learning)

Unsupervised machine learning is a class of methods for extracting information from unstructured, multi-dimensional data.
Roughly, these methods amount to reducing the dimensionality of the data.
Stated differently, unsupervised machine learning tries to address the following questions\footnote{
These are not the only questions taken up by unsupervised machine learning.
For example, one of the more interesting, and fast-changing fields is the problem of \emph{community detection}.}:
\begin{itemize}
 \item Is there a clear way of visualizing the data?
 \item Can we partition the set into cohesive subgroups, distinctive from each other?
\end{itemize}
The collective variable discussed above was a method of dimensionality reduction, but one based on a probabilistic model derived from physical postulates.

Data produced by MD simulations of biomolecules are multi-dimensional trajectories, points $\{ \mathbf{q}_t \}_{t=1}^N$, where $\mathbf{q}_t\in\mathbb{R}^{3n}$ ($N$ is the number of steps of the simulation, and $n$~--~the number of atoms of the system).
Depending on the sampling technique {\bf adapted} in the MD simulation, the $t$ parameter might by independent of time, as for example in the Monte Carlo sampling.
Also, configurations acquired from NMR experiments are a multitude of 3-dimensional points.
Analogously to MD, configurations acquired from NMR experiments can be thought of as a set of multi-dimensional elements, $\mathbf{q}_t\in\mathbb{R}^{3n}$, in which the order imposed by the $t$ parameter has very little to do with time.

Regardless of the source, we are faced with the problem of purifying the information hidden within our data.
Unsupervised machine learning offers us two methods of simplifying the data: \emph{principal component analysis} (for visualization and pre-processing) and \emph{clustering} (for identifying distinctive subgroups of points).

\subsection{Graph representation}

Tutaj powinna pojawic sie definicja grafow i, byle predko, przedstawiona reprezentacja macierzowa.
Musisz tez zaakcentowac, ze wybor algorytmu klastrujacego to jedno, a wlasciwa miara podobienstwa nodow - to drugie.
The choice of a 

\subsection{Dynamic domains as clusters}

Small, globular proteins are often thought of as fairly rigid biomolecules.
However, the set of low-energy, accessible configurations grows as number of atoms in a system increases.
Consequently, large molecular systems can undergo significant structural changes, involving coordinated, multi-step transitions.
Due to {\bf nieogarnialnej} complexity of these systems, such phenomena are difficult to comprehend, {\bf let alone}/{\bf a co dopiero} to interpret.

We attempted a method of simplifying these {\bf zlozone} transitions by means of a clustering scheme specifically adapted for this purpose.


\subsection{Molecular cogs as clusters}
