\documentclass[a4paper,11pt,twoside]{book}%{mwbk}
\usepackage[toc,page]{appendix}

\usepackage[inner=4.2cm,outer=3.0cm,top=1.5in,bottom=1.75in]{geometry}
%\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{amsmath,amssymb} %do wzorow matematycznych
\usepackage{amsfonts} %---------||--------------
\usepackage{eufrak} % TODO: co to?


\usepackage{multirow}
\usepackage{tikz}
\usepackage{subfigure}
\usepackage{gensymb}
\usepackage{rotating}

\usepackage{float}

\usepackage{color}% Include colors for document elements
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math

% \usepackage[style=numeric,bibencoding=UTF-8]{biblatex}
% \addbibresource{references.bib}

\graphicspath{{imagesForSM/}}
\usepackage[ruled,vlined]{algorithm2e}


\usepackage{filecontents}
\begin{filecontents}{references.bib}
@article{bahar1997direct,
  title={Direct evaluation of thermal fluctuations in proteins using a single-parameter harmonic potential},
  author={Bahar, Ivet and Atilgan, Ali Rana and Erman, Burak},
  journal={Folding and Design},
  volume={2},
  number={3},
  pages={173--181},
  year={1997},
  publisher={Elsevier}
}

@article{bernhard2010optimal,
  title={Optimal identification of semi-rigid domains in macromolecules from molecular dynamics simulation},
  author={Bernhard, Stefan and Noe, Frank},
  journal={PloS one},
  volume={5},
  number={5},
  pages={e10491},
  year={2010},
  publisher={Public Library of Science}
}

@article{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1603.02754},
  year={2016}
}

@article{dziubinski2015toward,
  title={Toward the identification of molecular cogs},
  author={Dziubi{\'n}ski, Maciej and Lesyng, Bogdan},
  journal={Journal of computational chemistry},
  year={2015},
  publisher={Wiley Online Library}
}

@article{dziubinski2016resicon,
  title={ResiCon: a method for the identification of dynamic domains, hinges and interfacial regions in proteins},
  author={Dziubi{\'n}ski, Maciej and Daniluk, Pawe{\l} and Lesyng, Bogdan},
  journal={Bioinformatics},
  volume={32},
  number={1},
  pages={25--34},
  year={2016},
  publisher={Oxford Univ Press}
}

@inproceedings{ester1996density,
  title={A density-based algorithm for discovering clusters in large spatial databases with noise.},
  author={Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei and others},
  booktitle={Kdd},
  volume={96},
  number={34},
  pages={226--231},
  year={1996}
}

@techreport{fix1951discriminatory,
  title={Discriminatory analysis-nonparametric discrimination: consistency properties},
  author={Fix, Evelyn and Hodges Jr, Joseph L},
  year={1951},
  institution={DTIC Document}
}

@book{gorban2008principal,
  title={Principal manifolds for data visualization and dimension reduction},
  author={Gorban, Alexander N and K{\'e}gl, Bal{\'a}zs and Wunsch, Donald C and Zinovyev, Andrei Y and others},
  volume={58},
  year={2008},
  publisher={Springer}
}


@article{maaten2008visualizing,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

@inproceedings{macqueen1967some,
  title={Some methods for classification and analysis of multivariate observations},
  author={MacQueen, James and others},
  booktitle={Proceedings of the fifth Berkeley symposium on mathematical statistics and probability},
  volume={1},
  number={14},
  pages={281--297},
  year={1967},
  organization={Oakland, CA, USA.}
}

@article{sarich2010approximation,
  title={On the approximation quality of Markov state models},
  author={Sarich, Marco and No{\'e}, Frank and Sch{\"u}tte, Christof},
  journal={Multiscale Modeling \& Simulation},
  volume={8},
  number={4},
  pages={1154--1177},
  year={2010},
  publisher={SIAM}
}

@inproceedings{gionis1999similarity,
  title={Similarity search in high dimensions via hashing},
  author={Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev and others},
  booktitle={VLDB},
  volume={99},
  number={6},
  pages={518--529},
  year={1999}
}

@article{nair2015massively,
  title={Massively parallel methods for deep reinforcement learning},
  author={Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and De Maria, Alessandro and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and others},
  journal={arXiv preprint arXiv:1507.04296},
  year={2015}
}

@book{efron1992bootstrap,
  title={Bootstrap methods: another look at the jackknife},
  author={Efron, Bradley},
  year={1992},
  publisher={Springer}
}

@article{bork1991shuffled,
  title={Shuffled domains in extracellular proteins},
  author={Bork, Peer},
  journal={FEBS letters},
  volume={286},
  number={1},
  pages={47--54},
  year={1991},
  publisher={Elsevier}
}

@article{bu2011proteins,
  title={Proteins move! Protein dynamics and long-range allostery in cell signaling},
  author={Bu, ZIMEI and Callaway, DJ},
  journal={Adv Protein Chem Struct Biol},
  volume={83},
  pages={163--221},
  year={2011}
}

@article{daniluk2011novel,
  title={A novel method to compare protein structures using local descriptors},
  author={Daniluk, Pawel and Lesyng, Bogdan},
  journal={BMC bioinformatics},
  volume={12},
  number={1},
  pages={344},
  year={2011},
  publisher={BioMed Central Ltd}
}


@incollection{daniluk2014theoretical,
  title={Theoretical and Computational Aspects of Protein Structural Alignment},
  author={Daniluk, Pawel and Lesyng, Bogdan},
  booktitle={Computational Methods to Study the Structure and Dynamics of Biomolecules and Biomolecular Processes},
  pages={557--598},
  year={2014},
  publisher={Springer}
}

@article{farago2010activation,
  title={Activation of nanoscale allosteric protein domain motion revealed by neutron spin echo spectroscopy},
  author={Farago, Bela and Li, Jianquan and Cornilescu, Gabriel and Callaway, David JE and Bu, Zimei},
  journal={Biophysical journal},
  volume={99},
  number={10},
  pages={3473--3482},
  year={2010},
  publisher={Elsevier}
}

@article{freedberg2002rapid,
  title={Rapid structural fluctuations of the free HIV protease flaps in solution: relationship to crystal structures and comparison with predictions of dynamics calculations},
  author={Freedberg, Daron I and Ishima, Rieko and Jacob, Jaison and Wang, Yun-Xing and Kustanovich, Irina and Louis, John M and Torchia, Dennis A},
  journal={Protein science},
  volume={11},
  number={2},
  pages={221--232},
  year={2002},
  publisher={Wiley Online Library}
}

@article{genoni2012identification,
  title={Identification of domains in protein structures from the analysis of intramolecular interactions},
  author={Genoni, Alessandro and Morra, Giulia and Colombo, Giorgio},
  journal={The Journal of Physical Chemistry B},
  volume={116},
  number={10},
  pages={3331--3343},
  year={2012},
  publisher={ACS Publications}
}

@article{gorecki2009redmd,
  title={RedMD~--~reduced molecular dynamics package},
  author={Gorecki, Adam and Szypowski, Marcin and Dlugosz, Maciej and Trylska, Joanna},
  journal={Journal of computational chemistry},
  volume={30},
  number={14},
  pages={2364--2373},
  year={2009},
  publisher={Wiley Online Library}
}

@article{gorecki36causality,
  title={Causality and correlation analyses of molecular dynamics simulation data},
  author={Gorecki, A and Trylska, J and Lesyng, B},
  journal={From Computational Biophysics to Systems Biology (CBSB07), volume NIC Series},
  volume={36},
  pages={25--30}
}

@article{grant2006bio3d,
  title={Bio3d: an R package for the comparative analysis of protein structures},
  author={Grant, Barry J and Rodrigues, Ana PC and ElSawy, Karim M and McCammon, J Andrew and Caves, Leo SD},
  journal={Bioinformatics},
  volume={22},
  number={21},
  pages={2695--2696},
  year={2006},
  publisher={Oxford Univ Press}
}

@article{hamelberg2005fast,
  title={Fast peptidyl cis-trans isomerization within the flexible Gly-rich flaps of HIV-1 protease},
  author={Hamelberg, Donald and McCammon, J Andrew},
  journal={Journal of the American Chemical Society},
  volume={127},
  number={40},
  pages={13778--13779},
  year={2005},
  publisher={ACS Publications}
}

@article{han2001data,
  title={Data mining: concepts and techniques},
  author={Han, Jiawei and Kamber, Micheline},
  journal={United States of America: Morgan Kauff mann Publishers},
  year={2001}
}

@article{hayward1997model,
  title={Model-free methods of analyzing domain motions in proteins from simulation: a comparison of normal mode analysis and molecular dynamics simulation of lysozyme},
  author={Hayward, Steven and Kitao, Akio and Berendsen, Herman JC},
  journal={Proteins: Structure, Function, and Bioinformatics},
  volume={27},
  number={3},
  pages={425--437},
  year={1997},
  publisher={Wiley Online Library}
}

@article{hayward1998systematic,
  title={Systematic analysis of domain motions in proteins from conformational change: New results on citrate synthase and T 4 lysozyme},
  author={Hayward, Steven and Berendsen, Herman JC},
  journal={Proteins Structure Function and Genetics},
  volume={30},
  number={2},
  pages={144--154},
  year={1998}
}

@article{hinsen1998analysis,
  title={Analysis of domain motions by approximate normal mode calculations},
  author={Hinsen, Konrad},
  journal={Proteins Structure Function and Genetics},
  volume={33},
  number={3},
  pages={417--429},
  year={1998}
}

@inproceedings{ho1995random,
  title={Random decision forests},
  author={Ho, Tin Kam},
  booktitle={Document Analysis and Recognition, 1995., Proceedings of the Third International Conference on},
  volume={1},
  pages={278--282},
  year={1995},
  organization={IEEE}
}

@book{james2013introduction,
  title={An introduction to statistical learning},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  volume={6},
  year={2013},
  publisher={Springer}
}

@article{kabsch1976solution,
  title={A solution for the best rotation to relate two sets of vectors},
  author={Kabsch, Wolfgang},
  journal={Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography},
  volume={32},
  number={5},
  pages={922--923},
  year={1976},
  publisher={International Union of Crystallography}
}

@article{kirchner2011objective,
  title={Objective identification of residue ranges for the superposition of protein structures},
  author={Kirchner, Donata K and Guntert, Peter},
  journal={BMC bioinformatics},
  volume={12},
  number={1},
  pages={170},
  year={2011},
  publisher={BioMed Central Ltd}
}

@article{lee2003dyndom,
  title={The DynDom database of protein domain motions},
  author={Lee, Richard A and Razaz, Moe and Hayward, Steven},
  journal={Bioinformatics},
  volume={19},
  number={10},
  pages={1290--1291},
  year={2003},
  publisher={Oxford Univ Press}
}

@book{martin1988two,
  title={Two Dimensional NMR Methods for Establishing Molecular Connectivity},
  author={Martin, Gary E and Zektzer, Andrew S and others},
  year={1988},
  publisher={VCH}
}

@article{meilua2007comparing,
  title={Comparing clusterings~--~an information based distance},
  author={Meila, Marina},
  journal={Journal of multivariate analysis},
  volume={98},
  number={5},
  pages={873--895},
  year={2007},
  publisher={Elsevier}
}

@article{potestio2009coarse,
  title={Coarse-grained description of protein internal dynamics: an optimal strategy for decomposing proteins in rigid subunits},
  author={Potestio, Raffaello and Pontiggia, Francesco and Micheletti, Cristian},
  journal={Biophysical journal},
  volume={96},
  number={12},
  pages={4993--5002},
  year={2009},
  publisher={Elsevier}
}

@book{richardson1981anatomy,
  title={The anatomy and taxonomy of protein structure},
  author={Richardson, Jane S},
  volume={34},
  year={1981},
  publisher={Academic Press}
}

@article{romanowska2012determining,
  title={Determining geometrically stable domains in molecular conformation sets},
  author={Romanowska, Julia and Nowinski, Krzysztof S and Trylska, Joanna},
  journal={Journal of Chemical Theory and Computation},
  volume={8},
  number={8},
  pages={2588--2599},
  year={2012},
    publisher={ACS Publications}
}

@article{sadiq2010explicit,
  title={Explicit solvent dynamics and energetics of HIV-1 protease flap opening and closing},
  author={Sadiq, S Kashif and De Fabritiis, Gianni},
  journal={Proteins: Structure, Function, and Bioinformatics},
  volume={78},
  number={14},
  pages={2873--2885},
  year={2010},
  publisher={Wiley Online Library}
}

@article{sinitskiy2012optimal,
  title={Optimal number of coarse-grained sites in different components of large biomolecular complexes},
  author={Sinitskiy, Anton V and Saunders, Marissa G and Voth, Gregory A},
  journal={The Journal of Physical Chemistry B},
  volume={116},
  number={29},
  pages={8363--8374},
  year={2012},
  publisher={ACS Publications}
}

@article{snyder2005clustering,
  title={Clustering algorithms for identifying core atom sets and for assessing the precision of protein structure ensembles},
  author={Snyder, David A and Montelione, Gaetano T},
  journal={Proteins: Structure, Function, and Bioinformatics},
  volume={59},
  number={4},
  pages={673--686},
  year={2005},
  publisher={Wiley Online Library}
}

@article{sokal1958statistical,
  title={A statistical method for evaluating systematic relationships},
  author={Sokal, R. R. and Michener, C. D.},
  journal={University of Kansas Scientific Bulletin},
  volume={28},
  pages={1409--1438},
  year={1958}
}

@article{tuckerman2007free,
  title={Free Energy Calculations: Theory and Applications in Chemistry and Biology},
  author={Tuckerman, Mark E},
  journal={Journal of the American Chemical Society},
  volume={129},
  number={35},
  pages={10963--10964},
  year={2007},
  publisher={ACS Publications}
}

@article{taylor2013classification,
  title={Classification of domain movements in proteins using dynamic contact graphs},
  author={Taylor, Daniel and Cawley, Gavin and Hayward, Steven},
  year={2013}
}

@article{vondrasek2002hivdb,
  title={HIVdb: a database of the structures of human immunodeficiency virus protease},
  author={Vondrasek, Jiri and Wlodawer, Alexander},
  journal={Proteins: Structure, Function, and Bioinformatics},
  volume={49},
  number={4},
  pages={429--431},
  year={2002},
  publisher={Wiley Online Library}
}

@book{weber2004perron,
  title={Perron cluster analysis and its connection to graph partitioning for noisy data},
  author={Weber, Marcus and Rungsarityotin, Wasinee and Schliep, Alexander},
  year={2004},
  publisher={Konrad-Zuse-Zentrum fur Informationstechnik Berlin}
}

@article{wriggers1997protein,
  title={Protein domain movements: detection of rigid domains and visualization of hinges in comparisons of atomic coordinates},
  author={Wriggers, Willy and Schulten, Klaus},
  journal={Proteins Structure Function and Genetics},
  volume={29},
  number={1},
  pages={1--14},
  year={1997}
}

@article{ye2003flexible,
  title={Flexible structure alignment by chaining aligned fragment pairs allowing twists},
  author={Ye, Yuzhen and Godzik, Adam},
  journal={Bioinformatics},
  volume={19},
  number={suppl 2},
  pages={ii246--ii255},
  year={2003},
  publisher={Oxford Univ Press}
}

@article{yesylevskyy2006dynamic,
  title={Dynamic protein domains: identification, interdependence, and stability},
  author={Yesylevskyy, Semen O and Kharkyanen, Valery N and Demchenko, Alexander P},
  journal={Biophysical journal},
  volume={91},
  number={2},
  pages={670--685},
  year={2006},
  publisher={Elsevier}
}

@article{zhang2009defining,
  title={Defining coarse-grained representations of large biomolecules and biomolecular complexes from elastic network models},
  author={Zhang, Zhiyong and Pfaendtner, Jim and Grafmuller, Andrea and Voth, Gregory A},
  journal={Biophysical journal},
  volume={97},
  number={8},
  pages={2327--2337},
  year={2009},
  publisher={Elsevier}
}

@article{periole2012structural,
  title={Structural determinants of the supramolecular organization of G protein-coupled receptors in bilayers},
  author={Periole, Xavier and Knepp, Adam M and Sakmar, Thomas P and Marrink, Siewert J and Huber, Thomas},
  journal={Journal of the American Chemical Society},
  volume={134},
  number={26},
  pages={10959--10965},
  year={2012},
  publisher={ACS Publications}
}

@article{torrie1977nonphysical,
  title={Nonphysical sampling distributions in Monte Carlo free-energy estimation: Umbrella sampling},
  author={Torrie, Glenn M and Valleau, John P},
  journal={Journal of Computational Physics},
  volume={23},
  number={2},
  pages={187--199},
  year={1977},
  publisher={Elsevier}
}

@article{kirkwood1935statistical,
  title={Statistical mechanics of fluid mixtures},
  author={Kirkwood, John G},
  journal={The Journal of Chemical Physics},
  volume={3},
  number={5},
  pages={300--313},
  year={1935},
  publisher={AIP Publishing}
}

@article{arnautova2011development,
  title={Development of a new physics-based internal coordinate mechanics force field and its application to protein loop modeling},
  author={Arnautova, Yelena A. and Abagyan, Ruben A. and Totrov, Maxim},
  journal={Proteins: Structure, Function, and Bioinformatics},
  volume={79},
  number={2},
  pages={477--498},
  year={2011},
  publisher={Wiley Online Library}
}

@article{dill1997additivity,
  title={Additivity principles in biochemistry},
  author={Dill, Ken A},
  journal={Journal of Biological Chemistry},
  volume={272},
  number={2},
  pages={701--704},
  year={1997},
  publisher={ASBMB}
}

@article{brady1995decomposition,
  title={Decomposition of interaction free energies in proteins and other complex systems},
  author={Brady, G Patrick and Sharp, Kim A},
  journal={Journal of molecular biology},
  volume={254},
  number={1},
  pages={77--85},
  year={1995},
  publisher={Elsevier}
}

@article{boresch1995meaning,
  title={The meaning of component analysis: decomposition of the free energy in terms of specific interactions},
  author={Boresch, Stefan and Karplus, Martin},
  journal={Journal of molecular biology},
  volume={254},
  number={5},
  pages={801--807},
  year={1995},
  publisher={Elsevier}
}


@article{bao2009protein,
  title={Protein mechanics: a new frontier in biomechanics},
  author={Bao, G},
  journal={Experimental mechanics},
  volume={49},
  number={1},
  pages={153--164},
  year={2009},
  publisher={Springer}
}

@article{lavery2007protein,
  title={Protein mechanics: a route from structure to function},
  author={Lavery, Richard and Sacquin-Mora, Sophie},
  journal={Journal of biosciences},
  volume={32},
  number={1},
  pages={891--898},
  year={2007},
  publisher={Springer}
}

@article{sacquin2014motions,
  title={Motions and mechanics: investigating conformational transitions in multi-domain proteins with coarse-grain simulations},
  author={Sacquin-Mora, Sophie},
  journal={Molecular Simulation},
  volume={40},
  number={1-3},
  pages={229--236},
  year={2014},
  publisher={Taylor \& Francis}
}

@article{chiang2013molecular,
  title={Molecular mechanics and dynamics characterization of an in silico mutated protein: A stand-alone lab module or support activity for in vivo and in vitro analyses of targeted proteins},
  author={Chiang, Harry and Robinson, Lucy C and Brame, Cynthia J and Messina, Troy C},
  journal={Biochemistry and Molecular Biology Education},
  volume={41},
  number={6},
  pages={402--408},
  year={2013},
  publisher={Wiley Online Library}
}

@article{van1997engineering,
  title={Engineering protein mechanics: inhibition of concerted motions of the cellular retinol binding protein by site-directed mutagenesis.},
  author={van Aalten, DM and Jones, PC and De Sousa, M and Findlay, JB},
  journal={Protein engineering},
  volume={10},
  number={1},
  pages={31--37},
  year={1997},
  publisher={Oxford Univ Press}
}

@article{chipot2014frontiers,
  title={Frontiers in free-energy calculations of biological systems},
  author={Chipot, Christophe},
  journal={Wiley Interdisciplinary Reviews: Computational Molecular Science},
  volume={4},
  number={1},
  pages={71--89},
  year={2014},
  publisher={Wiley Online Library}
}

@article{comer2014adaptive,
  title={The Adaptive Biasing Force Method: Everything You Always Wanted To Know but Were Afraid To Ask},
  author={Comer, Jeffrey and Gumbart, James C and Henin, Jerome and Lelievre, Tony and Pohorille, Andrew and Chipot, Christophe},
  journal={The Journal of Physical Chemistry B},
  year={2014},
  publisher={ACS Publications}
}

@article{gumbart2012standard,
  title={Standard binding free energies from computer simulations: What is the best strategy?},
  author={Gumbart, James C and Roux, Benoit and Chipot, Christophe},
  journal={Journal of chemical theory and computation},
  volume={9},
  number={1},
  pages={794--802},
  year={2012},
  publisher={ACS Publications}
}


@article{carter1989constrained,
  title={Constrained reaction coordinate dynamics for the simulation of rare events},
  author={Carter, EA and Ciccotti, Giovanni and Hynes, James T and Kapral, Raymond},
  journal={Chemical Physics Letters},
  volume={156},
  number={5},
  pages={472--477},
  year={1989},
  publisher={Elsevier}
}

@article{woo2005calculation,
  title={Calculation of absolute protein--ligand binding free energy from computer simulations},
  author={Woo, Hyung-June and Roux, Benoit},
  journal={Proceedings of the national academy of sciences of the united states of america},
  volume={102},
  number={19},
  pages={6825--6830},
  year={2005},
  publisher={National Acad Sciences}
}

@article{seifert2013protein,
  title={Protein mechanics: How force regulates molecular function},
  author={Seifert, Christian and Grater, Frauke},
  journal={Biochimica et Biophysica Acta (BBA)-General Subjects},
  volume={1830},
  number={10},
  pages={4762--4768},
  year={2013},
  publisher={Elsevier}
}

@article{den2013revisiting,
  title={Revisiting the Exact Relation between Potential of Mean Force and Free-Energy Profile},
  author={den Otter, Wouter K},
  journal={Journal of Chemical Theory and Computation},
  volume={9},
  number={9},
  pages={3861--3865},
  year={2013},
  publisher={ACS Publications}
}

@article{sugita2000multidimensional,
  title={Multidimensional replica-exchange method for free-energy calculations},
  author={Sugita, Yuji and Kitao, Akio and Okamoto, Yuko},
  journal={The Journal of Chemical Physics},
  volume={113},
  number={15},
  pages={6042--6051},
  year={2000},
  publisher={AIP Publishing}
}

@book{press2007numerical,
  title={Numerical recipes 3rd edition: The art of scientific computing},
  author={Press, William H},
  year={2007},
  publisher={Cambridge university press}
}


@book{chipot2007free,
  title={Free energy calculations: theory and applications in chemistry and biology},
  author={Chipot, Christophe and Pohorille, Andrew},
  volume={86},
  year={2007},
  publisher={Springer}
}

@article{olboyle2011open,
  title={Open Babel: An open chemical toolbox},
  author={O'Boyle, Noel M and Banck, Michael and James, Craig A and Morley, Chris and Vandermeersch, Tim and Hutchison, Geoffrey R},
  journal={J Cheminf},
  volume={3},
  pages={33},
  year={2011}
}

@article{eastman2010openmm,
  title={OpenMM: A hardware-independent framework for molecular simulations},
  author={Eastman, Peter and Pande, Vijay},
  journal={Computing in Science \& Engineering},
  volume={12},
  number={4},
  pages={34--39},
  year={2010},
  publisher={AIP Publishing}
}


@article{frey2007clustering,
  title={Clustering by passing messages between data points},
  author={Frey, Brendan J and Dueck, Delbert},
  journal={science},
  volume={315},
  number={5814},
  pages={972--976},
  year={2007},
  publisher={American Association for the Advancement of Science}
}

@article{carlstein1986use,
  title={The use of subseries values for estimating the variance of a general statistic from a stationary sequence},
  author={Carlstein, Edward},
  journal={The Annals of Statistics},
  pages={1171--1179},
  year={1986},
  publisher={JSTOR}
}

@book{cole1998clustering,
  title={Clustering with genetic algorithms},
  author={Cole, Rowena Marie},
  year={1998},
  publisher={Citeseer}
}


@article{hall1995blocking,
  title={On blocking rules for the bootstrap with dependent data},
  author={Hall, Peter and Horowitz, Joel L and Jing, Bing-Yi},
  journal={Biometrika},
  volume={82},
  number={3},
  pages={561--574},
  year={1995},
  publisher={Biometrika Trust}
}

@article{kreiss2011bootstrap,
  title={Bootstrap methods for dependent data: A review},
  author={Kreiss, Jens-Peter and Paparoditis, Efstathios},
  journal={Journal of the Korean Statistical Society},
  volume={40},
  number={4},
  pages={357--378},
  year={2011},
  publisher={Elsevier}
}

@article{wu2009external,
  title={External validation measures for K-means clustering: A data distribution perspective},
  author={Wu, Junjie and Chen, Jian and Xiong, Hui and Xie, Ming},
  journal={Expert Systems with Applications},
  volume={36},
  number={3},
  pages={6050--6061},
  year={2009},
  publisher={Elsevier}
}
\end{filecontents}



\usepackage[backend=biber, refsegment=chapter, defernumbers=true, bibencoding=UTF-8]{biblatex}
  

\addbibresource{references.bib}

  
\begin{document} 

\include{Title}

%\include{Abstract}

\tableofcontents

\chapter{Introduction}

The primary aim of this work was the development of methods for extracting relevant information from multi-dimensional data describing biomolecular systems.
We adapted advanced approaches from the general-purpose field of \emph{machine learning}, and in particular~--~from the \emph{unsupervised} machine learning methodology.
In Chapter~2, we present our recently published method of discovering quasi-rigid parts in proteins, that can be used to better interpret experimental as well as simulation data.
Chapter~3 presents a different problem of identifying parts of a molecular system that propel (or hinder) a given structural transition.
In both these cases we applied \emph{clustering} algorithms, commonly used for finding patterns in unstructured~--~and seemingly chaotic~--~data.

In this introductory chapter, we give an overview of the machine learning approach and the molecular dynamics simulation scheme.
These opening mini-reviews were intended to sketch a map of the current state of knowledge, and mark the research frontier engaged in this work.
Particular clustering algorithms and molecular dynamics technique are discussed in more details in Chapters~2 and~3.

\section{Machine learning}

Machine learning is a branch of computer science, concerned with algorithms that ``learn'' how to extract pertinent information from noisy, complicated datasets.
The three main branches of machine learning are: \emph{supervised}, \emph{unsupervised} and \emph{reinforcement} learning.
The first is concerned with assigning a class (or: label) to a given observation, assuming that an independent and correctly labeled dataset is known beforehand.
The unsupervised learning deals with a similar task, except that there is no prior (nor posterior, for that matter) knowledge about which observations should be assigned to which class, nor is it clear what is the ``right'' number of such classes.
The reinforcement learning regime, often identified with artificial intelligence, deals with the problem of taking \emph{actions} under certain \emph{conditions} (or: in a given \emph{environment}), so as to maximize the cumulative \emph{reward}. 
Most notably, reinforcement learning algorithms are used in games (such as chess), but despite their tremendous impact on present and~--~probably~--~future research\footnote{See, for example, Google's Gorila project~\cite{nair2015massively}, or Skynet from the \emph{Terminator} franchise.}, their discussion reaches beyond the scope of this dissertation, and we shall not present them in more detail.

\subsection{Supervised machine learning}

The data may have an underlying~--~but difficult to define~--~structure, so that each observation comprising the dataset belongs to a particular class\footnote{
Typically \emph{regression} is also considered a part of the supervised learning scheme~\cite{james2013introduction}.
In principal, the regression problem is broader than classification~--~instead of discreet classes, observations are ascribed real values. 
Thus, regression deals with the task of approximating the function $f$ that for each observation $\mathbf{x}$ assigns its corresponding value, $f(\mathbf{x})$.
However, for the purposes of this short review we shall focus only on classification.
}.
We say that the data are \emph{labeled}, i.e. each observation has a label assigning it to one of $m$ classes.
In other words, the dataset is composed of $N$ observations, $\{ (\mathbf{x}_i, y_i) \}_{i=1}^N$, where each observation is a pair with: a vector $\mathbf{x}_i$ describing the $i^\text{th}$ instance, and a corresponding label $y_i$.
The vector $\mathbf{x}_i$ includes features, each with its own domain (discreet, continuous, or other).
The label $y_i$, on the other hand, takes on one of $m$ values, corresponding to one of $m$ pre-defined classes.
Supervised machine learning algorithms try to capture correlation, mutual information, or any other types of relations between the features and the labels $y$, to produce a prediction for any new, previously unseen, observation.

Because the primary aim of supervised learning is the assignment of observations to classes, it is also known as the \emph{classification problem}, and particular algorithms are referred to as \emph{classifiers}.
One of the first classifying algorithms was the \emph{k nearest neighbors} ($k$-NN) method, proposed as early as 1951~\cite{fix1951discriminatory}.
In $k$-NN we assume the observations are points embedded in a metric space, so that their mutual distance can be readily calculated.
For a given $k$, and a set of points with known labels, we assign a new observation to the class, which is most prevalent among its $k$ nearest neighbors.
In the case of a 2-class problem, it is advised to choose an odd number of neighbors to avoid ties.
However, $k$-NN suffers from noisy data (uninformative coordinates in the vectors $\mathbf{x}$ describing the observations), and from the ``curse of dimensionality'' (the problem of finding truly close neighbors in a high-dimensional space).

A great number of classification methods have been proposed in lieu of $k$-NN, and it is not the purpose of this work to present them all.
However, it is noteworthy that alongside new supervised learning algorithms, auxiliary meta-methods have been developed, that combine several classifiers and greatly surpass their individual predictive power.
Most notable are the \emph{bagging} and \emph{boosting} schemes.
In fact, one of the most powerful classification method to date is the \emph{eXtreme Gradient Boosting} (XGBoost) algorithm~\cite{chen2016xgboost}, that combines boosting and \emph{random forests}~\cite{ho1995random} (another high-ranking classifier which we shall not discuss further).

Practitioners of supervised machine learning can test their skill by participating in contests held at \texttt{www.kaggle.com}.
Most of these contests offer prizes (most famous is the \$1 million Netflix competition), but more importantly, the discussion at \texttt{www.kaggle.com} forums is the most current source of information about the trends, novelties and successes of cutting-edge machine learning methods. 

\subsection{Unsupervised machine learning}

Unsupervised machine learning is a class of algorithms for extracting information from unstructured datasets.
Fundamentally, these methods address the following questions\footnote{
These are not the only questions raised by unsupervised machine learning methods.
Most notably, one of the more interesting, and fast-changing studies is the \emph{community detection}, which is a more general problem than clustering.
But as machine learning becomes more and more popular, new problems and applications arise, and it is difficult to draw a clear line that would encompass the whole field of unsupervised machine learning.}:
\begin{itemize}
 \item Is there a clear way of visualizing a complex set of data?
 \item Can we partition a dataset into cohesive subgroups, distinctive from each other?
 \item What  statistically significant patterns (e.g. correlations, rules, causality relationships) are present in the dataset?
\end{itemize}

As we explain further, the data considered in this dissertation came from numerical simulations of molecular models.
However, unsupervised machine learning is commonly used for analyses of all sorts of objects: websites, genomic data, points on a map, etc. 
But regardless of the source of these data, we are faced with the problem of purifying the information hidden within it.
Unsupervised machine learning offers two main methods of simplifying the data: \emph{dimensionality reduction} (predominantly: principal component analysis), and \emph{clustering} (for identifying distinctive subgroups of objects).

\subsubsection{Principal component analysis}

The \emph{principal component analysis} (PCA) is a transformation that, for a given set of observation points with correlated coordinates, yields a set of points with coordinates that are no longer correlated.
Typically, those uncorrelated observations are represented using a lesser number of dimensions, therefore PCA is often regard as a dimensionality reduction procedure.
As such, PCA is often employed in the exploratory phase of an analysis.

However, exploratory data analysis is not the only application for PCA; it is commonly used in reducing noise in observations in supervised learning methods, thus enhancing their predictive power.
In Chapter~2, we facilitate PCA to select representative structures from a large set of configurations of the HIV-1 protease.
These representatives are then used as input for our cluster analysis, leading to quasi-rigid structural parts of a protein.

The PCA is not without its limitations.
At its core, PCA relies on covariance as a measure of relatedness between coordinates of the vectors representing observations in the dataset.
However, null covariance of two random variables $X$ and $Y$ does not imply that they are independent\footnote{
In particular, $Y$ and $X = Y^2$ are uncorrelated if (for example) $Y$ is normally distributed with a zero mean, yet they are clearly dependent.
}.
Thus, whenever we are unwilling to assume that covariance is a reliable measure of dependence, we should use more advanced techniques (e.g., one of many non-linear PCA variants~\cite{gorban2008principal}, or the immensely popular $t$-SNE algorithm~\cite{maaten2008visualizing}).

\subsubsection{Overview of popular clustering methods}

One of the first clustering methods was the \emph{k-means} algorithm~\cite{macqueen1967some} ($k$ indicating the number of clusters), in which points are assigned to the cluster with the closest mean.
Finding an exact solution to the $k$-means problem is computationally intractable, therefore most implementations carry out a greedy, iterative procedure that converges to a local minimum.
Such an approximate solution is, in most cases, satisfactory, however the main disadvantage of the $k$-means algorithm reveals itself whenever clusters have a stretched out shape.
In such cases, the $k$-means disregards the oblong shape of the groupings, because it does not account for gaps between clusters, neither for their ``continuity''.

Another popular clustering algorithm, DBSCAN (\emph{Density-Based Spatial Clustering of Applications with Noise})~\cite{ester1996density}, alleviates $k$-means' problems by identifying clusters as continuous, dense ``clouds'' of points.
As a result, the clusters are allowed to have even complex shapes, as long as they retain their connected structure.
However, although DBSCAN is capable of recognizing oblong clusters, it often fails to identify frontiers between them.
That is, DBSCAN has difficulties in discerning weak connections between clusters from strong, frequent ones within them.
As a result, DBSCAN is a poor choice when dealing with sets of points with many weak inter-cluster connections.

DBSCAN requires a metric or a similarity function, that expresses relatedness of two observations.
This is a common scenario, in which we are more focused on the relations between objects, rather than how to describe them using a multi-dimensional vector.
It makes for a convenient setup as it puts an emphasis on the meaning of clusters (assuming we chose a meaningful measure of relatedness), instead of individual objects.

Alternative to DBSCAN, and a fairly natural approach to clustering, is a methods called \emph{agglomerative hierarchical} clustering.
Like in DBSCAN, we need to specify a metric (or a similarity measure) that quantifies relatedness pairs of objects in the dataset.
Hierarchical clustering is a greedy, iterative procedure, that builds clusters by merging groups of points that are closest.
This closeness may be expressed in terms the minimal distance between any two points from the two groups, in terms of their mean distance, or in a number of different ways.
The procedure stops, when the minimal closeness of clusters exceeds a pre-defined threshold, called the \emph{height parameter}\footnote{
Its name is associated with the fact that mergers carried out by the hierarchical procedure may be represented as a specific type of a graph, called the \emph{tree}.
The point at which we stop the mergers is related to the height of that tree, hence the name of the parameter.}.
One of the drawbacks of the hierarchical clustering scheme is that it is difficult to choose the optimal value for the height parameter.
Additional criteria may be utilized for this purpose, but even then the ``best'' value of the parameter may be volatile and even a slight change in its value yields a completely different result.

Great many other clustering algorithms have been proposed to answer the needs arising in many different fields of application.
Currently, the most valued clustering algorithms are those that prove useful in a whole variety of problems.
That is, methods that work well regardless of the shape of the clusters, the exact nature of the similarity measure, or small changes in the input paramters.

\subsubsection{Number of clusters}

Most clustering algorithms require as input a parameter $k$, indicating the number of clusters.
Alternatively, some other parameter may be required that ultimately determines the number of clusters (for example, the height parameter in hierarchical clustering~--~see the discussion in Chapter~2).

The $k$ parameter is crucial, as it pre-determines the result of any clustering algorithm.
Assuming we chose a clustering algorithm, a typical strategy of finding the optimal number of clusters is to carry out the procedure for $k=2,\ldots,K$.
Then, for each clustering we evaluate the result using an external quality measure~(see \cite{wu2009external} for a comprehensive review).
However, such an approach only adds to the overall complexity of the clustering problem, because different external quality measures are suitable for different applications.

\subsection{Adjacency matrix representation and spectral clustering}\label{sec:matrixRepresentation}

It is important to introduce at this point the matrix representation of a set of objects and their similarity.
If we enumerate the objects in our set using natural numbers $1,2,3,\ldots,N$, and denote the similarity between objects $i$ and $j$ by $w_{ij}$, we can represent the relations between the objects using a matrix $W\in\mathbb{R}^{N\times N}$.
Matrix $W$ is referred to as the \emph{similarity} (or: \emph{adjacency}) \emph{matrix}, and we refer to it repeatedly throughout this dissertation.

The clustering method used in Chapter~2 finds an optimal subdivision of the dataset by examining the eigenvectors and eigenvalues of the similarity matrix.
At a glance, it seems plausible that the eigenvectors of the similarity matrix somehow summarize the structure of the data.
But~--~perhaps more suprisingly~--~the eigenvalues contain information about the ``distortion'' of the clustering, and can thus help in choosing the optimal number of clusters.
Therefore, the spectral clustering algorithm discussed in Chapter 2 not only provided us with high-quality partitions, but also suggested the \emph{right} number of clusters.


\subsubsection{Unsupervised machine learning in this dissertation}

In Chapter~2, we propose and utilize a similarity function expressing geometrical variability of pairs of amino acids.
As a result, the clustering indicates structural parts of a protein that are internally rigid, i.e. composed of amino acids sharing low geometrical variability.
In Chapter~3, the similarity is defined for pairs of atoms, and expresses their contribution to the free energy change associated with a given structural transition.
As a result, the clustering yields two parts of a molecule that cooperate in pushing the transformation forward, or pulling it backwards.
We call these parts \emph{molecular cogs}, to draw an analogy to the popular view that large biomolecules can often be thought of as ``molecular machines''. 

\section{Molecular dynamics}

Molecular dynamics (MD) comprise a wide range of numerical methods designed for the study of motions and properties of molecular systems.
Typically, the MD approach assumes a classical potential energy function, approximated by an analytical function $U$, with force field parameters, which treats atoms as electrostatically charged points, and chemical bonds as springs and hinges.
The arguments of the $U$ function are atomic configurations $\mathbf{q}$ of the molecular system, with the potential energy values, $U(\mathbf{q})$.
If we denote the set of all configurations of a given system by $\Omega$, then $U\colon\Omega\to\mathbb{R}$. % TODO: zobacz, czy to nie jest zbedne
The force acting on a particular atom $i$ is then derived from $U$ by taking the gradient with respect to position $\mathbf{q}_i$ of that atom:
$$
\mathbf{F}_i(\mathbf{q}) = -\nabla_i U(\mathbf{q}),
$$
which requires of $U$ to be differentiable. 

More accurate algorithms take into account quantum effects, making the simulation more reliable, but also immensely expensive in terms of computational time.
However, although modeling intricate properties of molecular systems using a classical potential may seem an over-simplification, MD has achieved considerable success over the recent years, and is constantly improving.
Classical MD simulations are popular mainly because of their significantly lower computational cost in comparison with their quantum-mechanical counterparts.
This, in particular, allows for estimation of free energy profiles of structural transitions, even for large, biologically-relevant systems (such as enzymes, DNA, membrane systems, and others).
MD can be therefore thought of as a testing ground for advanced sampling techniques, but also pattern-recognition techniques such as the aforementioned unsupervised machine learning.

The free energy translates into experimentally observable properties.
In particular, the affinity of a ligand and an enzyme, or the chance of transferring an ion through a membrane channel.
Free energy profiles offer insight into the process at hand, but also allow for utilizing feedback from experimental data to refine the modeling potentials.

To run MD simulations, one needs to choose the right force field, but also~--~and perhaps more importantly~--~the appropriate \emph{sampling technique}.
The underlying premise is that molecular systems in thermal equilibrium experience random impulses, which can be modelled \emph{via} collisions with molecules of the surrounding world.
The total energy of a system in thermal equilibrium is, therefore, \emph{not} constant, but rather fluctuates around a certain mean value.

Consequently, configurations of a system are characterized by a probability density, which in the case of thermal equilibrium ($T=const.$) is the well-known \emph{Boltzmann distribution}:
\begin{equation}
 \rho_B(\mathbf{q})=Z^{-1} \exp[-U(\mathbf{q})/k_BT],
\end{equation}
where $k_B$ is the Boltzmann constant, and $Z$ is a normalizing factor of great importance, often referred to as the \emph{partition function}, given by: 
$$Z=\int_\Omega \exp[-U(\mathbf{q})/k_BT] d\mathbf{q}.$$
From this probabilistic perspective we can view an MD simulation as a sampling procedure, during which we \emph{should} acquire configurations according to the Boltzmann distribution (assuming $T=const.$).
Macroscopic properties of the system (affinity of a ligand to an enzyme, for example) are defined as \emph{expected values} of appropriate microscopic quantities, and are estimated by averages over configurations sampled in an MD simulation.
In the constant temperature regime, MD and Monte Carlo simulation data can be used to estimate internal energy, $E$, entropy, $S$, and the free energy, $A$, of a system using basic formulae: 
$$E = \langle U \rangle,$$ 
$$S = -k_B \langle \log \rho_B \rangle,$$
and 
$$A = E - TS,$$ 
respectively.
The $\langle \cdot \rangle$ denotes either the time-average (MD case) or ensemble-average (Monte Carlo case).
It is of paramount importance that the sample is generated correctly, i.e. so that the \emph{whole} configurational space is adequately scanned, and that the estimates do not suffer from enormous statistical errors.


\subsection{Sampling techniques and the potential of mean force}

Underlying the simplistic, classical assumptions embedded in MD simulations are complex sampling techniques used for estimating macroscopic properties of microscopic systems.
MD simulations can, for example, mimic time evolution of a system with the use of integration schemes such as the \emph{Langevin dynamics}, which numerically integrates the following stochastic differential equation:
$$
 \mathbb{M}\ddot{\mathbf{q}} = -\nabla U(\mathbf{q}) -\gamma \mathbb{M}\dot{\mathbf{q}} + \sqrt{2\gamma k_BT\mathbb{M}}\hspace{0.1cm}\mathbf{R}(t)
$$
where $\mathbb{M}$ is a diagonal matrix of atom masses, and $\mathbf{R}(t)$ is a stationary Gaussian process with zero mean, and auto-correlation expressed by the Dirac delta function: $\langle \mathbf{R}(t)\mathbf{R}(t') \rangle = \delta(t-t')$. 
It is important to note that although the output of a Langevin dynamics simulation is a trajectory, most sampling techniques avoid calling it the real time-evolution of a system.

\subsubsection{Potential of mean force}

Vast samples of multidimensional points produced in the course of an MD simulation are impossible to interpret, unless we utilize some simplifying technique.
Probably the most popular representation of a change or transformation occurring in a system is the so-called \emph{collective variable}, and the accompanying concept of the \emph{potential of mean force} (PMF).
A collective variable $\xi$ is a function of the system's configuration $\mathbf{q}$, defined so that its change can be readily translated into the progress of a particular transformation.
The PMF for a particular value $\xi^*$ of that collective variable, is defined as follows: 
$$ A(\xi^*):=-k_BT\log \rho (\xi^*), $$ 
where $\rho$ is a probability density of the configuration $\mathbf{q},$ such that $\xi(\mathbf{q})=\xi^*$.

While the collective variable $\xi$ serves as a means of reducing the dimensionality of a complex structural transition, the PMF provides a probabilistic interpretation of the process.
Through the PMF we gain insight into the transformation's bottle-necks and into the ranges of values of the collective variable, corresponding to the system's meta-stable states.
In Chapter~3, we describe our method of discovering the so-called \emph{molecular cogs}, which was possible by embedding a particular sampling technique aimed at extracting the PMF.

\subsection{Unsupervised machine learning in molecular data analysis}

Data produced by MD simulations of biomolecules are multi-dimensional trajectories, points $\{ \mathbf{q}_t \}_{t=1}^S$, where $\mathbf{q}_t\in\mathbb{R}^{3n}$ ($S$ is the number of steps of the simulation, and $n$~--~the number of atoms of the system).
Depending on the sampling technique adapted in the MD simulation, the $t$ parameter may or may not be associated with time, as, for example, in the case of Monte Carlo sampling.
Analogously to MD, configurations acquired from NMR experiments can be also thought of as a set of multi-dimensional points, $\mathbf{q}_t\in\mathbb{R}^{3n}$, in which the order imposed by the $t$ parameter has very little to do with time.

The methods presented in Chapters~2 and~3 of this dissertation exploit unsupervised machine learning to infer properties and mechanics of molecular systems from noisy, multi-dimensional data.
It is our hope that this type of analysis allows for better understanding of molecular systems, and gives an incentive for posing new hypotheses for more experimentally-oriented research.

\subsubsection{Dynamic domains as clusters}

Small, globular proteins are often thought of as fairly rigid biomolecules.
However, as the number of atoms in a system increases, the set of low-energy, accessible configurations grows rapidly.
Consequently, large molecular systems can undergo significant structural changes, involving collective, multi-step transitions.
Due to immense complexity of these systems, such phenomena are difficult to describe, let alone to interpret.

In Chapter~2, we introduce a method of simplifying these incomprehensible transitions using a clustering scheme.
The main idea of our method is to find parts of the protein which~--~although moving with respect to each other~--~remain internally rigid.
Or, more accurately, \emph{quasi-}rigid, because small variations in distances between amino acids are omnipresent.
In the literature, these quasi-rigid parts of the protein are referred to as \emph{dynamic domains}~\cite{bahar1997direct,hayward1998systematic,hinsen1998analysis}.

In the dynamic domains identification we adapted a clustering framework.
We proposed a measure of strength of a contact between amino acids, and applied an appropriate clustering algorithm to identify quasi-rigid parts of a protein.
The strength of a contact depends on structural variability of a given pair of residues.
Having a set of objects (amino acids) and a similarity measure (contact strength between residues), we constructed an adjacency matrix.
This matrix was used as input for a particular, best suited clustering procedure, one of the class of \emph{spectral algorithms}, which is focused on finding eigenvectors of a matrix, closely related to the adjacency matrix for a given protein.

\subsubsection{Molecular cogs as clusters}

In Chapter~3, we present our newly-developed method of identifying \emph{molecular cogs}~--~subsystems of a molecule which, for a given structural transformation, drive the transition forwards or backwards.
This is another application of the cluster analysis, although very different from the one discussed in Chapter~2.

For the purpose of identifying molecular cogs we proposed a ``similarity function'' between pairs of atoms, which indeed quantifies the contributions of that pair the the free energy change along a collective variable.
With that, we again construct adjacency matrices, and discover clusters which correspond to groups of atoms sharing a common contribution to the free energy (positive or negative).
However, the method of dynamic domains and molecular cogs identification differ not only at the stage of adjacency matrix construction, but also in the choice of the clustering algorithm.

\section{The overall aim of this dissertation}

Both, simulation and experimentally acquired data become increasingly complex, in particular because:
\begin{itemize}
 \item the systems under study are progressively larger,
 \item and their structural transitions comprise chaotic paths in multi-dimensional configurational spaces.
\end{itemize}
For both these reasons, we need to facilitate methods of extracting the underlying structure of the data, to gain perspective, and to augment our 3-dimensional perception in order to pose new hypotyses about the biomolecular world.

One problem arising in computational biophysics is the identification of dynamic domains, where the input are configurations produced in the course of an MD simulation, or an NMR experiment.
Although highly mobile and flexible, many proteins have structurally static sub-regions.
By expressing protein's conformations in terms of quasi-rigid parts that move with respect to one another, but remain internally  rigid, we could significantly simplify the description of their dynamic nature.

The second problem tackled in this dissertation is the identification of molecular subsystem that play an active role in structural transformations of the whole systems.
The focus of this study was more technical, as the molecules analyzed therein were small.
However, we were aiming at discovering conceptual and methodological caveats that were bound to hinder an analysis of larger molecules such as proteins.

The overall aim of this dissertation was the simplification of complex, dynamical properties of molecular systems.
We applied unsupervised machine learning techniques to two such problems.
In the following chapters we describe how we constructed the adjacency matrices used for clustering, how we chose the optimal number of clusters, and how we verified that the obtained clusters conveyed valuable insight.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% CHAPTER II 
\definecolor{kugray5}{RGB}{224,224,224}

\newcommand{\angstrom}{\mbox{\normalfont\AA}}
\newcommand{\myHeight}{0.13}
\renewcommand{\arraystretch}{1.9} % space between rows

\chapter{ResiCon: a method for the identification of dynamic domains, hinges and interfacial regions in proteins}

The text of Chapter~2 largely contains material published in the Oxford Journal \emph{BIONFORMATICS}~\cite{dziubinski2016resicon}. 
The co-authors, Bogdan Lesyng and Pawe\l{} Daniluk, came up with the idea of using contacts as a means of assessing structural variability and provided expertise regarding analysis of protein structures and clustering strategies.

\subsubsection{Motivation and set-up}

In the introductory chapter, we observed that the interpretation of a clustering procedure depends on the measure used to express pairwise similarity between objects in a dataset.
Likewise, the \emph{quality} of the clustering depends on how cleverly this measure is chosen.
The similarity measure can be conjured up from experience, intuition or any oter premise, but it stand to reason that only those measures that successfully capture pertinent information are likely to yield valuable insight.

The method of dynamic domains detection described in the following chapter makes use of a structure-based similarity measure between two amino-acid residues.
On the one hand, such a measure is fairly simple, which makes the interpretation of the clustering much easier.
On the other hand, the measure was expected to capture essential interactions between residues, as pairs of amino-acids that remain close together are more likely to attract, rather than repel one another.

The results of our method (ResiCon) were compared to those of two other methods in the field.
The first one (GeoStaS) is purely structure-based, however the formulation of the similarity measure used therein lacks any physical motivation.
The second (PiSQRD) uses a measure that was aimed at describing physical interactions between amino-acid residues.
Results presented in this chapter show that ResiCon found a middle ground between these two methods, yielding more compact and rigid dynamic domains than either GeoStaS or PiSQRD.

\section{Introduction}
Proteins are not static.
Nuclear magnetic resonance (NMR) spectroscopy~\cite{martin1988two} and the spin-echo spectroscopy~\cite{bu2011proteins} experiments show that.
In several cases it was proven that flexibility may be crucial to protein functionality~\cite{farago2010activation,hamelberg2005fast}.
Although experimental methods provide only general clues about intramolecular motions, molecular dynamics (MD) simulations extend their reach by giving a higher resolution picture~--~both in space and time~--~of protein mobility.
By studying an NMR ensemble or MD trajectory one may notice that it is composed of relatively rigid structural parts, often referred to as \emph{dynamic domains}~\cite{hayward1997model}.

Domains in traditional sense are regarded as parts of the protein which are: conserved (in terms of evolution), autonomous (in terms of folding), and/or compact (in terms of tertiary structure).
Such ``static'' domains are identified through sequence homology, structural analysis of a single configuration, or both.
(For conventional methods of identifying protein domains based on multiple sequences or a single structure see e.g. \cite{richardson1981anatomy,bork1991shuffled}.)
Conversely, dynamic domains depend on structural transitions performed by the protein.

A number of methods for identification of dynamic domains have been developed.
The simplest procedures are based on normal mode analysis, which assumes a harmonic approximation of the potential energy function~\cite{hinsen1998analysis}.
More advanced approaches use the Gaussian Network Model and analyze correlations in motions between the residues of the protein~\cite{yesylevskyy2006dynamic}.
Many other approaches have also been developed~\cite{bahar1997direct,wriggers1997protein,bernhard2010optimal,genoni2012identification,potestio2009coarse}, but all of them anticipate dynamic domains by analyzing a single structure of a protein.
Another class of methods for dynamic domains assignment requires exactly two configurations (see \cite{hayward1998systematic,lee2003dyndom,ye2003flexible}).
However, the assumption that two ``representative'' structures encompass all relevant motions is rather speculative.

Experimental and \emph{in silico} methods reach beyond single-structure representation, and are capable of producing numerous configurations of a given protein.
Rather than inferring dynamic domains from one or two structures, a more natural approach would be to determine them based on an ensemble of configurations.
GeoStaS is the only method known to us that analyzes a whole ensemble of configurations and assigns each residue to a dynamic domain \cite{romanowska2012determining}.
Although GeoStaS can analyze not only proteins but also nucleic acids, it fails to discover dynamic domains whenever they rotate with respect to each other.
Alternative methods of analyzing ensembles of configurations assign residues to a static ``core'' or unstructured bundle (see \cite{snyder2005clustering,kirchner2011objective}).

The purpose of this study was to develop a novel methodology named ResiCon, capable of extracting dynamic domains from an ensemble of protein's configurations.
ResiCon analyzes strengths of contacts between residues based exclusively on geometrical changes occuring in the provided set of structures.
The set may be an NMR ensemble of configurations, or snapshots produced in the course of an MD simulation.
ResiCon's main functionality is to identify dynamic domains, but it can also find hinges and interfacial (interdomain) regions.

\section{Approach} 
ResiCon starts with identifying pairs of residues which are \emph{in contact}.
There are several definitions of contacts between amino-acid residues in the literature.
We used the definition presented in~\cite{daniluk2011novel} and adapted it to the case when more than one structure is given (see also~\cite{daniluk2014theoretical}).

Next, ResiCon constructs a virtual scaffold, connecting residues which are in contact with bars.
Stiffness of a given bar reflects the estimated strength of the corresponding contact. 

Finally, to identify dynamic domains, ResiCon carries out a partitioning (by computing minimal cuts) of the scaffold, cutting weaker and preserving stiffer bars.
This partitioning is carried out by applying a spectral clustering algorithm presented in the following section.

The fundamental underlying assumption is that stability of rigid parts results from stable interactions between its residues.
However, in our approach we do not analyze physical interactions between residues~--~they may be hydrophobic, electrostatic or significant in some other way.
We simply assume that the measure of strength of a contact between residues is reflected by their geometrical variability across a given sample of structures.

\section{Methods}
Throughout this paper we use terms: \emph{model}, \emph{configuration}, \emph{structure} and \emph{conformation} interchangeably.
We will refer to a set of structures acquired from an MD trajectory or NMR experiment as the \emph{ensemble of configurations} or simply: \emph{an ensemble}.
Let us denote the number of structures in an ensemble by $S$.

\subsection*{Residue contact}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{F1II}
\caption{
For each configuration in an ensemble, pairs of elements are constructed.
In this picture residues are in contact because there exists a configuration for which the condition~(\ref{condition}) is satisfied (the distance $d_C=7.4\angstrom$, and $d_\alpha-d_C = 1.8\angstrom$).
}
\label{contact}
\end{figure}

For each pair of residues in every model we compute distances between $C_\alpha$ atoms ($d_\alpha$) and between geometrical centers of side-chains $R_C$ ($d_C$) (for glycine $R_C = C_\alpha$, and for alanine $R_C = C_\beta$).
We say that two residues are \emph{in contact}, if at least one configuration in the ensemble satisfies the condition:
\begin{equation}
(d_\alpha \leq 6.5\angstrom)\quad \text{or} \quad (d_C \leq 8\angstrom\quad \text{and} \quad d_\alpha - d_C \geq 0.75\angstrom)
\label{condition}
\end{equation}
Threshold values are the same as in definition of contact presented in~\cite{daniluk2011novel} and relate to the range of distances in which physical interactions between residues occur.
The second sub-condition favours residues whose side-chains point towards each other (see~Figure~\ref{contact}). 
Residues that are sequential neighbors are not taken into account.

We assign a quantitative value to the strength of a contact in terms of geometrical variability of the structural part associated with that contact.
Such measure is required to capture not only changes in $d_\alpha$, but also rotational shifts and alterations in the backbone in the vicinity of both residues that are in contact.
To do so, we constructed structural parts, comprising sequential neighbors of the two residues in contact.
ResiCon assigns a numerical value to the geometric variability by using the least root mean square deviation (RMSD)~(\cite{kabsch1976solution}).
Before elaborating on the details, we proceed with the following definitions.

\subsubsection*{Elements.}
An \emph{element} is a structural part of a protein centered around a given residue.
It comprises five points, corresponding to the positions of the $C_\alpha$ atoms of the central residue, and its four sequential neighbors (two preceding and two following).
For each model $s$ in an ensemble, and each residue $i$, an element~--~denoted by $E_i^s$~--~is constructed.
Residues for which an element cannot be built (e.g. N- and C-termini) are omitted.

\subsubsection*{Geometrical variability.}
We consider pairs of elements, $E_{ij}^s := ( E_i^s , E_j^s )$, and express structural deviation of a contact between two configurations $r$ and $s$ by RMSD of $E_{ij}^r$ and $E_{ij}^s$.
We use the following function to express the strength of a contact in terms of the whole ensemble:
\begin{displaymath}
G(i,j) := \max_{\text{pairs of states }\\(r,s)} \text{RMSD}(E_{ij}^r,E_{ij}^s) .
\end{displaymath}
The smaller the geometric variability, the stronger the contact.

We tested several statistics based on RMSDs of pairs of elements.
This particular definition of $G$ assumes that a strong contact is not ``broken'' in any pair of models.
Conversely, a contact whose structural stability is breached at least once is assumed to be weak and not contributing to the stability of a given dynamic domain.

Note that conformational transitions may be rapid or insufficiently sampled.
Therefore, defining geometrical variability in terms of some averaging statistic (e.g. mean, median) might lead to omitting significant structural changes occurring in a protein.

\subsection*{Contact matrix}
We now describe a matrix representation of an edge-weighted graph, in which nodes correspond to residues.
Because we used a spectral clustering algorithm which required that weights in the graph were in the interval $[0,1]$, we needed to renormalize the geometrical variability.
We calculated the weight between node $i$ and $j$ using the following \emph{contact function}:
\begin{displaymath}
D(i,j) := \left\{ \begin{array}{ll}
1&  |i-j| \leq 1 \\
L_{\alpha,\beta}(G(i,j))& \text{residues {\it i} and {\it j} are in contact} \\
0& \textrm{otherwise}
\end{array} \right .
\end{displaymath}
where $L_{\alpha,\beta}$:
$$
L_{\alpha,\beta}(x):= \left ( 1 + e^{\frac{x-\alpha}{\beta} } \right )^{-1}
$$
is a logistic function.
We refer to matrix $[D(i,j)]_{ij}$~as~the~\emph{contact~matrix}.

Parameters $\beta>0$ and $\alpha>0$ allow for the customization of ResiCon.
The logistic function, $L_{\alpha,\beta}$, can be thought of as a rescaling transformation for the measure of geometrical variability, $G$.
It has a simple interpretation~--~values of $G$ exceeding $\alpha$ are smoothly cut-off, where the degree of ''smoothness'' is determined by $\beta$.
All results presented in this paper were acquired with default values of $\alpha$ and $\beta$:
\begin{equation}
\beta_{\text{default}} := \frac{1}{\sigma(G)} \qquad\qquad  \alpha_{\text{default}} := \mu(G)
\label{default}
\end{equation}
where the $\sigma$ and $\mu$ stand for standard deviation and mean taken over values of $G$ for all pairs of elements.

Another feature of the logistic function becomes apparent if we consider an ensemble composed of (nearly) identical structures.
This has two possible interpretations: the protein is very stable and no conformational changes exist, or that the provided ensemble does not represent such changes.
Thus, ResiCon will assume that the contacts are strong~--~nearly as strong as the peptide bonds ($G\approx 0 \Rightarrow L_{\alpha,\beta}\approx 1$).
Consequently, the contact matrix becomes a so-called \emph{contact map}, assigning binary values to pairs of residues (i.e. 1 if a contact occurred at least once, and 0 otherwise).

\subsection*{Clustering}

The contact matrix may be treated as a \emph{similarity matrix}, denoted $W$, and be used as input in a clustering procedure.
Thus, we consider residues to be similar if they are likely to belong to the same dynamic domain.
The identified clusters would then correspond to quasi-rigid structural parts.

The choice of a clustering algorithm is not a trivial task and for the identification of dynamic domains two crucial requirements need to be met.
Firstly, contact matrices for various proteins vary in dimension and density and the clustering algorithm needs to perform well regardless of these variabilities.
Secondly, the algorithm should facilitate an automated method of choosing the optimal number of clusters. 

Agglomerative hierarchical clustering algorithms are one of the most popular approaches to clustering~\cite{han2001data}.
They follow a greedy scheme to construct a dendrogram encoding distances between clusters.
This dendrogram can be cut at a certain height, which determines the number of clusters.
If the height parameter could be set so that for all similarity matrices we would obtain high-quality clusters, the hierarchical clustering would have been a good candidate for a clustering procedure.
However, as we explain in the \emph{Supplementary Materials}, estimation of this parameter is difficult, and to determine the number of clusters we would need to extend the conventional hierarchical clustering with a measure of cluster quality.

This was one of the reasons we have chosen a spectral clustering algorithm, which has an inherent indicator of a partitioning's quality.

\subsubsection*{Spectral clustering.}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{F2II}
\caption{
An example of a similarity matrix $W$ and three eigenvectors of the stochastic matrix $D^{-1}W$ with two different orderings.
}
\label{clustering}
\end{figure}

Clustering algorithms based on finding the eigensystem of the similarity matrix (or more often a matrix derived from it) are termed \emph{spectral algorithms}.
They perform a clustering by minimizing the cost of cutting a graph into subgraphs, which agrees with our intuition about finding quasi-rigid parts based on a contact matrix.
Optimal clustering is achieved by discarding contacts with the lowest total weight (as few and as weak as possible) to achieve a partitioning into unconnected regions.

In the case of the clustering algorithm used in ResiCon optimal partitioning is decoded from eigenvectors corresponding to the largest eigenvalues of a stochastic matrix $D^{-1}W$, where $D:=diag(d(1),\ldots,d(n))$ and $d(i):=\sum_{j=1,j\neq i}^{n} w(i,j)$ (see~\cite{weber2004perron}).
This transformation of the similarity matrix ensures that the identified clusters tend to have similar sizes, which prevents from identifying singular nodes as clusters.
Spectral algorithms make no assumptions on the shape of clusters, and, in contrast to the greedy procedures, are insensitive to the ordering of vertices.

In Figure~\ref{clustering} we present a $150\times 150$ similarity matrix with rows and columns ordered in two different ways.
The ordering on the right is dictated by the spectral clustering~--~elements $1\text{--}50$ were assigned to the first cluster, $51\text{--}90$ to the second, and $91\text{--}150$ to the third.
To find these three clusters we use the first three eigenvectors (corresponding to three largest eigenvalues) of the stochastic matrix $D^{-1}W$.
The first eigenvector always has a constant value in all positions and corresponds to a trivial clustering into a single group.
The second eigenvector encodes a partitioning into two groups: nodes $51\text{--}90$ in the first, and the remaining nodes in the second group.
The third eigenvector allows to discern the third cluster, composed of nodes $1\text{--}50$.

\subsubsection*{Clustering algorithm.}
We used a spectral clustering algorithm in which partitioning of a graph is expressed in terms of a membership matrix $\chi$.
A short description of the procedure is presented below, for details refer to~\cite{weber2004perron}.

Let $Y$ denote the matrix containing $k$ eigenvectors of the $D^{-1}W$ stochastic matrix, corresponding to $k$ largest eigenvalues.
The procedure computes a linear mapping $\mathcal{A}$ from the eigenvectors $Y$ to the membership matrix:
\begin{displaymath}
\mathcal{A} Y = \chi
\end{displaymath}
The element $\chi_{ij}$ of this matrix represents the membership of the $i^{\textrm{th}}$ node in the $j^{\textrm{th}}$ cluster.
Therefore, if $n$ is the number of nodes in the graph and $k$ is the number of clusters, then $\chi\in\mathbb{R}^{n\times k}$.

This algorithm has two important features:
\begin{itemize}
\item it computes the membership matrix $\chi$ which allows for overlapping clusters,
\item it offers an indicator, called $\chi_{\text{min}}$, used to determine the optimal number of clusters.
\end{itemize}

\subsubsection*{Number of clusters.}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{F3II}
\caption{
Values of the $\chi_{\text{min}}$ indicator for $k=2,\ldots,10$, computed for HIV-1 protease.
Because $\chi_{\text{min}}(4)$ is closer to 0 than $\chi_{\text{min}}(6)$, the optimal number of clusters is 4.
}
\label{minChiList}
\end{figure}
ResiCon first checks if $k=1$.
To do so, a partitioning into two clusters is carried out.
The spectral algorithm presented above finds an optimal cut~\cite{weber2004perron} leading to clusters $A$ and $B$ (two sets of indices, that correspond to nodes).
We express the \emph{cost} of such cut by:
\begin{displaymath}
f :=  \frac{\sum_{i \in A}\sum_{j \in B} w_{ij}}{\left ( \sum_{k,l \in \text{A}} w_{kl} \right ) \left ( \sum_{k,l \in \text{B}} w_{kl}   \right )} \\
\end{displaymath}
where $w_{ij}$ is the weight of the edge between nodes $i$ and $j$.
The validity of a clustering into clusters $A$ and $B$ was checked by asserting that its cost is less than a given threshold.
If the criterion was met, we assumed that a clustering into two or more clusters existed.
Default threshold for $f$ used to produce all results presented in this study was $0.1$. 
Above this value we observed that compact static proteins were partitioned into short (less than four residues long) segments, which we regarded as improper dynamic domains.
On the other hand, lower values resulted in a single dynamic domain assignment in several cases where two domains were apparent.

If $k>1$, the optimal number of clusters is determined with the use of the indicator presented in~\cite{weber2004perron}.
Here, we give a short overview of the properties of $\chi_{\text{min}}$ and propose a simple procedure for computing the optimal number of clusters.
The indicator $\chi_{\text{min}}(k)$ is defined as the minimal element of the membership matrix $\chi$ found by partitioning it into $k$ clusters.

In the case of $k=2$ the indicator is always zero, $\chi_\text{min}(2)=0$.
For $k>2$ the indicator is less than zero, however if $\chi_\text{min}(k)\approx 0$, the clustering into $k$ clusters is the optimal one (Figure~\ref{minChiList}).
Let us recall the notion of visualizing a clustering by a block-like similarity matrix.
Roughly speaking, the value of $\chi_\text{min}(k)$ resembles the deviation of the similarity matrix from the ``pure'' block-like form.
However, it is difficult to decide which values of $\chi_\text{min}$ are sufficiently close to zero to indicate the optimal number of clusters.
Therefore, the problem at hand is: does the optimal number of clusters equal two, or more?

In our first approach, the optimal number of clusters was chosen based on a threshold~--~the optimal $k$ was the one for which $\chi_{\text{min}}$ was above a certain value.
However, it was difficult to find the right threshold because values of $\chi_{\text{min}}$ strongly depend on the number of nodes in the graph.
Therefore, the following procedure was adapted in ResiCon:
\begin{enumerate}
\item Determine the cost of the optimal cut.
If it exceeds the 0.1 threshold, the optimal number of clusters is $k=1$.
Otherwise, assume that $k>1$ and continue the procedure.
\item Compute the values of $\chi_{\text{min}}$ for partitionings into $ 3,4,\ldots,M$ clusters.
\item Find $k_1>2$ for which $\chi_{\text{min}}$ is closest to 0, and $k_2>2$ for which $\chi_{\text{min}}$ is the second closest to 0.
\item If $\chi_{\text{min}}(k_1)>0.5\hspace{1mm}\chi_\text{min}(k_2)$, then the number of clusters is $k=k_1$.
Otherwise, $k=2$.
\end{enumerate}
The $0.5$ constant in the fourth point means that we choose $k_1$ as the number of clusters, if $\chi_\text{min}(k_1)$ is closer to $0$ then to the next best $\chi_\text{min}$ (i.e. $\chi_\text{min}(k_2)$).
The maximal number of clusters $M$ is set to $10$ by default, but the user can specify a different number.
All results presented in this paper were computed with $M=10$.

In other words, ResiCon chooses $k>2$ for which the indicator $\chi_\text{min}$ is ``relatively close'' to 0.
When no such value exists, a partitioning into two clusters is assumed to be optimal.
\subsection*{Hinges and interfacial regions}
\subsubsection*{Hinges}
We define hinges as parts of the structure satisfying both of the following conditions:
\begin{enumerate}
\item they do not belong conclusively (in terms of membership as explained below) to any dynamic domain ,
\item they are sequentially located between dynamic domains.
\end{enumerate}

The first condition is tested in terms of membership: if a residue membership in any cluster does not exceed certain threshold $\chi_\text{hinge}$ it may belong to a hinge. 
The default value of the parameter is $0.65$, but the user can specify a different value.

\subsubsection*{Interfacial regions}
A residue is assumed to compose an interfacial region if two conditions are met:
\begin{enumerate}
\item it does not belong to any hinge,
\item it was in contact with a residue that does not belong to the same dynamic domain at least once.
\end{enumerate}

\subsection*{Results comparison}
According to our knowledge, no expert curated database of dynamic domains exists.
Also, we are not aware of any quality measure for the dynamic domains assignment.
Therefore, we compared different methods by analyzing agreement between their results.

We used the measure presented in~\cite{meilua2007comparing}, called \emph{Variation of Information} ($\mathcal{VI}$) to analyze the results compatibility.
It has the advantage of being a metric in the space of all partitionings of a given dataset.
The downside of $\mathcal{VI}$ is that its values do not lie in a fixed interval (e.g. $[0,1]$), but instead have an upper bound that depends on the size of data.
In our case data size equals the number of residues in a given protein.
This means that values of $\mathcal{VI}$ for partitionings of one protein are not directly comparable to the values acquired for partitionings of another protein, with a different number of residues.
Nonetheless, when considering a particular protein, the $\mathcal{VI}$ metric quantifies the agreement between different assignments of dynamic domains.
Here we give an outline of the method, for details see~\cite{meilua2007comparing}.

Let us denote a clustering by $\mathcal{C}$.
It is composed of clusters~--~mutually disjoint subsets $C_1,\ldots,C_k$.
That is, $\mathcal{C} = \{ C_1,\ldots,C_k\}$ such that \mbox{$C_i \cap C_j = \emptyset$} for all pairs $i,j$.
Assume that the numbers of points in consecutive clusters are $n_1,\ldots,n_k$.
Then, the probability that a random point from the dataset belongs to the $i^{\textrm{th}}$ cluster is $$P(i):=\frac{n_i}{n},$$ where $n$ is the number of all points in the set.
Note that $\sum_{i=1}^k n_i = n$.
Analogously, let another clustering of the same set of points $\mathcal{C'}=\{ C_1',\ldots,C_{k'}' \}$ be composed of clusters with $n_1',\ldots,n_{k'}'$ points.
By $n_{ij'}$ we will denote the number of points assigned to cluster $i$ in clustering $\mathcal{C}$ and cluster $j'$ in $\mathcal{C'}$.
Then, $$P(i,j'):=\frac{n_{ij'}}{n} $$ is the probability of randomly choosing a point that belongs to both clusters.

The $\mathcal{VI}$ measure is defined in terms of entropy and joint entropy of the probability distributions defined above.
That is, if the entropy of clustering $\mathcal{C}$ is expressed by: $$H(\mathcal{C}) := -\sum_{i=1}^k P(i) \log_2 P(i),$$
and the joint entropy of two clusterings is given by: $$ H(\mathcal{C},\mathcal{C'}) := - \sum_{i=1}^k \sum_{j'=1}^{k'} P(i,j') \log_2 P(i,j'), $$
then the variation of information of the two clusterings is defined as: $$ \mathcal{VI}(\mathcal{C},\mathcal{C'}) := 2 H(\mathcal{C},\mathcal{C'}) - H(\mathcal{C}) - H(\mathcal{C'}) $$

\subsection*{Quality of dynamic domains}
We consider dynamic domains to be structural parts of the protein, which move with respect to each other, but remain internally rigid.
In order to assess which method for dynamic domains identification is better, a measure was required that would quantify the quality of a given assignment.
We did not find such a scoring function in the literature, and propose the following geometrical measure called \emph{total geometrical variability}:
$$
Q := \sum_{i=1}^k \max_{\text{pairs of states }\\(r,s)} \text{RMSD}(D^r_i,D^s_i),
$$
where $D^s_i$ is the set of $C_\alpha$ atoms comprising the domain $D_i$ in the state $s$, and $k$ is the number of domains.
Smaller values of $Q$ indicate higher quality.


Note that typically, if a domain is structurally rigid, the maximal RMSD for that domain is smaller than the sum of maximal $\text{RMSDs}$ of its two subsets.
Therefore the proposed measure favors large, compact domains.
It is also worth noting that for a trivial dynamic domain (single residue) the RMSD is undefined.
We set its value to 0, although this artificially reduces $Q$ (see \emph{Quality analysis}).


\section{Results and discussion}
\begin{table*}[b]
\hspace{-0.7cm}
\includegraphics[width=1.1\linewidth]{T1II}
\caption{Summary of results produced by GeoStaS, ResiCon and PiSQRD.
Dynamic domains shown for PiSQRD are those for which the lowest value of $Q$ was achieved.}
\label{wholeTable}
\end{table*}

\begin{table*}[b]\footnotesize
%\centering
\sffamily
\renewcommand{\arraystretch}{1}
\hspace{-1.2cm}
\begin{tabular}{c >{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{1cm}>{\centering\arraybackslash}p{1cm}>{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm}>{\centering\arraybackslash}p{1cm}>{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm}>{\centering\arraybackslash}p{1cm}>{\centering\arraybackslash}p{1cm}} 
& {\bf \textsf{ResiCon vs. GeoStaS}} & \multicolumn{3}{c}{{\bf \textsf{ResiCon vs. PiSQRD}}} & \multicolumn{3}{c}{{\bf \textsf{GeoStaS vs. PiSQRD}}} & \multicolumn{3}{c}{{\bf \textsf{PiSQRD vs. PiSQRD}}} \\ 
& $\mathcal{VI}$ & $\textsf{r}_\textsf{25\%}$ & $\textsf{r}_\textsf{50\%}$ & $\textsf{r}_\textsf{75\%}$ & $\textsf{r}_\textsf{25\%}$ & $\textsf{r}_\textsf{50\%}$ & $\textsf{r}_\textsf{75\%}$ & $\langle \textsf{r}_\textsf{25\%} \rangle$ & $\langle \textsf{r}_\textsf{50\%} \rangle$ & $\langle \textsf{r}_\textsf{75\%} \rangle$ \\ 
\cline{1-11}
\texttt{2rgf} & 0.62 & 0.45 & 0.48 & 0.57 & 0.26 & 0.26 & 0.32 & 0.01 & 0.14 & 0.20  \\
\texttt{2pas} & 1.94 & 2.43 & 2.45 & 2.47 & 2.00 & 2.08 & 2.11 & 0.61 & 0.96 & 1.23  \\
\texttt{1aey} & 0.00 & 1.99 & 2.33 & 2.37 & 1.99 & 2.33 & 2.37 & 0.77 & 1.10 & 1.52  \\ 
\texttt{1pkt} & 0.00 & 2.07 & 2.24 & 2.36 & 2.07 & 2.24 & 2.36 & 1.38 & 1.63 & 1.84  \\ 
\texttt{4a5v} & 1.00 & 1.08 & 1.12 & 1.19 & 1.53 & 1.54 & 1.60 & 0.24 & 0.29 & 0.38  \\ \cline{1-11}
\texttt{1pit} & 1.59 & 1.76 & 2.05 & 2.14 & 2.42 & 2.55 & 2.74 & 1.35 & 1.70 & 1.94  \\ 
\texttt{2vil} & 1.89 & 1.19 & 1.39 & 1.89 & 2.31 & 2.49 & 2.67 & 1.11 & 1.36 & 1.57  \\ 
\texttt{1aiw} & 1.64 & 1.66 & 1.80 & 2.13 & 2.58 & 2.69 & 2.77 & 1.30 & 1.53 & 1.90  \\ 
\texttt{2ktf} & 0.80 & 1.69 & 2.01 & 2.06 & 2.15 & 2.36 & 2.48 & 0.93 & 1.17 & 1.74  \\ 
\texttt{1vve} & 1.05 & 0.00 & 0.00 & 0.00 & 1.05 & 1.05 & 1.05 & 0.62 & 0.62 & 0.62  \\ \cline{1-11}
\texttt{3mef} & 0.32 & 1.30 & 1.43 & 1.50 & 1.21 & 1.32 & 1.40 & 0.64 & 0.86 & 1.09  \\ 
\texttt{1vvd} & 1.24 & 0.21 & 0.21 & 0.21 & 1.26 & 1.26 & 1.26 & 0.31 & 0.31 & 0.31  \\ 
\texttt{2spz} & 0.13 & 1.42 & 1.62 & 1.94 & 1.37 & 1.59 & 1.96 & 1.28 & 1.60 & 1.73  \\ 
\texttt{1leb} & 0.61 & 2.05 & 2.27 & 2.41 & 2.33 & 2.37 & 2.45 & 0.92 & 1.21 & 1.52  \\ 
\texttt{2ait} & 0.71 & 1.26 & 1.65 & 1.82 & 1.49 & 1.67 & 1.99 & 0.99 & 1.31 & 1.52  \\ \cline{1-11}
\texttt{2k3c} & 1.86 & 0.82 & 0.97 & 0.98 & 1.61 & 1.69 & 1.69 & 0.16 & 0.42 & 0.63  \\ 
\texttt{1cfc} & 1.60 & 0.41 & 0.44 & 0.97 & 1.41 & 1.48 & 1.86 & 0.77 & 0.92 & 1.30  \\ 
\texttt{1a67} & 1.07 & 1.90 & 1.96 & 2.06 & 2.22 & 2.38 & 2.69 & 1.58 & 1.93 & 2.12  \\ 
\texttt{3egf} & 0.14 & 1.30 & 1.40 & 1.43 & 1.29 & 1.41 & 1.45 & 0.63 & 0.91 & 1.14  \\ 
\texttt{2pni} & 1.60 & 1.16 & 1.58 & 1.78 & 2.00 & 2.25 & 2.35 & 1.18 & 1.44 & 1.65  \\ \cline{1-11}
\texttt{1zda} & 0.79 & 1.37 & 1.55 & 1.96 & 0.83 & 1.28 & 1.78 & 1.26 & 1.54 & 1.75  \\ 
\texttt{1adr} & 0.84 & 0.80 & 1.30 & 1.81 & 1.11 & 1.64 & 2.22 & 1.19 & 1.57 & 1.94  \\ 
\texttt{1yug} & 1.79 & 0.90 & 1.00 & 1.31 & 1.98 & 2.12 & 2.15 & 0.70 & 0.99 & 1.29  \\ 
\texttt{1d1d} & 1.47 & 0.00 & 0.21 & 3.15 & 1.47 & 1.51 & 3.80 & 1.08 & 3.08 & 3.17  \\ 
\texttt{2l14} & 0.55 & 0.49 & 0.57 & 0.75 & 0.91 & 0.98 & 1.18 & 0.61 & 0.71 & 0.95  \\ \cline{1-11}
\texttt{1bf8} & 1.61 & 0.74 & 1.27 & 3.22 & 1.69 & 1.89 & 4.20 & 1.13 & 1.74 & 2.99  \\ 
\texttt{2htg} & 0.00 & 1.36 & 1.58 & 1.91 & 1.36 & 1.58 & 1.91 & 0.84 & 1.13 & 1.30  \\ 
\texttt{1qo6} & 0.88 & 0.63 & 0.72 & 0.88 & 0.87 & 0.98 & 1.18 & 0.57 & 0.71 & 0.94  \\
\texttt{2k0e} & 0.97 & 0.59 & 0.66 & 0.82 & 1.05 & 1.16 & 1.31 & 0.72 & 0.87 & 1.03  \\ 
\texttt{2kr6} & 1.11 & 0.79 & 0.81 & 0.84 & 0.85 & 0.87 & 0.93 & 0.61 & 0.68 & 0.82  \\ 
\end{tabular}
\normalfont
\vspace{.25cm}
\caption{Discrepancies in assignments expressed by radii of balls encompassing 25\%, 50\% and 75\% of results.
The ordering of the results is the same as in Figure~\ref{boxWhiskers}, i.e. best-scoring results are first.}
\label{results}
\end{table*}


We compared ResiCon with two recent methods: GeoStaS~\cite{romanowska2012determining} and PiSQRD~\cite{potestio2009coarse}.
The latter method represents the class of methods which identify dynamic domains by analyzing a single structure.
We used a test set comprising 30 NMR-resolved protein structures exhibiting significant mobility, which was previously used in~\cite{snyder2005clustering,kirchner2011objective}, and in~\cite{romanowska2012determining}.
The set was initially proposed to examine the efficacy of a method of identifying structurally-stable cores in flexible proteins.
These structures often contain a single rigid core with significant geometrical distortions present in peripheral regions (as indicated in \cite{snyder2005clustering} and also observable in ResiCon's results). 

We have also used ResiCon to analyze a canonical test case~--~the HIV-1 protease molecule~--~using an MD trajectory computed with a coarse-grained force field RedMD~\cite{gorecki36causality,gorecki2009redmd} as input data.
This example shows that ResiCon is capable of finding dynamic domains of a protein whose functionality depends on flexibility and mobility of its rigid parts.

We also explain the so-called \emph{zebra effect}~--~a peculiar result produced by ResiCon, observed in several cases.
This effect is especially strong when a suboptimal numer of clusters is chosen.

\subsection*{Comparative analysis}



\begin{figure}
\centering
\includegraphics[width=\linewidth]{F4II}
\caption{Values of $\mathcal{VI}$ for partitionings of the \texttt{1d1d} protein molecule.
A pair of results produced by PiSQRD which had the highest $\mathcal{VI}$ are denoted by P1 and P2.}
\label{comparison}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{F5II}
\caption{Box and whiskers plot of the dynamic domains quality score $Q$ for ResiCon, GeoStaS, PiSQRD.
In blue are $Q$ values for PiSQRD's dynamic domains determined from structural covariance matrices (see \emph{Supplementary Materials}).}
\label{boxWhiskers}
\end{figure*}

ResiCon and GeoStaS are both designed to work on an ensemble of structures, and produce a single partitioning into dynamic domains.
Both methods impose no assumptions about sampling and order of provided conformations.
ResiCon uses maximal local distortions computed over all pairs of frames, indifferent to over- or undersampling of configurations, as long as they are present in the ensemble.

However, the PiSQRD server by default analyzes a single structure and estimates the so-called \emph{low-energy modes}, which are the eigenvectors of the structural covariance matrix (under the canonical ensemble, i.e. assuming Boltzmann distribution of configurations).
These low-energy modes are assumed to carry the information relevant to dynamic domains identification.
We observed that the choice of an input structure influences the results significantly, but there is no definite criterion for choosing the \emph{right} structure for the analysis.
The PiSQRD server provided with a PDB file containing NMR models by default finds dynamic domains for the first model.

This might introduce a bias unfavorable for PiSQRD's performance. 
We, therefore, decided to examine results produced by PiSQRD for every structure in the ensemble in order to compare ResiCon against its full capacity.
The best dynamic domains were chosen and presented together with the results produced by ResiCon and GeoStaS in Table~\ref{wholeTable}.

PiSQRD also gives a possibility of providing a set of low-energy modes extracted from a structural covariance matrix estimated from a set of structures, but this procedure is not straightforward and requires additional assumptions (see \emph{Supplementary Materials}).
We scrutinize the quality of dynamic domains found by PiSQRD from user-provided low-energy modes in section \emph{Quality analysis}.

Because of a large number of models, it was not possible to provide a graphical representation for each partitioning.
We therefore focused on values of the agreement measure $\mathcal{VI}$ between results produced by the three methods.

To familiarize the Reader with values of the measure $\mathcal{VI}$, we take a look at two most distant partitionings produced by PiSQRD (denoted by P1 and P2) for an exemplary \texttt{1d1d} protein, and how they relate to the results given by ResiCon (R) and GeoStaS (G) (Figure~\ref{comparison}).
The clustering P2 seems to be very similar to the one produced by ResiCon, while P1 gives a sliced-and-diced picture of the protein's mobility.
It seems that a partitioning produced by PiSQRD depends on physical properties embedded in a particular configuration.
Although NMR models carrying information about dynamic domains may exist, others lead to chaotic partitionings.
In order to produce a single clustering, PiSQRD would need a procedure for interpreting physical properties based on an ensemble of configurations.

Nearly 50\% of the assignments produced by PiSQRD for \texttt{1d1d} are coherent with the clustering given by ResiCon (see \emph{Supplementary Materials}).
On the other hand, the result given by GeoStaS differs from all results produced by PiSQRD.
In fact, ResiCon and PiSQRD are more coherent than GeoStaS and PiSQRD, which can be expressed by the mean of $\mathcal{VI}$.
However, it would be naive to use the mean value as an indicator of self-consistency of PiSQRD.
Among partitionings produced by PiSQRD, $N(N-1)/2$ agreements were calculated (where $N$ is the number of models of \texttt{1d1d}), whereas the comparison of ResiCon or GeoStaS with PiSQRD gave $N$ values of $\mathcal{VI}$.
Consequently, the average value of $\mathcal{VI}$ between PiSQRD clusterings is not directly comparable with the average value of $\mathcal{VI}$, e.g. for ResiCon vs. PiSQRD.

Therefore, to give a better picture of the divergence of results we exploit the fact that $\mathcal{VI}$ is a metric in the space of all clusterings.
For a given partitioning we computed the radius of a ball %(in the sense of a set, using $\mathcal{VI}$ as a metric)
 centered at that partitioning, encompassing a certain fraction of the results.
In Table~\ref{results} we provide values of radii of balls which encompass 25, 50 and 75\% of the results.
In the case of PiSQRD, we constructed balls (for a given percentage of results) centered at each partitioning, and computed the mean values of their radii.
The mean of radii does not carry the bias mentioned earlier.
Additionally, histograms of $\mathcal{VI}$ for each protein can be found in \emph{Supplementary Materials}.

\subsection*{Quality analysis}

The box-and-whisker plot in Figure~\ref{boxWhiskers} provides a concise picture of quality scores $Q$ (see \emph{Methods}) of the dynamic domains assignments.
It clearly shows that beyond a few exceptions ResiCon gives the best results.
There are 7 notable exceptions: \texttt{4a5v}, \texttt{2ktf}, \texttt{3egf}, \texttt{1adr}, \texttt{1bf8}, \texttt{2htg} and \texttt{2kr6}.

The single dynamic domains found by PiSQRD (blue in Figure~\ref{boxWhiskers}) were produced using low-energy modes, computed as eigenvectors of a structural covariance matrix estimated by superimposing all models in an ensemble on a representative structure.
In the \emph{Supporting Materials} we explain how this representative structure is chosen, and show qualities of dynamic domains found by PiSQRD using different methods of estimating the structural covariance matrix.
It should be emphasized, however, that these dynamic domains strongly depend on the method of estimating the structural covariance matrix, which is not part of PiSQRD's functionality.
Therefore, these results should only be treated as an additional insight into what the user can expect from a more complex analysis of NMR structures.

In the case of \texttt{1adr} GeoStaS gave partitionings with lower $Q$, by introducing a trivial domain (cutting off C- and N-termini~--~see \emph{Supplementary Materials}).
On the other hand, for \texttt{4a5v} GeoStaS identified a single domain, which gave a lower value of $Q$ than any other partitioning.

For proteins \texttt{2ktf}, \texttt{3egf} and \texttt{2htg}, PiSQRD produced results with lower $Q$ than ResiCon, by finding numerous small quasi-rigid fragments.
However, for \texttt{2ktf} more than 25\% of assignments found by PiSQRD contained a trivial, single-residue domain (indicated in red, with an asterisk).
Although our measure does not penalize for this, we consider such behavior undesirable.
Also, note that proteins \texttt{3egf} and \texttt{2htg} comprise 53 and 27 residues accordingly.
It seems that in case of small proteins ResiCon often identifies a single domain, which does not necessarily result in the lowest $Q$.

The \texttt{2kr6} protein is an interesting example. 
It contains a flexible linker composed of more than 30 residues.
As a consequence, partitionings with high values of $Q$ are observed.
Only PiSQRD was able to produce lower values of $Q$, by cutting the linker into many shorter parts.
The example of the \texttt{2kr6} protein shows, that ResiCon does not consider unstructured regions to be separate dynamic domains.
Instead, residues constituting linkers and lacking long-distance contacts, are assigned to dynamic domains which are closest in sequence.

\subsection*{Comments}
Although PiSQRD's capability of analyzing a single structure may be considered an advantage, results presented in Table~\ref{results} and Figure~\ref{boxWhiskers} show that there is a large discrepancy for different configurations of the same protein.
Nevertheless, based on the histograms of $\mathcal{VI}$ presented in \emph{Supplementary Materials} and the values of radii in Table~\ref{results}, we conclude that in most cases results given by ResiCon and PiSQRD are mutually more coherent, than GeoStaS and PiSQRD (e.g. \texttt{1cfc}, \texttt{1qo6}, \texttt{1vvd}, \texttt{1vve}, \texttt{1yug}, \texttt{2k3c}).
Notable exceptions are: \texttt{2rgf}, \texttt{2pas}, \texttt{3mef} and \texttt{1zda}. 
For these proteins ResiCon did not find any significant structural transitions and achieved the lowest value of $Q$ (see Figure~\ref{boxWhiskers}) by assigning a single domain.

Dynamic domains found by ResiCon are generally larger (particularly: \texttt{2k3c}, \texttt{1d1d}, \texttt{2k0e} and \texttt{2kr6}).
Conversely, GeoStaS often allocates flexible N- and C-terminal parts (e.g. \texttt{1adr}, \texttt{1qo6}, \texttt{1vve}, \texttt{3egf}) as quasi-rigid parts.
The size of dynamic domains is also the main difference between ResiCon and PiSQRD.
In case of small, static proteins (such as \texttt{1aey}, \texttt{1pkt}, \texttt{1pit}, \texttt{2spz}, \texttt{2ait}, \texttt{3egf} and \texttt{1zda}), PiSQRD identifies numerous small and often trivial dynamic domains. 
ResiCon on the other hand detects no significant conformational changes (by analyzing an ensemble), and assigns a single dynamic domain.
We observe that in many cases ResiCon identified a single domain which had the lowest value of $Q$ among all partitionings (see \texttt{2rgf}, \texttt{2pas}, \texttt{1aey}, \texttt{1pkt}, \texttt{1pit}, \texttt{2vil}, \texttt{1aiw}, \texttt{3mef}, \texttt{2spz}, \texttt{1leb}, \texttt{2ait}, \texttt{2pni}, \texttt{2l14}).
In these cases ResiCon correctly detected that no significant transitions were present in the protein.
Therefore, unlike PiSQRD and GeoStaS, ResiCon can reliably indicate whether conformational changes occur in an ensemble of structures.

It is also noteworthy that ResiCon does not employ any post-processing procedures.
This keeps the algorithm simple and clean, but results in a discontinuity of certain partitionings (see \texttt{4a5v}, \texttt{1bf8} and \texttt{1vvd}), which we refer to as the \emph{zebra effect}.
Although this seems to be an artifact of the clustering algorithm, we will take a closer look at this effect and show that it may also carry valuable information referring to the protein's dynamics.

\subsection*{HIV-1 protease}
An analysis of an MD trajectory of the HIV-1 protease showcases ResiCon's capabilities.
This protein undergoes substantial conformational changes associated with opening/closing of its structural parts, so-called \emph{flaps}~\cite{hamelberg2005fast}.
A database of X-ray-resolved structures, representing configurations which the protease can attain is available~\cite{vondrasek2002hivdb}.

We examined a set of configurations of the HIV-1 protease acquired from a simulation carried out using the RedMD package~\cite{gorecki2009redmd}.
This coarse-grained force field was designed to simulate intramolecular motions in proteins and nucleic acids.
It has correctly predicted the flap-opening motion in the HIV-1 protease, which is known biological fact~\cite{hamelberg2005fast}, and was independently confirmed by an all-atom MD simulation~\cite{sadiq2010explicit}.
Roughly, about 1\% of the trajectory seems to exhibit significant conformational transitions (flap opening events).
This is a typical scenario in MD simulations, where a transition between meta-stable states is swift and fairly short.
We needed a set of representative structures in order to find dynamic domains using ResiCon.
From the whole trajectory, we extracted a set of 200 configurations using a generic procedure facilitating the Principal Component Analysis (PCA), implemented in the R programming language (in the \texttt{bio3d} package~\cite{grant2006bio3d})~--~see \emph{Supplementary Materials}.

Values of $\chi_\text{min}$ for different numbers of clusters are given in Figure~\ref{minChiList}.
The optimal number of clusters according to our procedure is 4.
Figure~\ref{hiv} depicts the dynamic domains found by ResiCon and two representative configurations of the protease, as well as results from GeoStaS and PiSQRD.
Because the sample of configurations was drawn according to the Boltzmann distribution, we were able to straightforwardly estimate the structural covariance matrix and provide PiSQRD with well-founded low-energy modes, and acquire a high-quality partitioning into two domains.
Results produced by GeoStaS were acquired from the whole trajectory of the protease.

Dynamic domains identified by ResiCon have the highest value of the $Q$ measure.
Note that ResiCon does not try to minimize Q, but to produce a clustering with the optimal value of the $\chi_\text{min}$ indicator.
Consequently, at the cost of a slightly higher $Q$ (ca. $1\AA$) we arrive at a partitioning which corresponds to the biologically relevant sub-division of the protein.
It is also worth mentioning that ResiCon's partitioning into $k=2$ clusters incidentally leads to an identical assignment as the one produced by PiSQRD.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{F6II}
\caption{
Two representative conformations of HIV-1 protease with flaps being closed and open, and summary of results produced by GeoStaS, ResiCon and PiSQRD.
Structures are colored according to the dynamic domains detected by ResiCon.
}
\label{hiv}
\end{figure}
Motions of the flaps between the closed and open states are crucial in the functionality of the HIV-1 protease.
It is also known that throughout their motion they remain quasi-rigid~\cite{freedberg2002rapid}.
Therefore, using ResiCon, we successfully extracted a simplified picture of the mobility of HIV-1 protease, which agrees with experimental knowledge.

Alternative partitioning into 6 clusters (see Figure~\ref{6clusters}) also deserves interest.
\begin{figure}
\centering
\includegraphics[width=\linewidth]{F7II}
\caption{
More subtle division emerges when the number of clusters is set to~6.
}
\label{6clusters}
\end{figure}
Apart from the flaps, additional dynamic domains resembling ``arms'' carrying the flaps can be observed.
Note that conversely to the case of the four dynamic domains, the quasi-static regions are similar, but not ideally reflective.
This indicates that motions of the two centrally symmetric sub-units of the HIV-1 protease in the provided trajectory were slightly different.

It can be also seen that domains are discontinuous.
Especially residues belonging to the N-terminal lobe (yellow) and to the arm (cyan) are interleaved.
In the next section we will analyze this effect, and show that this partitioning also carries valuable information.

\subsection*{The zebra effect}
Dynamic domains found by ResiCon may include residues that seem to be pulled out from another quasi-rigid fragment.
This is indicated as discontinuities (stripes) in Figure~\ref{6clusters}.
Let us take a look at an example of such an extracted residue and try to understand the source of this effect.
\begin{figure}
\centering
\includegraphics[width=\linewidth]{F8II}
\caption{
From the analysis carried out by ResiCon one can see that throughout the trajectory ASN-83 (magenta) not only remains close to residues in the N-terminal arm domain (gray) but also fills a cavity in that dynamic domain.
}
\label{eradicated}
\end{figure}
ResiCon assigned the ASN-83 residue to the arm (gray) dynamic domain (see Figure~\ref{eradicated}).
However, its sequential neighbors belong to the lobe domain (yellow).
The reason of this discontinuity is that, although ASN-83 has peptide bonds with its yellow neighbors, they are outweighted by contacts with the gray residues (see Figure~\ref{eradicated}).
The discontinuity suggests that throughout the trajectory ASN-83 remained docked in the cavity of the arm domain, while its peptide bonds formed an axis of a hinge.

It seems that the zebra effect is not accidental, and that it can be used to find residues which act as ``pivot points''. 
However, this effect is volatile and depends on small fluctuations in the input.
Residues are assigned to dynamic domains based on the fuzzy membership matrix.
At certain positions values of membership to different domains may be almost equal, and when discretized cause emergence of stripes.
Therefore, to strengthen the identification of these ``pivot point'' residues, a more thorough analysis of the membership matrix is required.

\section{Conclusions}
Typically, protein structures are flexible and mobile, and their conformational changes may be crucial in facilitating signaling and metabolical processes in which they participate.
Breaking a structure into dynamic domains may be compared to discovering gears and levers connected by cogs and pegs analogous to those found in classical machines.
Such analyses allow us to better understand molecular mechanisms responsible for biological functions (e.g. \cite{taylor2013classification}), facilitate MD simulations in large time-scales with simplified forcefields (e.g. \cite{sinitskiy2012optimal}), or discover potential binding sites when designing inhibitors (e.g. \cite{zhang2009defining}).

We have presented a universal tool for discovering dynamic domains in proteins.
ResiCon is capable of analyzing a single structure, or an NMR ensemble of structures provided in a PDB format.
It can also be applied to the set of independently obtained structures (e.g. X-ray crystallographic structures obtained under different conditions).
In any case, it provides a complete set of results comprising partitioning of the molecule into dynamic domains with additional highlighting of residues composing hinges and interfacial regions.
ResiCon also provides an indicator of partitioning quality, and suggests the optimal number of domains.

We tested our method using the reference set of NMR structures.
It is comparable or better than the recently developed GeoStaS and PiSQRD.
Apart from giving more compact dynamic domains, it is also capable of distinguishing structures composed of a single quasi-rigid region.

We have made ResiCon available online.\footnote{http://dworkowa.imdik.pan.pl/EP/ResiCon}
To make the analysis feasible and limit the number of uploads, queries may be rerun with changed parameters.
Also, all queries and results are stored on our server.
%\printbibliography[segment=1,heading=subbibliography]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% CHAPTER III

\definecolor{background-color}{gray}{0.98}


% my definitions BEGIN
\newenvironment{packeditemize}
{ \begin{itemize}
    \setlength{\itemsep}{1.5pt}
    \setlength{\parskip}{1.5pt}
    \setlength{\parsep}{1.5pt}     }
{ \end{itemize}                  } 

\newenvironment{packedenum}
{ \begin{enumerate}
    \setlength{\itemsep}{1.5pt}
    \setlength{\parskip}{1.5pt}
    \setlength{\parsep}{1.5pt}     }
{ \end{enumerate}                  } 


\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\newcolumntype{Y}{D..{6.1}}
% my definitions END

  
\chapter{Towards the identification of molecular cogs}

The text of Chapter 3 largely contains material published in the \emph{Journal of Comuptational Chemistry}%~\cite{dziubinski2015toward}.
The co-author, Bogdan Lesyng, made the following contributions: ...

\subsubsection{Motivation and set-up}

Computer simulations of molecular systems allow determination of microscopic interactions between individual atoms or groups of atoms, as well as studies of intramolecular motions. 
Nevertheless, the identification of causal relations hidden within such transformations is very difficult.
In the thermodynamic analytical frame, structural and functional properties of molecules are related to their free energy changes.
Therefore, in order to better understand structural and functional properties of molecular systems, it is required to deepen our knowledge of free energy contributions arising from molecular subsystems in the course of structural transformations.

In this chapter, we present a method of quantifying energetic contribution of each pair of atoms to the total free energy change along a given collective variable.
Next, with the help of a genetic clustering algorithm, we proposes a division of the system into two groups of atoms referred to as \emph{molecular cogs}.
Atoms which cooperate to push the system forward along a collective variable are referred to as \emph{forward cogs}, and those which work in the opposite direction as \emph{reverse cogs}.

\section{Introduction}

Molecular dynamics (MD) simulation methods and advanced algorithms for calculating free energy bring us closer to predicting the physical properties of biomolecules \cite{woo2005calculation,chipot2014frontiers,comer2014adaptive}.
However, computer simulations are not limited to interpreting experimental results.
\emph{In silico} one may also process MD data which can provide much more detailed information than that accessible in any experiment.
The key to a deeper understanding of complex molecular systems is the extraction of valuable information from data produced in such simulations.

Biomolecules carry out their functions through conformational transitions between meta-stable states.
Such phenomena can be simplified and described by a selected reaction coordinate which, in many cases, is a collective variable~\cite{chipot2007free}.
A valuable result of many simulation procedures (such as Umbrella Sampling~\cite{torrie1977nonphysical}, Thermodynamic Integration~\cite{kirkwood1935statistical}, Adaptive Biasing Force~\cite{comer2014adaptive}, and others\cite{gumbart2012standard}), is a free energy profile, the so-called Potential of Mean Force (PMF)~\cite{kirkwood1935statistical}.
{\color{black}In this study, the discussion is limited to one-dimensional PMFs, although it should be noted that the aforementioned methods can be formulated more generally and produce multi-dimensional free energy profiles.}
Of course, a one-dimensional PMF may be an oversimplification of what is going on in a complex system, but a meaningful collective variable usually leads to a free energy profile which makes the complicated transition more comprehensible~\cite{periole2012structural,chipot2014frontiers}.
However, what the PMF does not provide is the information about what drives transitions between meta-stable states.

Various attempts to understand and describe the internal mechanics of {\color{black}molecular systems} have already been reported~\cite{arnautova2011development,bao2009protein,chiang2013molecular,lavery2007protein,sacquin2014motions,van1997engineering,seifert2013protein}.
We did not, however, find any general-purpose approach for studying a broader spectrum of cases, and in particular a methodology explaining the cause of a transition in terms of free energy contributions arising from certain parts of a molecule.

In this study we propose a new approach of analyzing the tendencies of a {\color{black}molecular} system to undergo a selected structural transition.
The main idea is to look at a shift along a collective variable as an effect of two opposing tendencies generated by interactions within the molecule.
Our method {\color{black}indicates} two groups of atoms~--~referred to as \emph{molecular cogs}~--~which, through cooperative interactions, are the source of these tendencies.
For this purpose, we construct undirected graphs with weights between nodes expressed by energetic contributions to the free energy change arising from pair-wise interactions between atoms.
{\color{black}
These graphs are then partitioned into subsystems~--~corresponding to molecular cogs~--~using a genetic clustering algorithm.
We present results for small, model systems which served as case-studies for the identification of molecular cogs and for testing if their functioning agrees with our intuitions.
}

%------------------------------------------------

\section{Methodology}


Decomposition of the Helmholtz free energy, $A$, was investigated in the past, most notably by Karplus and coworkers\cite{boresch1995meaning,brady1995decomposition}.
{\color{black}The aim was the determination of contributions to the free energy coming from components comprising the potential energy of the system.
These potential energy components might come from different interaction types, or from cooperation between subsystems of the whole molecule.}
The strategy of expressing the potential energy of a system as a sum of terms, $U=\sum_i U_i$, and {\color{black}computing} the contribution of each of these terms to the free energy is ineffective because additivity in the potential energy components does not imply additive contributions to the entropy\cite{dill1997additivity}.

Two attempts {\color{black}of describing contributions to the free energy coming from parts of the system} are worth mentioning.
The first one was an approximate approach based on Free Energy Perturbation, in which higher order terms were neglected \cite{brady1995decomposition}.
These terms are not, however, negligible, which severely limits the applicability of this method.
An alternative route, employing Thermodynamic Integration, was also explored \cite{boresch1995meaning}. 
However, this approach is also limited, namely~--~values of the free energy contributions depend on the choice of the integration path.

{\color{black}
We propose a different approach, in which pair-wise energetic contributions to the free energy are readily attained.
Alas, as in the aforementioned methods of free energy decomposition, our current formulation struggles with a description of the entropic contributions, and at the present is not included in our method.
The current implementation is primarily applicable to small molecules for which the role of entropy in structural transitions is negligible (see an example of a PMF in the \emph{Results} section).
}

Our analysis originates from the following formula \cite{carter1989constrained}:
\begin{equation}
\label{eq:dPMFdKsi}
A'(\xi^*)  =  \langle m_\xi \nabla U\cdot \mathbb{M}^{-1}\nabla\xi\rangle_{\xi^*}  - \langle \mathbf{v}\cdot\nabla(m_\xi\nabla\xi)\mathbf{v} \rangle_{\xi^*},
\end{equation}
where $A'$ is the derivative of the free energy with respect to the collective variable $\xi$, $\mathbb{M}^{-1}$ is a diagonal matrix of inverse masses, and $\mathbf{v}$ are velocities.
The term $m_\xi$ is defined by:
\begin{equation}
m_\xi := \left[ \sum_i m_i^{-1} \left(\frac{\partial\xi}{\partial\mathbf{x}_i}\right)^{2} \right]^{-1} 
\end{equation}
and can be interpreted as inertia of an effective point mass moving along the $\xi$ coordinate.

On the right-hand side of Equation~(\ref{eq:dPMFdKsi}) we have, respectively, energetic and entropic contributions to the free energy change, both expressed as conditional averages, with the collective variable fixed at $\xi^*$.
In the \emph{Supporting Information} we briefly explain how these averages are estimated \emph{via} constrained Molecular dynamics (cMD) simulations, and highlight an important limitation of this method.

Note, that if we consider the potential energy as a sum of interaction components:
\begin{equation}
\nonumber U = U^{ele}+U^{vdw}+U^{bond}+\ldots =  \sum_{i}^I U^I 
\end{equation}
the  energetic contribution in Equation~(\ref{eq:dPMFdKsi}) maintains this additivity:
\begin{equation}
\langle m_\xi  \nabla U\cdot \mathbb{M}^{-1} \nabla\xi\rangle_{\xi^*} =  \sum_i^{I} \langle m_\xi \nabla U^I\cdot \mathbb{M}^{-1}\nabla\xi\rangle_{\xi^*} ,
\end{equation}
as was noted by Chipot et al.~\cite{chipot2007free}.

In our numerical experiments we used a force field with the following set of interaction types:
\begin{packeditemize}
 \item \emph{ele} (electrostatic interactions);
 \item \emph{vdw} (van der Waals interactions);
 \item \emph{bond} (2-body bonded interactions);
 \item \emph{angl} (3-body angle interactions);
 \item \emph{tors} (4-body torsional interactions).
\end{packeditemize}
We shall also consider:
\begin{packeditemize} 
 \item \emph{nbd} (non-bonded interactions, the sum of \emph{ele} and \emph{vdw} interactions);
 \item \emph{conf} (conformational interactions, composed of \emph{bond}, \emph{angl} and \emph{tors} interactions);
 \item \emph{total} (the sum of \emph{nbd} and \emph{conf} interactions).
\end{packeditemize}

\subsection{Decomposition of the energetic contribution}\label{sec:decomposition}
Our purpose was to identify the molecular cogs{\color{black}, i.e.} sets of atoms, which cooperate and push the whole system forward/backward along a reaction coordinate.
We approached this problem by converting it {\color{black}in}to the task of finding clusters in a graph.
Nodes in such a graph correspond to atoms, whereas edges represent cooperation of pairs of atoms.
A natural measure of such cooperation can be introduced by taking into account the energetic contribution to the free energy in Equation~(\ref{eq:dPMFdKsi}).

Electrostatic, van der Waals and two-atom chemical bond interactions can be readily transformed into weights in the graph.
For example, electrostatic interactions between atoms $\alpha$ and $\beta$ lead to the following contribution:
\begin{equation}
c_{\alpha\beta}^{ele}(\xi^*):= \frac{1}{m_\alpha} \left\langle m_\xi \frac{\partial U^{ele}}{\partial \mathbf{x}_\alpha}\cdot\frac{\partial\xi}{\partial\mathbf{x}_\alpha} \right\rangle_{\xi^*}  + \frac{1}{m_\beta} \left\langle m_\xi \frac{\partial U^{ele}}{\partial \mathbf{x}_\beta}\cdot\frac{\partial\xi}{\partial\mathbf{x}_\beta} \right \rangle_{\xi^*} .
\end{equation}
Alas, $n$-body potential energy components cannot, in general, be decomposed into a sum of pair interactions.
However, because the free energy contributions are of the form $\nabla U\cdot\mathbb{M}^{-1}\nabla\xi$, such decomposition is not required.

To clarify, let us consider a 3-body potential energy component, e.g. $U^{angl}(\mathbf{x}_\alpha,\mathbf{x}_\beta,\mathbf{x}_\gamma)$.
The weight $c^{angl}_{\alpha\beta}$ of an edge joining nodes $\alpha$ and $\beta$  represents the contribution coming from atoms $\alpha$ and $\beta$ which interact \emph{via} $U^{angl}$, while the $\gamma$ atom is kept at a fixed position.
By asserting that the sum of pair-wise contributions is equal to the total contribution arising from $U^{angl}(\mathbf{x}_\alpha,\mathbf{x}_\beta,\mathbf{x}_\gamma)$, i.e.:
$$\sum_{i>j}c^{angl}_{ij}(\xi^*)=\sum_{i=\alpha,\beta,\gamma}\frac{1}{m_i} \left\langle m_\xi \frac{\partial U^{angl}}{\partial \mathbf{x}_i}\cdot\frac{\partial\xi}{\partial\mathbf{x}_i} \right\rangle_{\xi^*},$$
we arrive at the following definition of the cooperation term:
\begin{align}
\nonumber c_{\alpha\beta}^{angl}(\xi^*):=&\frac{1}{2}\sum_{i=\alpha,\beta}\frac{1}{m_i} \left\langle m_\xi \frac{\partial U^{angl}}{\partial \mathbf{x}_i}\cdot\frac{\partial\xi}{\partial\mathbf{x}_i} \right\rangle_{\xi^*},
\end{align}
where the $1/2$ means that the overall \emph{angl} contribution is evenly distributed between the cooperation terms: $c_{\alpha\beta}^{angl}$, $c_{\beta\gamma}^{angl}$ and $c_{\alpha\gamma}^{angl}$.

For the general case of an $n$-body energy component, $U^I$, we propose the following definition of the cooperation term between atoms $\alpha$ and $\beta$:
\begin{align}
\nonumber c_{\alpha\beta}^{I}(\xi^*):=&\frac{1}{n-1}\sum_{i=\alpha,\beta}\frac{1}{m_i} \left\langle m_\xi \frac{\partial U^{I}}{\partial \mathbf{x}_i}\cdot\frac{\partial\xi}{\partial\mathbf{x}_i} \right\rangle_{\xi^*}.
\end{align}
Note that we propose to distribute the cooperation evenly among all pairs of atoms involved in the interaction $I$.
With this definition we constructed graphs for all interaction types, $I$.

A matrix, $\mathcal{C}^{I}(\xi^*)$, such that $[\mathcal{C}^I(\xi^*)]_{\alpha\beta}:=c^I_{\alpha\beta}(\xi^*)$ constructed for a particular value of the collective variable $\xi^*$ and the interaction type $I$ is referred to as the \emph{transient cooperation matrix for} $I$ {\color{black}(see Figure~\ref{fig:numberedMatrix})}.
For convenience, the transient cooperation matrix for the \emph{total} interaction type does not contain any superscipt: $$\mathcal{C}(\xi_i):=\sum_I\mathcal{C}^I(\xi_i).$$
Note that the sum $\sum_{\alpha>\beta}[\mathcal{C}(\xi_i)]_{\alpha\beta}$, is the overall energetic contribution to $A'(\xi_i)$ in Equation~(\ref{eq:dPMFdKsi}).

\begin{figure}[h!]
\centering
\includegraphics{F1III}
\caption{
{\bf Graphical representation of a transient cooperation matrix for \emph{nbd} interactions.}
Warm colors denote positive contributions, whereas blue~--~negative.
White squares correspond to {\color{black}nil values}.
}
\label{fig:numberedMatrix}
\end{figure}

\subsubsection{Comment on the entropic contribution} 
The second term on the right-hand side of Equation~(\ref{eq:dPMFdKsi}) does not explicitly depend on $U$.
It is possible to propose pair-wise contributions of this quantity in order to construct matrices which in turn could be used as input for the genetic clustering procedure described below.
{\color{black}However, calculating these entropic contributions would require computation of the Hessian of the collective variable (the matrix of second-order derivatives with respect to atom coordinates) in each step of the simulation, which would make the computational cost of the procedure prohibitively high.}
{\color{black}We speculate on a possible solution of this problem in the \emph{Conclusions} section.}

\subsection{Clustering}
Data clustering is the procedure of finding disjoint subsets (clusters) of objects that share a high affinity, and are dissimilar to objects from other clusters.
To carry out a clustering procedure we need a pair-wise affinity measure; thus, the data set is often represented by a weighted graph.
Such {\color{black}a} graph is encoded by an \emph{affinity matrix} with elements corresponding to weights between nodes.
In this study we were looking for groups of atoms whose cooperation decreased (increased) the free energy, thus pushing (pulling) the whole system forward (backward) along a reaction coordinate.
Thus, we took the pair-wise energetic cooperation between atoms introduced earlier for the affinity measure. 

We will refer to the clusters identified in a transient cooperation matrix as \emph{transient molecular cogs}.
The free-energy-reducing group is referred to as \emph{forward cogs}, and the second group (which, conversely, increases the free energy gap) as \emph{reverse cogs}.
We, therefore, assume that a step taken by the molecule along a reaction coordinate is a consequence of a resultant tendency generated by the molecular cogs.
But cross-cooperation between atoms from different groups can also occur, and we {\color{black}designate} this unwanted effect 'gear grinding' (see the \emph{Results} section).

\subsubsection{Affinity Propagation algorithm}
A considerable challenge associated with clustering is that different procedures produce different results.
Therefore, it was crucial that the \emph{right} clustering algorithm was chosen.
We anticipated that the optimal algorithm should recover molecular cogs with as little gear grinding as possible, but we could not estimate how large this effect might be.
To gain some insight into how molecular cogs might look like, we chose a clustering algorithm which is known to be successful in other applications.

Most clustering algorithms assume a non-negative affinity measure, whereas cooperation between atoms can be negative as well as positive.
Among those algorithms which accepted negative values, Affinity Propagation (AP) was the procedure found to be the most promising\cite{frey2007clustering}.
The AP algorithm searches for \emph{exemplars}, i.e. nodes within the graph around which non-exemplars are grouped, thus forming a cluster.
It is an iterative procedure, where, in each step, ``messages'' between nodes are exchanged to designate exemplars and their followers.

In our case, the sign of cooperation is arbitrary i.e. it depends on the direction of the reaction coordinate, $\xi$.
It was crucial to ensure that molecular cogs found for a reaction coordinate $\xi':=-\xi$ were the same as those found for $\xi$.
We resolved this problem by carrying out two clusterings with the AP method: one for $\mathcal{C}(\xi)$, and the second one for $-\mathcal{C}(\xi)$, in which the sign of all elements is changed, such as produced for~$\xi'$.
Description of the AP algorithm and our method for merging two clusterings can be found in \emph{Supporting Information}.

One of the drawbacks of the AP method, similarly as in other clustering procedures, is that it requires several parameters, which influence the outcome.
It appeared that small variations in parameters lead to markedly different results, and it was difficult to find a single set of parameters suitable for all cases (see the discussion in the \emph{Supporting Information}).
Nevertheless, the AP method gave us a first estimation of how molecular cogs look like, and it appears to be sufficiently good to initialize the genetic clustering procedure (see the \emph{Genetic clustering} section).

\subsubsection{Objective function}\label{sec:objective}
Molecular cogs identified by the AP algorithm were laden with low gear grinding, but that was not always a valuable finding. 
We noticed that in cases where no good partitioning was achievable, the AP method produced one-element clusters, which we considered to be artificial and incorrect. 

The AP algorithm performs a clustering which maximizes the so-called \emph{net similarity} \cite{frey2007clustering}.
This objective function, although helpful in many applications, does not carry any physical meaning.
Note that cooperation, which we used to express affinity between atoms, translates into free energy differences, and thus into the tendency of the whole system to move along a collective variable.
Molecular cogs should not only be laden with low gear grinding, but also cooperation generated by the cogs should cover the overall free energy change as {\color{black}much} as possible.

To clarify, given a cooperation matrix $\mathcal{C}$, the sum of all negative (positive) elements is the overall tendency of the system to go forward (backward) along a reaction coordinate.
It is desired for forward (reverse) cogs to cover as much of this overall tendency as possible.
When there is only one atom in a cluster, there is no coverage, even though gear grinding might be small.

Let us denote by FC the set of atom indices, which were assigned to the forward cogs, and analogously by RC the set of atoms assigned to the reverse cogs.
Given a cooperation matrix, $\mathcal{C}=[c_{ij}]$, the following three quantities are useful in measuring the quality of molecular cogs:
\begin{packeditemize}
 \item Forward Cogs Rate: \begin{equation} \text{FCR}:= \frac{\sum_{\substack{i,j\in\text{FC}}}c_{ij}}{\sum_{c_{ij}<0}c_{ij}} \end{equation}
 \item Reverse Cogs Rate: \begin{equation} \text{RCR}:= \frac{\sum_{\substack{i,j\in\text{RC}}}c_{ij}}{\sum_{c_{ij}>0}c_{ij}} \end{equation}
 \item Gear Grinding Rate: \begin{equation} \text{GGR}:=\frac{\sum_{\substack{i\in\text{FC}\\ j\in\text{RC}\\ c_{ij}>0}} c_{ij}}{\sum_{c_{ij}>0}c_{ij}} +  \frac{\sum_{\substack{i\in\text{FC}\\ j\in\text{RC}\\ c_{ij}<0}} c_{ij}}{\sum_{c_{ij}<0}c_{ij}} \end{equation}
\end{packeditemize}
We assumed that both positive and negative elements exist in the cooperation matrix, $\mathcal{C}$, i.e. that $\sum_{c_{ij}<0}c_{ij}\neq0$ and $\sum_{c_{ij}>0}c_{ij}\neq0$.
In cases in which the denominator is null, we substitute the whole fraction by~0.

FCR is the ratio of the contribution captured by the forward cogs to the total forward propensity found in $\mathcal{C}$.
RCR is analogous, and both these measures have a maximum value of~1.
Magnitude of misplaced contributions between atoms from different clusters is indicated by GGR, which has a maximum value of~2.

We propose the following molecular cogs quality measure:
\begin{equation}\label{eq:score}
\text{SCORE}(\text{FC},\text{RC}, \mathcal{C}):= 0.5(\text{FCR}+\text{RCR}-\text{GGR}),
\end{equation}
and SCORE $:=0$ for cases when any of the cogs consists of a single atom. 
The above scoring function was used in our genetic clustering procedure (described in the following section).
{\color{black}See Figure~\ref{fig:numberedMatrixClustered} for an example of a clustering which maximizes the SCORE.}

SCORE equals 1 if the corresponding molecular cogs cover the whole propensity of the system to move along a reaction coordinate, and no gear grinding occurs.
We observed that a SCORE less than 0.5 often indicates that the subdivision of the system into molecular cogs is noisy.

\begin{figure}[h!]
\centering
\includegraphics{F2III}
\caption{
{\bf Graphical representation of a result of a clustering procedure.}
Rows and columns of the cooperation matrix in Figure~\ref{fig:numberedMatrix} were reordered according to their assignments to: forward cogs, non-interacting atoms, and reverse cogs.	
In the upper left corner of this transformed matrix we have a block (square submatrix) with negative contributions, which come from the atoms $2, 3, 4$ and $6$ (from the forward cogs).
In the lower right corner we have a block formed by pair contributions of the atoms in the reverse cogs.
The gear grinding is small and results from contributions between the atom $7$ and the atoms ${2, 3, 4, 9}$.
}
\label{fig:numberedMatrixClustered}
\end{figure}

Note that a trivial partitioning in which all atoms are assigned to one group, e.g. FC, indicates that the system as a whole has a tendency to move forward.
We denote such trivial partitionings as ``all FC'' and ``all RC''.
The maximal value of SCORE is then 0.5, and although this is unfavorable, in many cases a trivial partitioning had the highest SCORE.
In such cases, there was no convenient clustering into two groups, which means that the gear grinding was high.

\subsubsection{Genetic clustering algorithm}\label{sec:genetic}
Genetic algorithms are widely used in optimization problems when potential solutions are readily assessed and codified.
They emulate the process of natural selection, promoting solutions~--~referred to as \emph{specimens}~--~with higher scores.
The codification of a solution is treated as its chromosome, which allows for \emph{mutation} of a solution, and \emph{crossover} with other chromosomes, thus ``spawning'' new specimens.
At the end of each iteration of a genetic algorithm, there is a stage called \emph{selection}, during which low-scoring solutions have a lesser chance of ``survival''.

We used the genetic clustering algorithm to find molecular cogs with the highest SCORE, as defined in Equation~(\ref{eq:score}) (see \cite{cole1998clustering} for a good introduction to genetic clustering).
The partitioning of a molecular system into molecular cogs was encoded by an array of numbers from the set \mbox{$\{-1,0,1\}$}, where the $i$th element of the array corresponded to the $i$th atom in a molecule.
Values $-1$ and $1$ translate into assignments to the forward and reverse cogs, respectively.
An atom not belonging to any cluster was tagged by 0; this occurred when the atom did not cooperate with any other atom {\color{black}(see Figure~\ref{fig:numberedStripe})}.

\begin{figure}[h!]
\centering
\includegraphics{F3III}
\caption{
{\bf Graphical representation of an exemplary partitioning: $\mathbf{\{1,-1,-1,-1,0,-1,1,1,-1,1,1 \}}$.}
Orange squares correspond to $1$ (reverse cogs), blue  to $-1$ (forward cogs), and white to $0$ (non-interacting).
The cooperation matrix in Figure~\ref{fig:numberedMatrix} was reordered according to this assignment, yielding the transformed cooperation matrix in Figure~\ref{fig:numberedMatrixClustered}.
}
\label{fig:numberedStripe}
\end{figure}

To initialize the genetic clustering procedure, the AP algorithm was {\color{black}executed by applying} five different methods of setting the diagonal elements in the affinity matrix (see \emph{Supporting Information}).
Next, it adds the ``all FC'' and ``all RC'' trivial solutions to the starting set.
Following this, solutions are iteratively chosen, mutated and added to the set, until the starting population contained 200 candidate solutions.

Once the initial set is generated, the genetic procedure repeated the following steps:
\begin{packedenum}
 \item Compose (randomly) 50 pairs of solutions to generate offspring using the crossover procedure.
 \item Select (randomly) 20 solutions to generate offspring using the mutation procedure.
 \item Calculate SCORE for the offspring and add it to the population.
 \item Draw (randomly) 200 solutions from the population and discard the rest.
 \item Choose the best scoring solution and check if there is any improvement in the SCORE. If there was none for 10 consecutive iterations, return the best solution. Otherwise, return to 1.
\end{packedenum}
All random selections are done without repeats, so that a given solution with SCORE $s$ is chosen with a probability proportional to $e^{2s}$.
{\color{black}Although} the number of parameters required in the genetic algorithm is daunting, changes to the majority of these parameters only influence the speed of arriving at the optimal solution (see the \emph{Supporting Information} for a more detailed discussion of the parameter's influence on the outcome).

Figure~\ref{fig:scoring} shows that the genetic clustering finds the optimal solution for a range of transient cooperation matrices for \emph{nbd} (complete results can be found in {\color{black}the \emph{Results} section}).
The optimal solution{\color{black}s were} found by means of a brute-force search, i.e. by producing all possible partitionings and calculations of their SCOREs.
It is worth noting that in all cases there was a singular solution with the highest SCORE.

\begin{figure}[h!]
 \centering
\includegraphics{F4III}
\caption{
{\bf SCORE plot for \emph{nbd} interactions.}
The plot shows that solutions found by the genetic clustering algorithm overlap perfectly with the highest scoring assignments to the transient molecular cogs.
}
\label{fig:scoring}
\end{figure}

\subsection{Trapezoidal rule for integrating $A$}
The free energy difference between $\xi_X$ and $\xi_Y$ can be expressed as an integral:
\begin{equation}\label{eq:integral}
 \Delta A = \int_{\xi_X}^{\xi_Y}A'(\xi^*)d\xi^*.
\end{equation}
The free energy derivative, $A'(\xi^*)$, can be estimated at a given $\xi^*$ from a cMD simulations \emph{via} Equation~(\ref{eq:dPMFdKsi}).
The integral in Equation~(\ref{eq:integral}) can then be calculated using the trapezoidal rule~\cite{press2007numerical}:
\begin{equation}\label{eq:deltaA}
\widehat{\Delta A} \approx \frac{\Delta\xi}{2}\left\{ \widehat{A'(\xi_1)} + 2\widehat{A'(\xi_{i+1})}+\ldots+2\widehat{A'(\xi_{M-1})}+\widehat{A'(\xi_M)} \right\},
\end{equation}
where $\widehat{A'(\xi_i)}$ denotes an estimate of the free energy derivative at $\xi_i$.
This requires independent cMD simulations at $M$ values of the collective variable, $\{ \xi_i\}_{i=1}^M$, with the grid size of $\Delta\xi$.

Each estimate $\widehat{A'(\xi_i)}$ has a corresponding variance, $\sigma^2[\widehat{A'(\xi_i)}]$, which in turn propagates to the variance of the integral estimate, i.e. $\sigma^2[\widehat{\Delta A}]$.
In the following section we explain how we estimated the variances $\sigma^2[\widehat{A'(\xi_i)}]$.
Because estimates $\widehat{A'(\xi_i)}$ are independent, the variance of the whole integral follows from Equation~(\ref{eq:deltaA}) straightforwardly:
\begin{equation}\label{eq:varDeltaA}
 \sigma^2[\widehat{\Delta A}] = \frac{\Delta\xi^2}{4}\left\{ \sigma^2[A'(\xi_1)] + 4\sigma^2[A'(\xi_2)]+\ldots+4\sigma^2[A'(\xi_{M-1})]+\sigma^2[A'(\xi_M)] \right\}
\end{equation}
{\color{black}Details concerning estimation of the variances $\sigma^2[A'(\xi_1)]$ using a bootstrapping procedure can be found in the \emph{Supporting Information}.}

The same integration rule applies to all elements of the transient cooperation matrices.
A matrix in which every element is a result of the above numerical integration is referred to as the \emph{integrated cooperation matrix} or simply: \emph{cooperation matrix}, and denoted by $\mathcal{C}(\xi_X\to\xi_Y)$.
The sum \mbox{$\sum_{\alpha>\beta}[\mathcal{C}(\xi_X\to\xi_Y)]_{\alpha\beta}$} is the energetic contribution to $\Delta A$ for the $\xi_X\to\xi_Y$ path.
Molecular cogs found for this matrix are called \emph{global molecular cogs}.

%------------------------------------------------

\section{Results}

{\color{black}In the first part of this section} we present {\color{black}detailed} results for a small, 11-atom molecular model, to validate the concept of our theoretical approach.
We attempted to indicate molecular cogs to verify whether the genetic algorithm finds the optimal clustering for the scoring function proposed in the \emph{Objective function} section.

{\color{black}
In the second part of the \emph{Results} we show molecular cogs for the \emph{nbd}, \emph{ele} and \emph{vdw} interactions for three other molecules.
These case-studies allowed for testing of the transferability of the parameters used in the genetic clustering algorithm, but also uncovered certain subtleties characteristic to our approach.
}

\subsection{The \texttt{[NH3+]CC(I)I} molecular model}\label{sec:toy}
We were interested in finding molecular cogs propelling {\color{black}a structural} transition between two meta-stable states, separated by a high free energy barrier.
{\color{black}For our first case-study we required} a system with a fairly natural collective variable, in which all types of interactions are significant (\emph{conf} as well as \emph{nbd}).
We used the {\color{black}2,2-diiodoethan-1-aminium} molecule, {\color{black}which in the SMILES format is encoded as:} \texttt{[NH3+]CC(I)I} {\color{black}(we use the SMILES representation throughout this article because of its conciseness)}.
The dihedral angle between atoms \texttt{N1-C5-C6-I7} {\color{black}was} our collective variable of choice~(see Figure~\ref{fig:toyModel}).
We used the Generalized Amber Force Field (GAFF) to model interactions between atoms, with partial charges assigned using an empirical procedure, AM1-BCC (Table~\ref{tab:partial}).

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1}
\sffamily
\begin{tabular}{c|Y||c|Y||c|Y||c|Y}
\cline{1-8}
\multicolumn{2}{c||}{{\bf \texttt{[NH3+]CC(I)I}}} & \multicolumn{2}{c||}{{\bf \texttt{NCC(I)I}}} & \multicolumn{2}{c||}{{\bf \texttt{CCC(I)I}}} & \multicolumn{2}{c}{{\bf \texttt{CClCC(I)I}}}  \\ 
\cline{1-8}
  \multirow{2}{*}{atom} & \multicolumn{1}{c||}{partial} & \multirow{2}{*}{atom} & \multicolumn{1}{c||}{partial} & \multirow{2}{*}{atom} & \multicolumn{1}{c||}{partial} & \multirow{2}{*}{atom} & \multicolumn{1}{c}{partial}  \\
   & \multicolumn{1}{c||}{charge} &  & \multicolumn{1}{c||}{charge} &  & \multicolumn{1}{c||}{charge} &  & \multicolumn{1}{c}{charge} \\
\cline{1-8}
N1 & \textsf{-0.85} 	& N1 & \textsf{-0.92} 	& C1 & \textsf{-0.10}  		& C1 & \textsf{0.03}	\\
H2 & \textsf{0.47} 	& H2 & \textsf{0.36} 	& H2 & \textsf{0.04} 		& Cl2 & \textsf{-0.19} 	\\
H3 & \textsf{0.47} 	& H3 & \textsf{0.36}	& H3 & 	\textsf{0.04}		& H3 & \textsf{0.08} 	 \\
H4 & \textsf{0.47} 	&   &  			& H4 & 	\textsf{0.04}		& H4 & \textsf{0.08}  	\\
\cline{1-8}
C5 & \textsf{0.13} 	& C4 & \textsf{0.17} 	& C5 & \textsf{-0.07} 		& C5 & \textsf{-0.08} 	\\
C6 & \textsf{0.08} 	& C5 & \textsf{0.20} 	& C6 & \textsf{0.21} 		& C6 & \textsf{0.20} 	\\
I7 & \textsf{-0.08} 	& I6 & \textsf{-0.19} 	& I7 & \textsf{-0.19} 		& I7 & \textsf{-0.19} 	\\
I8 & \textsf{-0.08} 	& I7 & \textsf{-0.19} 	& I8 & \textsf{-0.19} 		& I8 & \textsf{-0.19} 	\\
H9 & \textsf{0.13} 	& H8 & \textsf{0.05} 	& H9 & \textsf{0.06} 		& H9 & \textsf{0.08} 	\\
H10 & \textsf{0.13} 	& H9 & \textsf{0.05} 	& H10 & \textsf{0.06} 		& H10 & \textsf{0.08} 	\\
H11 & \textsf{0.13} 	& H10 & \textsf{0.12} 	& H11 & \textsf{0.10} 		& H11 & \textsf{0.12} 	\\
\end{tabular}
\normalfont
\caption{{\bf Partial charges assigned by the AM1-BCC procedure.}}
\label{tab:partial}
\end{table}

We focused on finding molecular cogs propelling the transition between the dihedral angles of $\xi_X:=-172.5\degree$ and $\xi_Y:=-62.5\degree$, which correspond to two PMF minima (Figure~\ref{fig:toyModel}).
The free energy barrier separating these minima is high, and interactions between the \texttt{[NH3+]} group and the iodine atoms provide strong \emph{ele} and \emph{vdw} contributions which influence this barrier.

Constrained MD simulations were carried out for fixed values of the $\xi$ collective variable, each simulation with $10^5$  $1\text{fs}$ timesteps, at $T=300$K.
We chose 239 points, $\{ \xi_i \}_{i=1}^{239}$, equally separated by $\Delta\xi=0.5\degree$, so that $\xi_1=-179.0\degree$ and $\xi_{239}=-60.0\degree$.
This was done in order to encompass the $[\xi_X,\xi_Y]$ interval; note that $\xi_{14}=\xi_X$ and $\xi_{234}=\xi_Y$.

The length of the block in the block bootstrap estimation of averages {\color{black}and their corresponding variances} was set to $10^2$, which {\color{black}led} to $10^3$ blocks for each simulation data set.
We chose the length of the block with the assumption that the auto-correlation after $10^2$ steps is negligible in the case of our simple system.

The cMD procedure was implemented in the Python programming language (ver. 2.7.2), using the Open Babel\cite{olboyle2011open} package (ver. 2.3.2) to model the molecule (see \emph{Supporting Information}).
{\color{black}All simulations for the \texttt{[NH3+]CC(I)I} model} took about 30h on a desktop computer.

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{F5III}
\caption{
{\bf \texttt{[NH3+]CC(I)I} molecule summary.}
Three configurations of the molecule are shown, and the PMF with energetic and entropic contributions (for $T=300$ K).
Atoms \texttt{\textbf{N1-C5-C6-I7}} (yellow) were used to define the dihedral angle, $\xi$.
}
\label{fig:toyModel}
\end{figure}

We focused on the free energy differences and energetic contributions for the $\xi_X\to\xi_Y$ path.
For these end-points we {\color{black}obtain} the free energy difference\break\mbox{$\Delta A\approx-16.4 \frac{\text{kcal}}{\text{mol}}$}, which is close to the energetic contribution, $\Delta E\approx-16.7\frac{\text{kcal}}{\text{mol}}$.
The residual $0.3\frac{\text{kcal}}{\text{mol}}$ is the entropic contribution, which was negligible for our model.

\subsection{Overview}
\label{sec:overview}
Results for the \emph{total}, \emph{conf} and \emph{nbd} interactions are juxtaposed in Table~\ref{tab:resultsI}, and for 2-body interactions (\emph{bond}, \emph{ele}, \emph{vdw}) in Table~\ref{tab:resultsII}.
{\color{black}
The first row contains optimal partitionings of the integrated cooperation matrices, i.e. global molecular cogs (see Figure~\ref{fig:numberedStripe} for explanation), along with a picture of the model colored according to the clustering.
In the second row we placed the integrated cooperation matrices rearranged in accordance with the clusterings depicted in the previous row of the table (see Figure~\ref{fig:numberedMatrixClustered} for explanation).
This representation visualizes the ``density'' of cooperativity within molecular cogs{\color{black},} and gear grinding.
}The {\color{black}third} row shows SCOREs for transient molecular cogs and compares genetic clusterings with optimal, brute-force partitionings.
{\color{black}In the next row we see an} illustration presenting transient molecular cogs, which is a concise summary of how cooperation within the molecule changes with $\xi$.
{\color{black}This plot is helpful in judging whether the global molecular cogs are similar to the transient molecular cogs, and in assessing the consistency of cooperation within the molecule.}
Finally, the last row contains a PMF-like contribution profile {\color{black}which we call the \emph{energetic contribution profile}}; this is the most important result of our analysis.
The green line represents the total energetic contribution of a given interaction type, orange and blue lines show contributions of the reverse and forward cogs{\color{black}, respectively}, and the purple line~--~{\color{black}the magnitude of} gear grinding.
Gear grinding is quantified as the sum of absolute values of all misplaced contributions, i.e. from pairs of atoms from different clusters.
Forward and reverse cogs contributions are calculated as the sum of cooperation between atoms assigned to FC and RC, respectively.

\subsection{Results for the {\color{black}\texttt{[NH3+]CC(I)I}} molecular model}
{\color{black}
In this section we comment on the results presented in Tables~\ref{tab:resultsI} and~\ref{tab:resultsII}.
We explain the Tables row by row, highlighting important aspects of the analysis, starting with Table~\ref{tab:resultsI} and then proceeding with Table~\ref{tab:resultsII}.
}

In the first row of Table~\ref{tab:resultsI}, global molecular cogs for \emph{total} and \emph{conf} interactions are trivial (``all FC''){\color{black}, with very low SCOREs.
Thus, }no interesting cooperation was detected for these interaction types~--~the whole system has a general propensity for preferring the state around $\xi_Y$.   
{\color{black} On the other hand, global molecular cogs for the \emph{nbd} interactions have a high SCORE.
The clustering suggests, in particular, that non-bonded interactions between the nitrogen atom and the iodine atoms hinder the transformation (as expected).
A more surprising implication was that the hydrogen atoms from the \texttt{[NH3+]} group cooperate with the \texttt{C6} atom.

Clustered cooperation matrices are placed in the second row of Table~\ref{tab:resultsI}.
For the \emph{total} and \emph{conf} interactions, for which the partitioning was trivial, the matrices kept their original form, whereas for the \emph{nbd} interactions rows and colums were rearranged in accordance with the partitioning.
Matrices for \emph{total} and \emph{conf} are almost indistinguishable, which suggests that the transition is mainly governed by the \emph{conf} interactions.
It is also clear that the cooperation matrix for \emph{nbd} is less ``dense'', with little gear grinding.

In the next row we see that the} partitionings into transient molecular cogs for \emph{nbd} have consistently higher SCOREs than those found for \emph{total} and \emph{conf}.
There is a drop in SCOREs corresponding to the crossing of the free energy {\color{black}maximum} at about $-115\degree$ dihedral angle.
SCOREs for \emph{nbd} molecular cogs vary significantly for different values of the collective variable{\color{black}, and the division into forward and reverse cogs becomes slightly more difficult after crossing the free energy maximum.

The transient molecular cogs depicted in graphs in the fourth row of the table show that} the transient molecular cogs for \emph{total} and \emph{conf} exhibit no interesting structure and, in most cases, are of the ``all FC'' or ``all RC'' type.
{\color{black}Transient molecular cogs for \emph{nbd} are fairly consistent with global molecular cogs.
Interestingly, the previously noted \emph{nbd} cooperation between hydrogen atoms \texttt{H2, H3, H4} and the \texttt{C6} atom persists throughout the transition.

In the last row of the table we placed the energetic contribution profiles.
Because the global molecular cogs for the \emph{total} and \emph{conf} interactions were of the ``all FC'' form, {\color{black}the only contributions come from the forward cogs.}
For the \emph{nbd} interactions, the }plot shows a beautiful separation of contributions coming from the forward ($-4.2\frac{\text{kcal}}{\text{mol}}$) and the reverse cogs ($3.7\frac{\text{kcal}}{\text{mol}}$) for non-bonded interactions, and low gear grinding for the $\xi_X\to\xi_Y$ transition.

Let us now look at the results in Table~\ref{tab:resultsII}.
{\color{black}In all three cases global molecular cogs were non-trivial, although the SCORE for \emph{bond} is significantly lower than for \emph{ele} and \emph{vdw}.
The partitioning for \emph{bond} suggests that there is an impediment arising from interactions of the \texttt{C6, I7} and \texttt{I8} atoms, whereas the rest of the system favors the state around $\xi_Y$.
Moving on to \emph{ele} and \emph{vdw} interactions we see that their global molecular cogs share a common pattern, although the SCORE for the latter is slightly lower.
We can also see that the cooperation of the \texttt{H2, H3, H4} and \texttt{C6} atoms (indicated earlier for \emph{nbd}) is caused by the \emph{ele} interactions.

The next row of Table~\ref{tab:resultsII} shows graphical representations of the clustered cooperation matrices.
Not surprisingly, the \emph{bond} matrix is more sparse than any other matrix, however judging by the low SCORE for global molecular cogs, this was not sufficient to ensure a clear division into the forward and reverse cogs.
Conversely, for \emph{ele} and \emph{vdw} there seems to be a higher degree of gear grinding, yet the SCOREs were higher.
This is due to the fact that the cooperation within molecular cogs is much stronger than gear grinding between them.

SCOREs in the third row of the table show that the transient molecular cogs for the \emph{bond} interactions were consistently low.
We see the opposite for \emph{ele}, and a completely different situation for the \emph{vdw} transient molecular cogs.
The case of the \emph{vdw} cooperation is particularly interesting because it shows that the global molecular cogs can have a high SCORE despite the fact that most of the transient molecular cogs have SCOREs below $0.5$.

In the fourth row of the table we see that the transient molecular cogs for \emph{bond} are consistent with their global molecular cogs for dihedral angles in the $[-150\degree,-100\degree]$ interval (i.e. around the free energy maximum at $\xi\approx-115\degree$)
For the \emph{ele} interactions we see a stable cooperation between the \texttt{H1, H2, H3} and \texttt{C6} atoms, occasionally aided by atoms: \texttt{H9} and \texttt{I8}.
The reason behind the shape of the \emph{vdw} SCORE plot becomes clearer once we see that the cooperation for this interaction type has two stages~--~before and after crossing the free energy maximum.
Transient molecular cogs for \emph{vdw} on the left side of the free energy maximum are mainly trivial (``all RC'') with a SCORE of about 0.5.
To the right of the maximum there is a change in cooperation; we see a steady partitioning into forward cogs composed of atoms: \texttt{N1, I7, I8, H9, H10} and \texttt{H11}, and reverse cogs (atoms: \texttt{H2, H3, H4} and \texttt{C6}).

In the last row of the table we see that} the \emph{bond} interactions lead to molecular cogs with high gear grinding, which is the cause of low SCOREs.
The profile shows that these interactions lower the free energy gap in the $\xi_X\to\xi_Y$ transition by $-9.9\frac{\text{kcal}}{\text{mol}}$.
However, we can {\color{black}also} see that {\color{black}the contribution from} atoms \texttt{C6, I7} and \texttt{I8} alone increases this gap by $5\frac{\text{kcal}}{\text{mol}}$.
{\color{black}For the \emph{ele} interactions} the separation into molecular cogs was clean (low gear grinding), with a $-4.3\frac{\text{kcal}}{\text{mol}}$ contribution from the forward cogs, and a $2.8\frac{\text{kcal}}{\text{mol}}$ contribution from the reverse cogs due to the transition.
{\color{black}For \emph{vdw}, the separation has also led to low gear grinding, however the overall contribution from the forward cogs is much smaller than the one from the reverse cogs.
This is an important observation which occurs again in the next section, where we analyze global molecular cogs determined for other model systems.
}

\begin{table}[H]
\centering
\includegraphics[width=\linewidth]{F6III}
\caption{{\bf Results summary for interactions: \emph{total}, \emph{conf} and \emph{nbd}.} 
Out of the triple: \emph{total}, \emph{conf} and \emph{nbd}, only the last one leads to a high SCORE partitioning, yielding non-trivial molecular cogs.
{\color{black}Colors: green, blue, orange and purple in the last row of the table denote contributions from the: whole system, forward cogs, reverse cogs and gear grinding, respectively (see the \emph{Overview} section for details).}
}
\label{tab:resultsI}
\end{table}

{
\centering
 \begin{table}[H]
\centering
\includegraphics[width=\linewidth]{F7III}
\caption{{\bf Results summary for interactions: \emph{bond}, \emph{ele} and \emph{vdw}.} 
All three 2-atom interaction types lead to decompositions into non-trivial molecular cogs.
{\color{black}However, the transient molecular cogs have high SCOREs only for the \emph{ele} interactions.
Colors: green, blue, orange and purple in the last row of the table denote contributions from the: whole system, forward cogs, reverse cogs and gear grinding, respectively (see the \emph{Overview} section for details).}
}
\label{tab:resultsII}
\end{table}
}

{\color{black}
\subsection{Results for related molecules}
To better understand our approach, it is valuable to identify molecular cogs for other systems, related to the \texttt{[NH3+]CC(I)I} molecule scrutinized above.
It was also valuable from the perspective of testing the transferability of the parameters used in the genetic clustering algorithm (as shown in the full results in the \emph{Supporting Information}).
We took into account molecules which instead of the \texttt{[NH3+]} group had the: \texttt{NH2}, \texttt{CH3} and \texttt{CH2Cl} groups respectively, i.e. molecules: \texttt{NCC(I)I}, \texttt{CCC(I)} and \texttt{CClCC(I)I}. 
Partial charges assigned by the AM1-BCC procedure for these molecules are listed in Table~\ref{tab:partial}. 
In this section we report and shortly discuss global molecular cogs for the \emph{nbd}, \emph{ele} and \emph{vdw} interactions (see Table~\ref{tab:molecularCogs}).

Because we analyzed closely related molecules, it was expected that molecular cogs for the \emph{nbd} interactions would be comparable.
All molecules presented here, except for \texttt{CClCC(I)I}, have partial charges of the same sign for corresponding atoms, therefore global molecular cogs for \emph{ele} look similar.
The more interesting outcome was related to discrepancies in molecular cogs identified for the \emph{vdw} interactions.

As noted earlier, in the case of the \texttt{[NH3+]CC(I)I} molecule the overall \emph{vdw} contribution is almost entirely explained by cooperation within the reverse cogs.
The forward cogs were identified merely because their contribution was non-positive.
Nevertheless, this was a valuable insight~--~we have learned that the \emph{vdw} steric effects are due to interactions of particular atoms: those which comprise the reverse cogs.
On the other hand, the \emph{vdw} molecular cogs for the \texttt{NCC(I)I} molecule are trivial (``all RC'') and carry no such information.
No non-trivial partitioning with a higher SCORE was found because there were no legitimate forward cogs, even as ineffective as those discovered in \texttt{[NH3+]CC(I)I}.
This then suggests that we gained a simplified picture of \emph{vdw} cooperation in the \texttt{[NH3+]CC(I)I} molecule simply because we were fortunate enough to have analyzed a system in which there had been at least a minimal contribution from the forward cogs.
This is a consequence of the underlying assumption that a molecule's tendency to undergo a transition is a result of two opposing cooperations.
Perhaps we should approach the problem differently for instances in which the molecule as a whole has the propensity to move forward/backward (as we also saw for the \emph{total} and \emph{conf} interactions).

Molecular cogs for \emph{vdw} interactions for the \texttt{CCC(I)I} molecule are similar to those of \texttt{[NH3+]CC(I)I}.
This might seem natural; the \texttt{[NH3+]} and \texttt{CH3} groups share common properties.
However, the energetic contribution profile for the \emph{vdw} interactions (see \emph{Supporting Information}) reveals that the forward cogs in \texttt{CCC(I)I} have a contribution comparable to that of the reverse cogs.
Although \emph{vdw} molecular cogs for \texttt{CCC(I)I} and \texttt{[NH3+]CC(I)I} are identical, the underlying mechanism is different.

The \texttt{CClCC(I)I} model was designed to lower the \emph{ele} barrier.
However, the resulting molecular cogs for \emph{ele} became trivial and, as in the case of \emph{vdw} for \texttt{NCC(I)I}, we do not know which atoms are the main source of this effect.
This again suggests that perhaps an alternative method of finding molecular cogs should be considered.
In cases of trivial partitionings, such method should extract information about parts of the molecule which are the main source of the free energy difference. 
However, we leave investigation of properties and characteristics of alternative scoring functions for future studies.

{
\centering
\begin{table}[H]
\centering
\includegraphics[width=\linewidth]{F8III}
\caption{{\bf Global molecular cogs for the following molecules: \texttt{[NH3+]CC(I)I}, \texttt{NCC(I)I}, \texttt{CCC(I)I} and \texttt{CClCC(I)I}.}
We focused our discussion on partitionings into forward and reverse cogs for the \emph{nbd}, \emph{ele} and \emph{vdw} interactions.
Full results are detailed in the \emph{Supporting Information}.
}
\label{tab:molecularCogs}
\end{table}
}
}

\section{Discussion}

Note that in all cases the genetic clustering algorithm gave clusterings with the best possible SCOREs.
However, the efficiency of the genetic algorithm depends on the starting point, and without the help of the AP-generated initial population it took, on average, about 30 times longer, and the best result was not always achieved.
To generate these initial solutions we adapted the AP clustering procedure (see \emph{Supporting Information}).

The reason why conformational interactions lead to low-quality molecular cogs is that these are short-ranged interactions and our test molecular system is small.
Partitioning of a graph into clusters is laden with a cost which depends on the weights of cross-cluster edges.
But, as can be seen in Table~\ref{tab:resultsI}, the integrated cooperation matrices for non-bonded interactions are more sparse than for conformational interactions.
Consequently, the cost of partitioning is greater for more ``dense'' matrices.
But this cost decreases with increasing dimensionality of a matrix. 
It is therefore possible that, for more complex systems, molecular cogs for conformational interactions will have a higher SCORE.

The non-bonded interactions in the GAFF force field for atoms separated by three {\color{black}bonds of fewer} are zero.
This, and the fact that our model {\color{black}system} is small, led to a sparse \emph{nbd} cooperation matrix.
Note, however, that a larger system may yield matrices that would be more ``dense'' for non-bonded interactions than for short-ranged conformational interactions.
Whether a partitioning of these matrices would result in higher gear grinding remains an unanswered question.

We also anticipate that the entropic contribution in Equation~(\ref{eq:dPMFdKsi}) should play a more notable role for larger systems.
Nonetheless, we have not yet considered clustering a graph with edges weighted by the entropic contributions of pairs of atoms.
The reasons are twofold: practical (we want to avoid calculating Hessians of collective variables) and conceptual (the entropic cooperation of an atom with itself is non-zero).
Entropic contributions need to be considered, but this should be the subject of future studies.

{\color{black}
In the second part of the \emph{Results} section we identified and analyzed molecular cogs for three additional molecules, related to the \texttt{[NH3+]CC(I)I} model.
We discovered that trivial molecular cogs, which carry less information than any other partitioning, may occur whenever there is a lack of competition of cooperation within the molecule.
This is a consequence of the proposed approach of dividing the system into two competing subsystems.
In many cases in which global molecular cogs were trivial, we simply did not gain any insight into what is propelling the transformation.
However, it seems that in such cases the analysis should be aimed at finding parts of a molecule that play the dominant role in its mobility.
}

{\color{black}
\subsubsection{Comparison with qualitative expectations}
Results for the \texttt{[NH3+]CC(I)I} model, especially for the \emph{ele} interactions, were useful in checking our intuitions against what was shown by the analysis.
In this section we shortly discuss several sanity checks related to results for the \emph{ele} interactions that were helpful in verifying whether our implementation had any critical errors, and whether the proposed approach leads to reasonable assessments.

From the plot depicting transient molecular cogs for \emph{ele} (Table~\ref{tab:resultsII}) we see that atoms \texttt{N1} and \texttt{I7} consistently hinder the transformation.
These two atoms have negative partial charges ($-0.85$ and $-0.08$, respectively; see Table~\ref{tab:partial}), and therefore repel each other throughout the transition.
But the \texttt{I8} atom (partial charge of $-0.08$) behaves differently, i.e. for $\xi$ lower than $-115\degree$ (free energy maximum) it impedes the process by repelling the \texttt{N1} atom, and aids it for $\xi$ larger than $-115\degree$.
This effect was correctly captured by the above analysis, because the \texttt{I8} atom finds itself in the reverse cogs in the first part of the transition, and in the forward cogs in the second part (as shown by the graph in the sixth row of Table~\ref{tab:resultsII}).
From the integrated contribution matrix for \emph{ele} (second row in Table~\ref{tab:resultsII}) we see, however, that the cumulative contribution from the \texttt{N1-I8} atoms is positive, which results in assigning them to one cluster.

The analysis revealed that atoms: \texttt{H2, H3, H4} and \texttt{C6} were consistently cooperating electrostatically, lowering the free energy barrier.
This conclusion was more unexpected than the one concerning atoms: \texttt{N1, I7} and \texttt{I8}, but was also compatible with our intuitions, taking into account the partial charges in Table~\ref{tab:partial}.

Our method can suggest a qualitative interpretation of the cause behind a structural transition.
However, it should be noted the most valuable information gained from this type of analysis is the quantitative description of the molecular cogs \emph{via} the energetic contribution profile.
}

\section{Conclusions}

{\color{black}
The aim of this article was to introduce a new methodology of identifying molecular cogs~--~parts of a molecule that propel structural transitions in forward/backward directions along a collective variable.
The current framework allowed us to track energetic contributions to the free energy, leaving the problem of including entropic terms for future developments.
Results show that with the use of the genetic clustering algorithm we can successfully divide small molecules and identify forward and reverse molecular cogs associated with non-bonded interactions.}

In particular, we proposed the approach of defining free energy contributions originating from pairs of atoms, and a method of dividing a molecule into molecular cogs.
We showed that the proposed genetic clustering algorithm efficiently finds the optimal cogs, leading to high-quality partitionings for non-bonded interactions.
However, we also found that conformational interactions lead to low-scoring molecular cogs, and that the system as a whole favored one meta-stable state over the other.

Currently, our method is based on {\color{black}cMD} simulations for computing conditional averages (Equation~(\ref{eq:dPMFdKsi})).
Unfortunately, this solution has a critical drawback (see \emph{Supporting Information}), but also, in order to determine the entropic contributions, requires computing second-order derivatives of the collective variable.
To resolve this problem we should facilitate the Adaptive Biasing Force (ABF) scheme for calculating the PMF~\cite{comer2014adaptive}.
{\color{black}Specifically, our future work is aimed at reformulating the procedure as a plugin to the NAMD package (in which the ABF has already been implemented), to include entropic contributions and to assure scalable performance.
Once this is done, numerous new opportunities will become available, some of which we mention below.}

We constructed graphs in which nodes corresponded to atoms.
Note, however, that they may also be assigned to amino acids (or more sizable objects) to construct graphs, which would lead to a more mosaic clustering.
It could also be valuable to represent a whole ligand as a single object in a protein-ligand complex.
One could also consider the role of functional groups comprising a ligand and amino acids in a binding pocket; it might then be helpful to represent the rest of the protein as a single node in the graph.
{\color{black}Another example is a possible treatment of solvent molecules, for example: a particular group of interesting water molecules could be transformed into separate nodes in the graph, while others reduced to a single node.}

Our method may prove helpful in understanding why wild-type proteins perform better than mutants, or in explaining why certain drugs perform better than others, despite their structural similarity.
It would also be interesting to consider a multi-stage induced fit docking, and to study cooperativity between amino acids and the ligand along a collective variable.

This research study demonstrates how to extract pair-wise energetic contributions to the free energy change along a reaction coordinate.
In its present form\footnote{We made the source code available at: \texttt{http://github.com/ponadto/molecular-cogs}}, we could only use our method to analyze small, {\color{black}model} molecules.
However, {\color{black}we were able} to verify the efficacy of the genetic clustering algorithm, and to learn what can be expected from the new notion of "molecular cogs".
Our future aim is to analyze more complex systems, and to develop a publicly available implementation of our method for practical applications.


%\printbibliography[segment=2,heading=subbibliography]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Conclusions}

In this closing chapter we re-iterate the most important findings of the methods presented in Chapters~2 and~3, and propose several refinements to them.
Both studies exemplify projects in which physical knowledge works side by side with the unsupervised machine learning approaches.
Physical insight played a predominant role in choosing informative similarity measures used in both applications.
And although unsupervised machine learning is becoming more and more powerful, without a reliable similarity measure, even the best clustering algorithms are bound to fail.

\section{Dynamic domains}

ResiCon was built upon several ideas, some of which were essential for its high-quality results.
The first one made use of the concept of a \emph{contact} between residues (that had been proven to be advantageous in an earlier application) to define the \emph{geometrical variability}.
Thereupon, the second idea was to propose a specific \emph{spectral clustering} algorithm, whose output were clusters corresponding to dynamic domains of a given protein structure.
In its current implementation, ResiCon requires information about all pairs of configurations to calculate the geometrical variability. 

The similarity measure derived from geometrical variability was intended to express not only structural shifts, but also (inherently) the strength of physical interactions between pairs of amino-acids.
This similarity measure was indeed effective (as it lead to compact, high-quality dynamic domains), but as we discussed in Chapter~2, its interpretability proved to be especially useful.
As we investigated peculiar artifacts in the results of the spectral clustering algorithm, we encountered discontinuities in the clusters.
Because of the similarity measure's interpretability, these ``artifacts'' lead to valuable observations about the mechanics of structural transitions of the HIV-1 protease.

The spectral clustering method offered a criterion for selecting an optimal number of clusters.
One should note that popular methods, GeoStaS and PiSQRD, were vulnerable to the slightest changes in parameter values, often yielding absurd clusters, whereas ResiCon was able to discover relevant, rigid domains, or a lack of such.


\subsubsection{Potential improvements}

To analyze MD simulations, ResiCon made use of the \emph{principal components analysis} (PCA) to select representative configurations. % to reduce the computational complexity of the algorithm constructing the similarity matrix.
Applying PCA was fully justified~--~otherwise each pairs of configurations in the trajectory would have to be tracked for the similarity matrix to be constructed.
%The number of configurations in \emph{any} simulation is too great to allow such a straightforward approach to be feasible.
However, the PCA relies on linearity assumptions, which in case of structural transitions of biomolecular systems may seem overly optimistic.
It is possible to amend this obstacle by employing a non-linear algorithm (e.g., NLPCA, which stands for \emph{non-linear PCA}), but at prohibitively high computational cost.

%In order to truly adapt ResiCon to the MD trajectory set-up, one needs to amend the problem of analyzing each pair of configurations.
In practice, only a few pairs contribute (or, more precisely: \emph{could} have contributed) to the similarity matrix.
It would certainly be worthwhile to test a sub-procedure that might substitute PCA by finding relevant pairs of configurations.
In particular, the \emph{locality-sensitive hashing} (LSH)~\cite{gionis1999similarity} algorithm offers this kind of functionality.

\section{Molecular cogs}

In Chapter~3 we presented a novel methodology for extracting insight about internal mechanics of small molecules undergoing structural transions.
This was intended as a ``proof of concept'', with a stronger focus on discovering technical problems and potential pitfalls, rather than on scrutinizing the toy models presented in Chapter~3.
We have learned that the similarity measure used for identifying molecular cogs leads to reasonable results, and often highlights unexpected properties of the system at hand.
But~--~perhaps more importantly~--~the similarity measure has a clear interpretation in terms of free energy change associated with a structural transition.

Thus, we got a meaningful similarity measure, for which we needed an adequate clustering method.
For that purpose, we proposed a custom objective function, for which we found optimal solutions using a genetic clustering algorithm.

\subsubsection{Potential improvements}

The fundamental assumption of the molecular cogs methodology presented in Chapter~3 was that there are two, \emph{distinct} groups of atoms, cooperating to propel the system forwards (or backwards) along a given collective variable.
However, we concluded that the clustering procedure has its limitations in cases in which the molecular system \emph{as a whole} has a general propensity to move in a certain direction.
In such cases, the more valuable insight would be the knowledge about which group of atoms plays the dominant role in such an effect.

Instead of using a definitive approach of clustering, one might consider employing a more a general idea in which each element can be a member of a more than just one group.
\emph{Community detection} offers such functionality, but also allows for the identification of hierarchical structures.
With the help of a community detection algorithm, we could, for example, search for groups of atoms with different degrees of contribution to the system's tendency to make a transition.

From a different stand point, the method presented in Chapter~3 employed \emph{constrained molecular dynamics} (cMD) to estimate local changes in the Helmholtz free energy.
However, as we noticed in Chapter~3, the cMD has limitations, when trying to apply it to larger systems.
An alternative sampling technique~--~most notably: the \emph{adaptive biasing force} (ABF) method~--~could remedy this impediment, but at the cost of a more complicated and time-demanding implementation.
We left the ABF-driven improvement for a future molecular cogs identification scheme.

The ABF sub-procedure would also alleviate the problem of estimating entropic contributions to the molecular cogs.
Nevertheless, we are not yet certain how the entropic contribution matrix should be partitioned.
We need to re-evaluate the meaning of molecular cogs altogether, and decide on a strategy that would treat entropic and energetic contributions on an equal footing.

The molecular cogs setting in its current formulation suggests a more subtle analysis of the causality hidden in complex molecular transformations.
We noticed that by modeling the contributions to the PMF we are close to modeling the kinetics of a transition, as well.
That is, we can modify our approach to identify contributions of a given subsystem to the so-called \emph{reaction rate constant}.
For that, we would need to equip our analytical toolbox with yet another method called the \emph{Markov state models} (MSM)~\cite{sarich2010approximation}.
Proposed by Frank Noe, the MSM extracts kinetic properties of a system from the PMF.
It was proven to give reliable estimates of the reaction rate constant for small enzymes, and seems to be an interesting future extension to our molecular cogs methodology.
We might use MSM to discover the influence of a given atomic subset on the kinetics of a structural transition, as well as to identify those subsets whose impact is essential for a given biomolecule's functionality.

\section{Outcome}

This work presented the difficulties and benefits of adapting unsupervised machine learning to analyzing biophysical data.
The examples provided in Chapters~2 and~3 showed solutions to three main problems that are likely to be encountered in such investigations:
\begin{enumerate}
 \item Choice / definition of a similarity measure;
 \item Right clustering procedure and the optimal number of clusters;
 \item Verification of results.
\end{enumerate}

Our similarity measures were strongly based in physical properties of the analyzed systems.
This was crucial as it brought meaning to the resulting partitionings.
Next, we chose the appropriate clustering algorithms that worked well with the proposed similarity measures.
However, we would not be able to judge the quality of the clustering (and, thus, the clustering procedures) without an external method of verification of the results.
Therefore, these three aspects of an analysis involving unsupervised machine learning are interrelated and cannot be regarded separately.

In this work we have proven, that clustering can be a powerful tool for solving biophysical problems.
Chapter~2 describes our procedure for identifying \emph{dynamic domains}~--~quasi-rigid parts of proteins that undergo structural changes involving relative movements of these parts.
In Chapter~3, we presented the concept of \emph{molecular cogs}~--~an innovative approach to describing and discovering molecular subsystems that determine the propensity of a molecule to go through a conformational transition.
In both these cases, we were able to transform complex biophysical data into simple and meaningful summaries of properties and behaviors of molecular systems.
We hope that these numerical experiments will serve as guideposts to researchers willing to adapt machine learning in their areas of study.

\section{Final comments}

Complex data produced in biophysical experiments (both, \emph{in silico} and \emph{in vivo}) may often contain valuable information whose extraction demands efficient statistical methods.
Such information can contain unexpected elements, like for example the ``pivot point'' residues revealed by the analysis of the HIV-1 protease trajectory presented in Chapter 2.
Therefore, it is of paramount importance that such methods should convey easily interpretable, unambiguous, and visually appealing results.
These three conditions are indispensable for such method to fulfill its purpose.

\begin{appendices}
  
\chapter{Supplementary Materials for Chapter 2}

\section{Hierarchical clustering}
We show in the main article that the spectral clustering algorithm employed by ResiCon leads to high-quality results.
This algorithm has many advantages, one of which is a versatile internal indicator of clustering quality.
This is important because similarity matrices used as input for the clustering procedure differ in size and density.
In this paragraph we highlight the problem of choosing the right number of clusters in case of a standard agglomerative hierarchical clustering~--~UPGMA~\cite{sokal1958statistical}.

UPGMA starts out with all elements in separate groups.
The algorithm proceeds iteratively, at each stage joining the two nearest clusters, continuing until there is just a single cluster.
In the UPGMA the distance between two sets of points is the arithmetic average of distances between elements from one set and elements from the second.
As a result, UPGMA constructs a rooted tree (dendrogram) encoding clusterings into different numbers of clusters. %, with the height of the tree indicating the distance between groups.
By cutting the tree at a given height $h$ one can retrieve a clustering into groups separated by at least the distance equal to $h$.
Therefore, $h$ can be understood as a parameter determining the number of clusters.

To perform UPGMA clustering we needed a distance matrix, instead of a similarity matrix.
A similarity matrix $W$ was transformed into a distance matrix by taking $\mathbf{1}-W$, where $\mathbf{1}$ is a matrix with the same dimensions as $W$, whose elements are all equal to $1$.  

In Table~\ref{heights} we show how different values of the height parameter lead to different numbers of clusters for the examples used in the main article.
The values provided in the table are the minimal heights at which cutting the tree yielded a given number of clusters.
Bold font indicates the number of clusters found by ResiCon's spectral algorithm; we used the corresponding values of the height parameter to order the rows in the table.

\begin{table*}[h!]\footnotesize
\centering
\sffamily
\renewcommand{\arraystretch}{1}
\begin{tabular}{cc|c|c|c|c|c} 
%\cline{2-7}
 & \multicolumn{6}{ c }{{\bf \textsf{Number of clusters}}} \\
%\cline{2-7}
 & \multicolumn{1}{|c|}{\bf \textsf{1}} & {\bf \textsf{2}} & {\bf \textsf{3}} & {\bf \textsf{4}} & {\bf \textsf{5}} & {\bf \textsf{6}} \\ 
\hline
\multicolumn{1}{c|}{\texttt{2htg}} & {\bf \textsf{0.907}}  & 0.730  & 0.574 & 0.494 & 0.423 & 0.257 \\
%\hline
\multicolumn{1}{c|}{\texttt{1aey}} & {\bf \textsf{0.967}}  & 0.962  & 0.933 & 0.924 & 0.918 & 0.898 \\
%\hline
\multicolumn{1}{c|}{\texttt{3mef}} & {\bf \textsf{0.967}}  & 0.950  & 0.944 & 0.921 & 0.914 & 0.845 \\
%\hline
\multicolumn{1}{c|}{\texttt{1leb}} & {\bf \textsf{0.974}}  & 0.953  & 0.951 & 0.935 & 0.912 & 0.868 \\
%\hline
\multicolumn{1}{c|}{\texttt{1pkt}} & {\bf \textsf{0.975}}  & 0.958  & 0.946 & 0.938 & 0.931 & 0.930 \\
\hline
\multicolumn{1}{c|}{\texttt{1zda}} & {\bf \textsf{0.976}}  & 0.904  & 0.869 & 0.865 & 0.814 & 0.745 \\
%\hline
\multicolumn{1}{c|}{\texttt{1pit}} & {\bf \textsf{0.979}}  & 0.948  & 0.948 & 0.908 & 0.886 & 0.871 \\
%\hline
\multicolumn{1}{c|}{\texttt{1aiw}} & {\bf \textsf{0.983}}  & 0.959  & 0.952 & 0.912 & 0.898 & 0.829 \\
%\hline
\multicolumn{1}{c|}{\texttt{2rgf}} & {\bf \textsf{0.985}}  & 0.983  & 0.967 & 0.956 & 0.953 & 0.947 \\
%\hline
\multicolumn{1}{c|}{\texttt{2pas}} & {\bf \textsf{0.985}}  & 0.980  & 0.962 & 0.958 & 0.954 & 0.941 \\
\hline
\multicolumn{1}{c|}{\texttt{2ktf}} & {\bf \textsf{0.986}}  & 0.977  & 0.971 & 0.966 & 0.964 & 0.945 \\
%\hline
\multicolumn{1}{c|}{\texttt{2spz}} & {\bf \textsf{0.986}}  & 0.953  & 0.922 & 0.904 & 0.898 & 0.883 \\
%\hline
\multicolumn{1}{c|}{\texttt{3egf}} & {\bf \textsf{0.990}}  & 0.959  & 0.938 & 0.900 & 0.870 & 0.868 \\
%\hline
\multicolumn{1}{c|}{\texttt{2vil}} & {\bf \textsf{0.992}}  & 0.986  & 0.979 & 0.974 & 0.970 & 0.955 \\
%\hline
\multicolumn{1}{c|}{\texttt{2ait}} & {\bf \textsf{0.992}}  & 0.972  & 0.962 & 0.949 & 0.932 & 0.911 \\
\hline
\multicolumn{1}{c|}{\texttt{1a67}} & {\bf \textsf{0.993}}  & 0.986  & 0.969 & 0.965 & 0.948 & 0.948 \\
%\hline
\multicolumn{1}{c|}{\texttt{2pni}} & {\bf \textsf{0.994}}  & 0.991  & 0.970 & 0.953 & 0.947 & 0.933 \\
%\hline
\multicolumn{1}{c|}{\texttt{2l14}} & {\bf \textsf{0.997}}  & 0.996  & 0.995 & 0.992 & 0.972 & 0.969 \\
%\hline
\multicolumn{1}{c|}{\texttt{1yug}} & 0.971  & {\bf \textsf{0.917}}  & 0.916 & 0.883 & 0.880 & 0.861 \\
%\hline
\multicolumn{1}{c|}{\texttt{1vvd}} & 0.996  & {\bf \textsf{0.979}}  & 0.967 & 0.952 & 0.951 & 0.944 \\
\hline
\multicolumn{1}{c|}{\texttt{1vve}} & 0.995  & {\bf \textsf{0.983}}  & 0.973 & 0.954 & 0.952 & 0.942 \\
%\hline
\multicolumn{1}{c|}{\texttt{2k0e}} & 0.999  & {\bf \textsf{0.986}}  & 0.959 & 0.935 & 0.907 & 0.895 \\
%\hline
\multicolumn{1}{c|}{\texttt{4a5v}} & 0.994  & {\bf \textsf{0.986}}  & 0.976 & 0.975 & 0.973 & 0.965 \\
%\hline
\multicolumn{1}{c|}{\texttt{1cfc}} & 1.000  & {\bf \textsf{0.989}}  & 0.988 & 0.985 & 0.978 & 0.969 \\
%\hline
\multicolumn{1}{c|}{\texttt{2kr6}} & 1.000  & {\bf \textsf{0.997}}  & 0.994 & 0.991 & 0.986 & 0.973 \\
\hline
\multicolumn{1}{c|}{\texttt{1d1d}} & 1.000  & {\bf \textsf{0.998}}  & 0.997 & 0.992 & 0.986 & 0.974 \\
%\hline
\multicolumn{1}{c|}{\texttt{2k3c}} & 0.991  & 0.967  & {\bf \textsf{0.892}} & 0.882 & 0.837 & 0.760 \\
%\hline
\multicolumn{1}{c|}{\texttt{1adr}} & 0.997  & 0.972  & {\bf \textsf{0.937}} & 0.937 & 0.912 & 0.902 \\
%\hline
\multicolumn{1}{c|}{\texttt{1qo6}} & 0.999  & 0.970  & {\bf \textsf{0.964}} & 0.946 & 0.932 & 0.920 \\
%\hline
\multicolumn{1}{c|}{\texttt{1bf8}} & 0.999  & 0.994  & {\bf \textsf{0.981}} & 0.981 & 0.981 & 0.968 \\
\end{tabular}
\vspace{.5cm}
\normalfont
\caption{{\bf Heights in the UPGMA dendrogram leading to the provided number of clusters}.
The rows are sorted according to the bold values (indicating numbers of clusters as found by ResiCon).
}
\label{heights}
\end{table*}

Treating the number of clusters found by the spectral procedure as baseline, there is no value of the height parameter that could reproduce ResiCon's original results.
This is because the relation between the number of clusters and $h$ depends on the size of the distance matrix, and its density.
The reason why the parameter $h$ is so volatile is that hierarchical clustering algorithms require a definition of distance between groups (e.g. UPGMA uses the arithmetic average of distances between points, but other measures are also commonly used).
If we choose the distance matrix to be $\mathbf{1}-W$, then most of the edges between nodes will be equal to $1$.
This is the reason why the distance between clusters becomes closer to $1$ as their sizes increase.
In most cases, we are interested in a fairly small number of clusters (typically, $3$ at most), for which the discrepancies between corresponding $h$ values is high.
Without employing a measure of clustering quality we would not be able to straightforwardly choose the optimal number of clusters.

In order to use an agglomerative hierarchical clustering algorithm, it would have been necessary to either abandon the proposed definition of a contact matrix based on geometrical variability, or introduce a more complex transformation leading to the distance matrix.
We decided to use a less known, spectral clustering algorithm which overrides the problem of finding optimal numbers of clusters in graphs with different characteristics.
We also think that it is a more natural approach in the context of dynamic domains deduction based on a contact matrices.

\section{Quality analysis for PiSQRD}

In the main article we compare results produced by ResiCon, GeoStaS and PiSQRD.
We chose PiSQRD as a representative method for the physics-based approaches to dynamic domains identification.
Its premise is that the eigenvectors of the structural covariance matrix (Equation~\ref{covMat}), referred to as \emph{low-energy modes}, contain the information required for the identification of global, domain-like movements in a protein.

The structural covariance matrix is defined as:
\begin{equation}
 C_{ij}:=\langle \delta r_i, \delta r_j \rangle,
\label{covMat}
\end{equation}
where $\delta r_i$ is the displacement of the $i$th coordinate (assuming that a molecule comprising $N$ $C^\alpha$-atoms is described using $3N$ coordinates), and the angle brackets denote a canonical ensemble averaging.
A canonical ensemble average can be approximated, for example, using all-atom $NVT$ molecular dynamics (MD) simulations, i.e. by producing a sample of configurations according to the Boltzmann distribution.
It may also be approximated from a single configuration using a physical model, e.g. an elastic network approach, as is being done in PiSQRD.

The input protein models used in our study for method comparison and evaluation were NMR ensembles in the PDB format.
In general, these models are \emph{not} a sample from the canonical ensemble of configurations.
Additionally, in many cases the coordinates of a model in a PDB file are provided with respect to a reference frame unrelated to the protein's structure.
As a result, the average configuration in such cases may be non-physical (improbable due to extremely high energy).
Nevertheless, we took an effort of approximating the structural covariance matrix using the models acquired from an NMR experiment and used the eigenvectors extracted from it as input for PiSQRD.
The aim of this analysis was to show a potential user (willing to assume that the NMR structures are sufficient to estimate the structural covariance matrix) what can be expected from the results.

In order to produce a single assignment to dynamic domains from a set of structures, the PiSQRD server requires as input: a reference structure, 10 largest eigenvalues of the covariance matrix, and their corresponding eigenvectors.
We tested four methods of choosing the reference structure and estimating the structural covariance matrix:
\begin{enumerate}
 \item take the average configuration (arithmetic average of all coordinates) as the reference structure, and calculate the covariance matrix;
 \item take the average configuration as the reference structure, superimposing all models on it, and then calculate the covariance matrix;
 \item take the model whose cumulative RMSD to all other models is the smallest as the reference structure, superpose all models on it, and then calculate the covariance matrix;
 \item use the mRMSD algorithm (see below) to structurally align all models, take the average configuration as the reference structure, and then calculate the covariance matrix.
\end{enumerate}
In all cases superpositions were done with the Kabsch algorithm, using only the $C^\alpha$-atoms.
The displacements $\delta r_i$ needed for the calculations of the covariance matrices were calculated with respect to the reference structures.

The first method makes sense in case of a sample of structures produced in the course of a MD simulation, where the reference frame is set so that the momentum and angular momentum are both null.
The second method was assumed to work better in cases in which the reference frame with respect to which the coordinates are given in a PDB file is not related to the protein structure.
In such cases, the orientation and relative position of the models seems random.
The third method guarantees that the reference structure is physical, and that the displacements $\delta r_i$ are reasonable.
The fourth method is an improvement over the second method, in which we use the mRMSD algorithm to structurally align all models.

The mRMSD algorithm is an iterative procedure in which: 
\begin{enumerate}
 \item superimpose all models onto the average structure;
 \item compute the new average structure;
 \item calculate the RMSD of new average structure with respect to the old average structure;
 \item if the RMSD is greater than $0.01$ go back to point 1.
\end{enumerate}

\begin{figure}[h!]
 \includegraphics[width=\linewidth]{PiSQRDquality.pdf}
\caption{
{\bf Dynamic domains quality for ResiCon and PiSQRD.}
We tested the single dynamic domain assignment produced by PiSQRD for four methods of estimating the structural covariance matrix.
}
\label{boxPlot}
\end{figure}

For all four methods we extracted the eigenvectors of the covariance matrix and uploaded them along with the reference structures as input for the PiSQRD server.
Figure~\ref{boxPlot} summarizes the quality of dynamic domains found using PiSQRD, and ResiCon.
It is difficult to choose the best method of producing input for PiSQRD based solely on the dynamic domains quality.
We, therefore, decided to pick the third method as worth mentioning in the main article because, in our opinion, it is the most natural one, and because the reference structures are in all cases physically viable.





\section{Procedure for selecting representative configurations from a trajectory}
We needed to extract an ensemble of representative configurations of the HIV-1 protease from its MD trajectory encoded in a \texttt{DCD} file.
For this purpose we have developed a greedy algorithm.
Greedy approaches are fairly common whenever there is a need to extract a subset of representative examples from a larger set.
In this particular case, however, we wanted to identify exactly $m$ representatives at which spheres of radius $R$ would be centered, thus excluding neighbors of these representatives.
Therefore, we had to identify the appropriate radius which would cause the greedy search to yield the required number of samples.
We assumed that the number of samples is an ``almost monotonous'' function of the sphere radius, and decided to use a ``softened'' binary search method (see Algorithm~\ref{al1}).
This heuristic turned out to be effective for the trajectory being analyzed.

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwIn{
\\$X$~--~a set containing $N$ configurations (points in $\mathbb{R}^{3n}$, where $n$ is the number of atoms)  
\\$p$~--~number of principal components
\\$m$~--~target number of representative configurations ($m<N$) \\
}
\KwResult{ensemble of $m$ configurations} 
\Begin{
 \emph{PCs}$\hspace{0.07cm} \longleftarrow \textrm{PrincipalComponents}(X)$\;
 $X_p \longleftarrow \textrm{Projection}(X,\textrm{\emph{PCs}},p)$ \tcc{$X_p$ is a set of $N$ $p$-dimensional points centered around 0}
 $R_\textrm{min} \longleftarrow 0$ \;
 $R_\textrm{max} \longleftarrow \max\limits_{x\in X_p} \|x \|$ \;
 \emph{Conf}$\hspace{0.07cm} \longleftarrow\emptyset$
 \tcc{set of representative configurations}
 \While{$| \textrm{Conf}\hspace{0.075cm}|\neq m$}{
  $R \longleftarrow (R_\textrm{min}+R_\textrm{max})/2$\;
  \mbox{\emph{Conf}$ \longleftarrow \textrm{ChooseConfigurations}(R,X_p)$}\;
%  \tcc{explanation below}
  $k \longleftarrow |$\emph{Conf}$|$\;
  \If{$|\textrm{Conf}\hspace{0.075cm}|>m$}{
  $R_\textrm{min} \longleftarrow R_\textrm{min} + (R-R_\textrm{min})/4$\;
  }
  \If{$|\textrm{Conf}\hspace{0.075cm}|<m$}{
   $R_\textrm{max} \longleftarrow R_\textrm{max} - (R_\textrm{max}-R)/4$\;
  }
 }
}
\Return{\emph{ExtractConfigurations}$(X,Conf)$\;}
\caption{ExtractRepresentativeConfigurations}
\label{al1}

\end{algorithm}

Function ExtractRepresentativeConfigurations (Algorithm~\ref{al1}) carries out the aforementioned procedure in the following steps.
First, we reduce the dimensionality of the configurations in $X$ using Principal Component Analysis (PCA) with $p$ components, yielding a set of points $X_p$ centered around 0.
In the case of the HIV-1 protease we used $p=4$ retaining 90\% of variability.
Next, the ChooseConfigurations procedure (Algorithm~\ref{al2}) selects a set of representative points for a given minimal distance between representatives.
% To do so, the procedure covers subsequent points in $X_p$ with spheres of radius $R$.
% Points at which the spheres are centered are selected to be representatives of their neighbors (points within the sphere).
The ExtractRepresentativeConfigurations procedure iteratively narrows the upper and lower bounds on the sphere radius, until the number of representative points found by ChooseConfigurationsGreedily is equal to the desired ensemble size $m$.
We used a ``softened'' binary search for selecting the radius, which narrowed the search area exponentially by a factor of $1/4$. 

The ChooseConfigurations procedure is defined in Algorithm~\ref{al2}.
The procedure start off by initializing sets \emph{Rep} and \emph{Rem}, containing representative and remaining configurations respectively.
Next, the algorithm iteratively select a random point $x$ from the \emph{Rem} set, center a sphere of radius $R$ at that point and finds neighboring points within that sphere.
Element $x$ is then added to the \emph{Rep} set, and the set of neighboring points is subtracted from the \emph{Rem} set.
These steps are repeated until there are no more elements in \emph{Rem} set, and return \emph{Rep} as output.

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwIn{
\\$X_p$~--~set of $N$ $p$-dimensional points
\\$R$~--~radius of spheres built around representative points
}
\KwResult{set of representative points} 
\Begin{
 %\tcc{we initialize the procedure by choosing the first point as a representative} 
 \emph{Rep}$\hspace{0.07cm} \longleftarrow \emptyset$
  \tcc{set of representative points}
 \emph{Rem}$\hspace{0.07cm} \longleftarrow X_p$
  \tcc{set of remaining points}
 \While{$|\textrm{Rem}\hspace{0.07cm}|>0$}{
  $x \longleftarrow \textrm{any } y\in\hspace{0.075cm}$\emph{Rem}\;
  \mbox{\emph{Neighbors}$\hspace{0.07cm}\longleftarrow \{ y \in \textrm{\emph{Rem}}\hspace{0.05cm}: \hspace{0.05cm} \| x-y \| \leq R  \}$}\;
  \tcc{a sphere of radius $R$ is centered at point $x$}
  \mbox{\emph{Rem}$\hspace{0.07cm} \longleftarrow \textrm{\emph{Rem}} \setminus $\emph{Neighbors}}\;
  \mbox{\emph{Rep}$\hspace{0.07cm} \longleftarrow $\emph{Rep}$\hspace{0.07cm} \cup \{ x \}$\;}
 }
\Return{Rep}
 \caption{ChooseConfigurations}
}
\label{al2}

\end{algorithm}

We have implemented the above procedures in the R language, using \texttt{bio3d} package (v. 1.1-6).
The principal component analysis was carried out with the \texttt{pca.xyz} function (provided in the \texttt{bio3d} package), which by default centers and scales configurations.


\section{Complete set of results}

In order to explain our procedure of comparing results of methods tested we discuss as an example the \texttt{1d1d} protein molecule.
To quantify the agreement between dynamic domains assignments, we used the Variation of Information metric ($\mathcal{VI}$, see \emph{Methods}).
In the case of \texttt{1d1d} ResiCon and GeoStaS exhibit a relatively large value (1.47) of $\mathcal{VI}$ (see~Figure~\ref{1d1dResiConVsGeoStaS}), which means that they produced dissimilar partitionings.

\begin{figure}[h!]
\setlength{\fboxsep}{0.3pt}
\setlength{\fboxrule}{0.5pt}
\hspace{0.2cm}%
\begin{minipage}{\linewidth}
\fbox{
\begin{minipage}[c][8.5em][c]{0.03\linewidth}
\centering
\rotatebox{90}{\centering\textbf{\texttt{ 1d1d }}}
\end{minipage}
}
\begin{minipage}{0.9\linewidth}
\begin{minipage}{\linewidth}
\fbox{
\begin{minipage}{0.3\linewidth}
\centering
\vspace{0.1cm}
ResiCon
\vspace{0.1cm}
\end{minipage}
}
\fbox{
\begin{minipage}{0.3\linewidth}
\centering
\vspace{0.1cm}
GeoStaS
\vspace{0.1cm}
\end{minipage}
}
\fbox{
\begin{minipage}{0.3\linewidth}
\centering
\vspace{0.1cm}
$\mathcal{VI}=1.47$
\vspace{0.1cm}
\end{minipage}
}
\end{minipage}

\begin{minipage}{\linewidth}
\fbox{
\begin{minipage}{0.3\linewidth}
\centering
\includegraphics[width=\linewidth]{clustering1D1D2}
\end{minipage}
}
\fbox{
\begin{minipage}{0.3\linewidth}
\centering
\includegraphics[width=\linewidth]{clustering1D1D1}
\end{minipage}
}
\fbox{
\begin{minipage}[c][7.0em][c]{0.3\linewidth}
\includegraphics[width=\linewidth]{stripe1D1D2}
\includegraphics[width=\linewidth]{stripe1D1D1}
\end{minipage}
}
\end{minipage}
\end{minipage}
\end{minipage}
%\vspace{0.5cm}
\caption{Dynamic domains assignments by ResiCon and GeoStaS, and their agreement for \texttt{1d1d}.}
\label{1d1dResiConVsGeoStaS}
\end{figure}

Note that GeoStaS identified the C-terminus and a loop as separate dynamic domains because of their flexibility.
However, GeoStaS failed to identify the relative motion of the two large subunits of the protein.
ResiCon found dynamic domains according to that motion, thus dynamic domains are larger.

In the case of ResiCon and GeoStaS all structures in a PDB file are considered and a single clustering into dynamic domains is being produced.
However, PiSQRD analyzes only one (the first) model from several available in the NMR ensemble.
It might seem that by requiring only a single structure, PiSQRD has an advantage over methods like ResiCon or GeoStaS.
But this would be the case only if PiSQRD gave the same results, regardless of the configuration provided as input.
As we show below, dynamic domains assigned by PiSQRD vary significantly.

Because of a large number of models it was not possible to provide a graphical representation for each partitioning.
We therefore focused on values of the agreement measure $\mathcal{VI}$ between results produced by the three methods.
The histograms of $\mathcal{VI}$ for \texttt{1d1d} are presented in Figure~\ref{histogram}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{histogramCalosciowy}
\caption{Histogram of $\mathcal{VI}$ between ResiCon and PiSQRD (R. vs. P.), GeoStaS and PiSQRD (G. vs. P.), and PiSQRD with itself (P. vs. P.) for 1D1D.}
\label{histogram}
\end{center}
\end{figure}


\newlength{\bufferspace}
\setlength{\bufferspace}{3cm}


Results presented in the following tables are presented in the same order as in the main article, i.e. best-scoring results are first.
\begin{sidewaystable}[h!]
\sffamily
\begin{tabular*}{4cm}{cc|c|c|c|c|c|c|c|c|c|}
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{2rgf}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram2RGF}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe2RGF2} & 0 & 0.62 & 0.45 & 0.57 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2RGF2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2RGF1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2RGFP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2RGFP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe2RGF1} & 0.62 & 0 & 0.32 & 0.26 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe2RGF_P1} & 0.45 & 0.32 & 0 & 0.21 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe2RGF_P2} & 0.57 & 0.26 & 0.21 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{2pas}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram2PAS}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe2PAS2} & 0 & 1.94 & 2.39 & 2.45 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2PAS2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2PAS1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2PASP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2PASP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe2PAS1} & 1.94 & 0 & 2.00 & 2.08 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe2PAS_P1} & 2.39 & 2.00 & 0 & 1.68 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe2PAS_P2} & 2.45 & 2.08 & 1.68 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1aey}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1AEY}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1AEY2} & 0 & 0.00 & 1.90 & 2.10 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1AEY2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1AEY1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1AEYP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1AEYP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1AEY1} & 0.00 & 0 & 1.90 & 2.10 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1AEY_P1} & 1.90 & 1.90 & 0 & 1.29 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1AEY_P2} & 2.10 & 2.10 & 1.29 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\end{tabular*}
\normalfont
\end{sidewaystable}



\begin{sidewaystable}[h!]
\sffamily
\begin{tabular*}{4cm}{cc|c|c|c|c|c|c|c|c|c|}
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1pkt}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1PKT}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1PKT2} & 0 & 0.00 & 4.20 & 1.85 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1PKT2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1PKT1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1PKTP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1PKTP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1PKT1} & 0.00 & 0 & 4.20 & 1.85 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1PKT_P1} & 4.20 & 4.20 & 0 & 2.87 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1PKT_P2} & 1.85 & 1.85 & 2.87 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{4a5v}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram4A5V}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe4A5V2} & 0 & 1.00 & 1.05 & 1.59 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering4A5V2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering4A5V1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering4A5VP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering4A5VP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe4A5V1} & 1.00 & 0 & 1.53 & 2.07 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe4A5V_P1} & 1.05 & 1.53 & 0 & 1.00 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe4A5V_P2} & 1.59 & 2.07 & 1.00 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1pit}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1PIT}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1PIT2} & 0 & 1.59 & 2.14 & 2.06 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1PIT2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1PIT1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1PITP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1PITP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1PIT1} & 1.59 & 0 & 2.70 & 2.98 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1PIT_P1} & 2.14 & 2.70 & 0 & 2.20 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1PIT_P2} & 2.06 & 2.98 & 2.20 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\end{tabular*}
\normalfont
\end{sidewaystable}



\begin{sidewaystable}[h!]
\sffamily
\begin{tabular*}{4cm}{cc|c|c|c|c|c|c|c|c|c|}
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{2vil}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram2VIL}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe2VIL2} & 0 & 1.89 & 1.19 & 1.66 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2VIL2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2VIL1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2VILP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2VILP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe2VIL1} & 1.89 & 0 & 2.43 & 2.49 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe2VIL_P1} & 1.19 & 2.43 & 0 & 0.91 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe2VIL_P2} & 1.66 & 2.49 & 0.91 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1aiw}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1AIW}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1AIW2} & 0 & 1.64 & 4.24 & 1.46 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1AIW2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1AIW1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1AIWP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1AIWP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1AIW1} & 1.64 & 0 & 3.76 & 2.64 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1AIW_P1} & 4.24 & 3.76 & 0 & 3.24 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1AIW_P2} & 1.46 & 2.64 & 3.24 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{2ktf}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram2KTF}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe2KTF2} & 0 & 0.80 & 1.68 & 2.10 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2KTF2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2KTF1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2KTFP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2KTFP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe2KTF1} & 0.80 & 0 & 1.82 & 2.37 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe2KTF_P1} & 1.68 & 1.82 & 0 & 1.77 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe2KTF_P2} & 2.10 & 2.37 & 1.77 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\end{tabular*}
\normalfont
\end{sidewaystable}



\begin{sidewaystable}[h!]
\sffamily
\begin{tabular*}{4cm}{cc|c|c|c|c|c|c|c|c|c|}
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1vve}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1VVE}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1VVE2} & 0 & 1.05 & 0.00 & 3.27 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1VVE2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1VVE1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1VVEP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1VVEP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1VVE1} & 1.05 & 0 & 1.05 & 4.25 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1VVE_P1} & 0.00 & 1.05 & 0 & 3.27 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1VVE_P2} & 3.27 & 4.25 & 3.27 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{3mef}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram3MEF}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe3MEF2} & 0 & 0.32 & 1.13 & 0.61 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering3MEF2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering3MEF1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering3MEFP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering3MEFP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe3MEF1} & 0.32 & 0 & 1.02 & 0.66 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe3MEF_P1} & 1.13 & 1.02 & 0 & 0.72 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe3MEF_P2} & 0.61 & 0.66 & 0.72 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1vvd}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1VVD}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1VVD2} & 0 & 1.24 & 0.21 & 3.36 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1VVD2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1VVD1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1VVDP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1VVDP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1VVD1} & 1.24 & 0 & 1.26 & 3.74 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1VVD_P1} & 0.21 & 1.26 & 0 & 3.28 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1VVD_P2} & 3.36 & 3.74 & 3.28 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\end{tabular*}
\normalfont
\end{sidewaystable}



\begin{sidewaystable}[h!]
\sffamily
\begin{tabular*}{4cm}{cc|c|c|c|c|c|c|c|c|c|}
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{2spz}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram2SPZ}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe2SPZ2} & 0 & 0.13 & 1.42 & 1.94 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2SPZ2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2SPZ1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2SPZP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2SPZP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe2SPZ1} & 0.13 & 0 & 1.37 & 1.96 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe2SPZ_P1} & 1.42 & 1.37 & 0 & 2.05 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe2SPZ_P2} & 1.94 & 1.96 & 2.05 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1leb}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1LEB}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1LEB2} & 0 & 0.61 & 2.47 & 2.43 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1LEB2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1LEB1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1LEBP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1LEBP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1LEB1} & 0.61 & 0 & 2.60 & 2.61 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1LEB_P1} & 2.47 & 2.60 & 0 & 2.25 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1LEB_P2} & 2.43 & 2.61 & 2.25 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\multicolumn{2}{|c|}{{\bf \texttt{2ait}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram2AIT}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe2AIT2} & 0 & 0.71 & 2.16 & 1.97 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2AIT2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2AIT1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2AITP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2AITP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe2AIT1} & 0.71 & 0 & 2.07 & 2.22 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe2AIT_P1} & 2.16 & 2.07 & 0 & 2.09 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe2AIT_P2} & 1.97 & 2.22 & 2.09 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\end{tabular*}
\normalfont
\end{sidewaystable}



\begin{sidewaystable}[h!]
\sffamily
\begin{tabular*}{4cm}{cc|c|c|c|c|c|c|c|c|c|}
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{2k3c}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram2K3C}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe2K3C2} & 0 & 1.86 & 1.13 & 1.08 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2K3C2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2K3C1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2K3CP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2K3CP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe2K3C1} & 1.86 & 0 & 1.35 & 1.87 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe2K3C_P1} & 1.13 & 1.35 & 0 & 1.52 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe2K3C_P2} & 1.08 & 1.87 & 1.52 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1cfc}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1CFC}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1CFC2} & 0 & 1.40 & 0.21 & 3.29 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1CFC2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1CFC1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1CFCP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1CFCP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1CFC1} & 1.40 & 0 & 1.48 & 3.56 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1CFC_P1} & 0.21 & 1.48 & 0 & 3.32 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1CFC_P2} & 3.29 & 3.56 & 3.32 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1a67}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1A67}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1A672} & 0 & 1.07 & 1.94 & 2.29 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1A672}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1A671}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1A67P1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1A67P2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1A671} & 1.07 & 0 & 2.22 & 2.69 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1A67_P1} & 1.94 & 2.22 & 0 & 2.59 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1A67_P2} & 2.29 & 2.69 & 2.59 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\end{tabular*}
\normalfont
\end{sidewaystable}



\begin{sidewaystable}[h!]
\sffamily
\begin{tabular*}{4cm}{cc|c|c|c|c|c|c|c|c|c|}
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{3egf}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram3EGF}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe3EGF2} & 0 & 0.14 & 1.70 & 1.17 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering3EGF2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering3EGF1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering3EGFP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering3EGFP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe3EGF1} & 0.14 & 0 & 1.70 & 1.14 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe3EGF_P1} & 1.70 & 1.70 & 0 & 1.70 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe3EGF_P2} & 1.17 & 1.14 & 1.70 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{2pni}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram2PNI}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe2PNI2} & 0 & 1.60 & 2.15 & 1.62 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2PNI2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2PNI1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2PNIP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2PNIP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe2PNI1} & 1.60 & 0 & 2.14 & 2.25 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe2PNI_P1} & 2.15 & 2.14 & 0 & 2.23 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe2PNI_P2} & 1.62 & 2.25 & 2.23 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1zda}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1ZDA}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1ZDA2} & 0 & 0.79 & 1.99 & 1.81 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1ZDA2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1ZDA1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1ZDAP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1ZDAP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1ZDA1} & 0.79 & 0 & 2.25 & 1.36 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1ZDA_P1} & 1.99 & 2.25 & 0 & 2.29 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1ZDA_P2} & 1.81 & 1.36 & 2.29 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\end{tabular*}
\normalfont
\end{sidewaystable}



\begin{sidewaystable}[h!]
\sffamily
\begin{tabular*}{4cm}{cc|c|c|c|c|c|c|c|c|c|}
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1adr}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1ADR}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1ADR2} & 0 & 0.74 & 0.82 & 2.27 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1ADR2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1ADR1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1ADRP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1ADRP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1ADR1} & 0.74 & 0 & 1.56 & 2.34 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1ADR_P1} & 0.82 & 1.56 & 0 & 2.57 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1ADR_P2} & 2.27 & 2.34 & 2.57 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1yug}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1YUG}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1YUG2} & 0 & 1.82 & 1.07 & 1.16 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1YUG2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1YUG1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1YUGP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1YUGP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1YUG1} & 1.82 & 0 & 2.56 & 1.57 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1YUG_P1} & 1.07 & 2.56 & 0 & 1.80 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1YUG_P2} & 1.16 & 1.57 & 1.80 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1d1d}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1D1D}}  \\  
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1D1D2} & 0 & 1.47 & 3.29 & 0.21 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1D1D2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1D1D1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1D1DP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1D1DP2}} &  \multirow{5}{*}{} \\ 
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1D1D1} & 1.47 & 0 & 3.76 & 1.51 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1D1D_P1} & 3.29 & 3.76 & 0 & 3.17 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1D1D_P2} & 0.21 & 1.51 & 3.17 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\end{tabular*}
\normalfont
\end{sidewaystable}



\begin{sidewaystable}[h!]
\sffamily
\begin{tabular*}{4cm}{cc|c|c|c|c|c|c|c|c|c|}
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{2l14}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram2L14}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe2L142} & 0 & 0.55 & 0.31 & 4.01 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2L142}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2L141}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2L14P1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2L14P2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe2L141} & 0.55 & 0 & 0.84 & 3.66 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe2L14_P1} & 0.31 & 0.84 & 0 & 3.74 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe2L14_P2} & 4.01 & 3.66 & 3.74 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1bf8}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1BF8}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1BF82} & 0 & 1.60 & 0.57 & 3.33 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1BF82}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1BF81}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1BF8P1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1BF8P2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1BF81} & 1.60 & 0 & 1.69 & 4.26 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1BF8_P1} & 0.57 & 1.69 & 0 & 3.33 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1BF8_P2} & 3.33 & 4.26 & 3.33 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{2htg}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram2HTG}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe2HTG2} & 0 & 0.00 & 1.56 & 2.29 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2HTG2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2HTG1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2HTGP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2HTGP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe2HTG1} & 0.00 & 0 & 1.56 & 2.29 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe2HTG_P1} & 1.56 & 1.56 & 0 & 1.68 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe2HTG_P2} & 2.29 & 2.29 & 1.68 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\end{tabular*}
\normalfont
\end{sidewaystable}



\begin{sidewaystable}[h!]
\sffamily
\begin{tabular*}{4cm}{cc|c|c|c|c|c|c|c|c|c|}
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{1qo6}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram1QO6}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe1QO62} & 0 & 0.96 & 3.07 & 0.61 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1QO62}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1QO61}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1QO6P1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering1QO6P2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe1QO61} & 0.96 & 0 & 3.09 & 0.62 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe1QO6_P1} & 3.07 & 3.09 & 0 & 3.12 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe1QO6_P2} & 0.61 & 0.62 & 3.12 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{2k0e}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram2K0E}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe2K0E2} & 0 & 0.97 & 0.50 & 3.29 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2K0E2}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2K0E1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2K0EP1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2K0EP2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe2K0E1} & 0.97 & 0 & 1.27 & 2.76 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe2K0E_P1} & 0.50 & 1.27 & 0 & 3.27 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe2K0E_P2} & 3.29 & 2.76 & 3.27 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\\
\cline{1-11}
\multicolumn{2}{|c|}{{\bf \texttt{2kr6}}} & R & G & P1 & P2 & ResiCon & GeoStaS & PiSQRD (P1) & PiSQRD (P2) & \multirow{5}{*}{\vspace{-0.15cm}\includegraphics[height=2.1cm]{threeHistogram2KR6}}  \\
\cline{1-10}
\multicolumn{1}{|c|}{R} & \includegraphics[width=3cm]{stripe2KR62} & 0 & 0.78 & 3.21 & 0.30 & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2KR62}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2KR61}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2KR6P1}} & \multirow{4}{*}{\vspace{-0.3cm}\includegraphics[height=1.82cm]{clustering2KR6P2}} &  \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{G} & \includegraphics[width=3cm]{stripe2KR61} & 0.78 & 0 & 3.10 & 0.89 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{} \\
\cline{1-6}
\multicolumn{1}{|c|}{P1} & \includegraphics[width=3cm]{stripe2KR6_P1} & 3.21 & 3.10 & 0 & 3.25 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-6}
\multicolumn{1}{|c|}{P2} & \includegraphics[width=3cm]{stripe2KR6_P2} & 0.30 & 0.89 & 3.25 & 0 & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{5}{*}{}  \\
\cline{1-11}
\end{tabular*}
\normalfont
\end{sidewaystable}




\chapter{Supplementary Materials for Chapter 3}
\section{Constrained molecular dynamics}\label{sec:appendixA}

In the blue moon ensemble method~\cite{carter1989constrained}, the conditional expected value of quantity $f$, given $\xi=\xi^*$ can be computed using the \emph{Fixman correction}:
\begin{equation}\label{eq:fixman}
 \langle f \rangle_{\xi^*} = \frac{\langle m_\xi^{1/2} f \rangle_{\xi^*\dot{\xi}}}{\langle m_\xi^{1/2} \rangle_{\xi^*\dot{\xi}}},
\end{equation}
where $\langle \cdot \rangle_{\xi^*\dot{\xi}}$ denotes an average at $\xi=\xi^*$, and $\dot{\xi}=0$. 
Also, one assumes that \mbox{$\nabla\xi\neq0$}.
This average is approximated \emph{via} a constrained Molecular Dynamics (cMD) simulation, i.e. by applying an additional force proportional to $\nabla\xi$ that holds the system at $\xi^*$.
Thus, the simulation is carried out using the modified Hamiltonian:
$$
\mathcal{H}_{\xi^*} := \mathcal{H} + \lambda (\xi^* - \xi),
$$
where $\lambda$ is chosen such that $\xi=\xi^*$ and $\dot{\xi}=0$:
$$
\lambda = m_\xi \left( \sum_i \frac{1}{m_i}\frac{\partial\xi}{\partial\mathbf{x}_i}\frac{\partial U}{\partial\mathbf{x}_i} - \mathbf{v}\cdot\mathbb{H}\mathbf{v} \right).
$$
The Andersen thermostat~\cite{den2013revisiting} for standard $NVT$ sampling was implemented in the Python programming language, and the Open Babel package (ver. 2.3.2) to model the molecule.

However, we observed that rotations around the \texttt{N1--C5} bond were much rarer than in a standard NVT simulation.
Consequently, the interactions of hydrogens from the \texttt{[NH3+]} with the rest of the molecule were inequivalent, and thus the clustering procedures assigned these hydrogens to different clusters.
Consequently, although the hydrogens from the \texttt{[NH3+]} group should have been equivalent, their interactions with the rest of the molecule were different.

To streamline the sampling, we added a multidimensional replica-exchange scheme~\cite{sugita2000multidimensional} to the Andersen thermostat.
We initiated each simulation with 3 models, with the dihedral angle \texttt{C6-C5-N1-H2} set to three different values (spanning the range of 360\degree evenly), all at the same temperature $T=300$ K.
The frequency of replica swapping was set to $1$ in every $10^3$ steps of the cMD simulation, for randomly chosen pairs of copies of the molecule.
If the swapping was accepted, the system was equilibrated for $10^2$ steps, keeping $\xi=\xi^*$ constant.

Sampling of the troublesome degree of freedom associated with rotations around the \texttt{N1--C5} bond required additional care, but was fairly simple.
However, as was noted by Darve in~\cite{tuckerman2007free}, sampling at $\xi=\xi^*$ using cMD may be, in general, impractical because energy barriers separating transitional pathways become virtually impassable.
Therefore, cMD simulations in a more general setting are likely to be prohibitively complicated.
This drawback is the main disadvantage of our current molecular cogs finding procedure.
We believe, however, that the Adaptive Biasing Force~\cite{tuckerman2007free} should help alleviate the limitations of cMD, and adapting this methodology to our approach should be the natural future approach in finding molecular cogs in more complex systems.

\section{Error estimation using a bootstrapping procedure}\label{sec:bootstrapping}
The conditional expected value of quantity $f$ given $\xi=\xi^*$, is estimated from a cMD simulation, {\color{black}as suggested by Equation~\ref{eq:fixman}.}
If the data points were uncorrelated, one could estimate $\langle f \rangle_{\xi^*}$ simply by taking a mean over the data.
Then, assuming a Gaussian distribution, one might approximate the error of this estimate by the standard deviation.
An alternative approach, called \emph{bootstrapping}, does not require any assumptions about how the data points are distributed.

Bootstrapping allows estimation of a given statistical quantity (e.g. average) along with its accuracy measure (e.g. variance).
Given a set of points \mbox{$S:=\{ x_i \}_{i=1}^K$}, new sets \mbox{$S_j:=\{ x_i^j \}_{i=1}^{K}$} are resampled from $S$ with replacement.
The basic idea of bootstrapping is to calculate a given statistical quantity independently for data sets $\{ S_j \}_{j=1}^K$, and, based on these results, extract the measure of accuracy. 
In its original form proposed by Efron et al.~\cite{efron1992bootstrap}, data points in set $S$ are assumed to be uncorrelated.
However, consecutive data points acquired from a cMD simulation are correlated.
We, therefore, applied a \emph{block bootstrap}, an extension of the original bootstrapping procedure, in which correlation within the data is allowed and accounted for~\cite{carlstein1986use}.
The block bootstrap divides the set $S$ into $b$ non-overlapping blocks\footnote{This is the \emph{Carlstein's blocking rule}; for a review of this and other methods see \cite{hall1995blocking,kreiss2011bootstrap}.} of length $k$, such that $bk=K$. 
The $i$th block, $B_i$, contains $b$ consecutive data points, such that $B_i=( x_{(i-1)k+1},\ldots,x_{ik} )$, for $1\leq i\leq b$.
Next, the procedure draws randomly $b$ blocks with replacement, combines them into a new data set and with its use calculates a given statistical quantity.
This procedure is repeated many times, yielding an array of estimates from which measures of accuracy are inferred.

In our case, the statistical quantity of interest was the average over sets of points acquired from a cMD simulation.
%In our case, means over sets of points acquired from a cMD simulation were calculated.
Using the block bootstrap we estimated averages and their corresponding variances.
The errors reported in the plots in the \emph{Results} section are the standard deviations of the estimated averages.

%It is worth emphasizing that with bootstrapping we only take into account statistical error, and DOKONCZ.

\section{Merging two clusterings from the AP method}\label{sec:appendixB}
Below we give a short description of the Affinity Propagation (AP) method and explain how we used this approach to produce an initial set of solutions for the genetic clustering algorithm.

Given an affinity matrix, $\mathcal{C}$, the AP algorithm constructs the \emph{availability}, $\mathcal{A}$, and \emph{responsibility}, $\mathcal{R}$, matrices (see Figure~\ref{fig:apExample}; a good description of the AP method can be found in~\cite{efron1992bootstrap}).
Then, for object $i$, the value of $k$ which maximizes $[\mathcal{A}]_{ik}+[\mathcal{R}]_{ik}$ indicates whether $i$ is an examplar (if $i=k$), or identifies the exemplar to which $i$ is assigned.
An object is chosen to be an exemplar if it is indeed the best representative of a group, or if it is dissimilar to all other objects.
In the latter case, such an exemplar is in fact an outcast, having a negative or null affinity to other objects.

\begin{figure}[h!]
\centering
INPUT ($\mathcal{C}$ and $-\mathcal{C}$ matrices):

\includegraphics[width=.49\linewidth]{apInput1}%
\includegraphics[width=.49\linewidth]{apInput2}

OUTPUT ($(\mathcal{A}+\mathcal{R})$ matrices):

\includegraphics[width=.49\linewidth]{aPlusR1}%
\includegraphics[width=.49\linewidth]{aPlusR2}
\caption{
{\bf Input and output of the AP method.}
Matrices $(\mathcal{A}+\mathcal{R})$ produced for affinity matrices $\mathcal{C}$ and $-\mathcal{C}$ contain the information required for a final clustering.
Positive elements in matrices are denoted with warm colors, whereas blue squares indicate negative values.
The AP algorithm found two exemplars for $\mathcal{C}$ (nodes 1 and 8), and two for $-\mathcal{C}$ (nodes 5 and 9).
These were the nodes for which there were positive values in the $(\mathcal{A}+\mathcal{R})$ matrices.
}
\label{fig:apExample}
\end{figure}

It is important to note, that the cooperation matrix (introduced in the main article) has zeros on the diagonal (an atom does not interact with itself), whereas in the AP method a value on the diagonal indicates the predisposition of an object to become an exemplar.
Frey and Dueck suggest setting the diagonal with a mean affinity of a given object to all other objects, or simply setting the whole diagonal with equal values~\cite{frey2007clustering}.

We needed to produce a single partitioning independent of the sign of the collective variable.
For this purpose, two sets of exemplars, $E_+$ and $E_-$, were inferred from $\mathcal{C}$ and $-\mathcal{C}$ affinity matrices.
Our aim was to merge these two sets of exemplars, and their followers, into one clustering, i.e. to distribute all objects between two sets, $P_+$ and $P_-$.
The $P_+$ group corresponds to the indices of atoms comprising the reverse cogs, and the $P_-$ to the forward cogs (denoted in the main article by RC and FC, respectively).

First, if an object was an exemplar with no followers in $E_-$, we dispatched it to $P_+$ (and analogously for $E_+$ and $P_-$), because singular exemplars were outcasts with no positive-weighted edges to any other nodes.
Non-singular exemplars in $E_-$ were dispatched to $P_-$ (analogously, in the other way around).
We canceled objects which had a null affinity to all other objects, therefore a situation in which an object is a singular exemplar in both partitionings did not occur.

Next, we looked at objects which were not chosen as exemplars in either of the AP runs.
These objects were initially assigned to two alternative groups, and to appoint them to $P_+$ or $P_-$, we checked their cumulative affinity to their suggested clusters.
If the total affinity of an object to its exemplar in $E_-$ (and followers) was greater than the alternative total affinity (associated with $E_+$), the object was added to the $P_-$ group (and \emph{vice versa}).
The result of this procedure was a rudimentary partitioning.

We observed that the diagonal-setting step had a huge impact on the clustering, and that a fairly good SCORE can be achieved by manipulating the diagonal.
Values of the diagonal elements can either be assigned based on values in their corresponding rows, or can be set uniformally based on the values in the whole matrix.
To generate a set of high-scoring initial partitionings for the genetic clustering algorithm, we carried out clusterings with the following list of methods to setting up the diagonal:
\begin{packeditemize}
 \item mean over affinities in a row;
 \item mean over non-zero affinities in a row;
 \item uniform: minimal positive affinity within the whole matrix;
 \item uniform: maximal positive affinity within the whole matrix;
 \item uniform: mean over positive affinities within the whole matrix.
\end{packeditemize}

Finally, we arrived at five affinity matrices, resulting in five clusterings, which were then mutated to create a set of solutions to initialize the genetic clustering procedure.

\section{Parameters of the genetic clustering algorithm}\label{sec:bootstrapping}

Within the framework of the AP algorithm, there were cases in which even the smallest variations in the values on the diagonal of the affinity matrix led to significantly different clusterings.
However, in most cases the obtained results had fairly high SCOREs. We could not determine whether this resulted from the properties of the graphs at hand, or simply from a drawback of the AP method.
We realized that we can use the AP procedure to generate sets of diverse, but fairly high-scoring results.
Thus, we decided to employ the genetic clustering algorithm, which is known to rely on the quality of the initial set of results.

The genetic clustering algorithm, aided by the initialization procedure based on the AP algorithm, returned the optimal partitionings for a wide range of its parameters.
When testing the efficacy of the genetic procedure, we scanned four parameters: the size of the population after each selection, the number of pair used for the crossover stage, the number of specimens used for the mutation stage, and the number of steps used in the stoping criterion.
The default values of these parameters were: 200, 50, 20, and 10, respectively (as detailed in the main article).
Keeping the ratio between these parameters fixed, a decrease of their values made the method faster, but there was a greater chance of arriving at a sub-optimal solution.
An increase of their values led to the optimal solution with an even higher probability, but at the cost of a longer running time.
The default values were chosen such that in all cases the genetic algorithm would yield the optimal partitionings.
  
\section{Complete results for the \texttt{CCC(I)I}, \texttt{NCC(I)I}, \texttt{CClCC(I)I} molecules}

Tables:~\ref{tab:0},~\ref{tab:1},~\ref{tab:2},~\ref{tab:3},~\ref{tab:4},~\ref{tab:5} collect results for the additional models discussed in the main article.
Note that in all cases the partitionings found by the genetic clustering algorithm were the optimal ones.

{
\centering
\begin{table}[h!]
\centering
\includegraphics[width=\linewidth]{tabular1_ii-ch3}
\caption{
{\bf Results summary for molecule \texttt{CCC(I)I}, for interactions: \emph{total}, \emph{conf} and \emph{nbd}.}
}
\label{tab:0}
\end{table}
}

{
\centering
\begin{table}
\centering
\includegraphics[width=\linewidth]{tabular2_ii-ch3}
\caption{
{\bf Results summary for molecule \texttt{CCC(I)I}, for interactions: \emph{bond}, \emph{ele} and \emph{vdw}.}
}
\label{tab:1}
\end{table}
}

{
\centering
\begin{table}
\centering
\includegraphics[width=\linewidth]{tabular1_ii-nh2}
\caption{
{\bf Results summary for molecule \texttt{NCC(I)I}, for interactions: \emph{total}, \emph{conf} and \emph{nbd}.}
}
\label{tab:2}
\end{table}
}

{
\centering
\begin{table}
\centering
\includegraphics[width=\linewidth]{tabular2_ii-nh2}
\caption{
{\bf Results summary for molecule \texttt{NCC(I)I}, for interactions: \emph{bond}, \emph{ele} and \emph{vdw}.}
}
\label{tab:3}
\end{table}
}

{
\centering
\begin{table}
\centering
\includegraphics[width=\linewidth]{tabular1_ii-ccl}
\caption{
{\bf Results summary for molecule \texttt{CClCC(I)I}, for interactions: \emph{total}, \emph{conf} and \emph{nbd}.}
}
\label{tab:4}
\end{table}
  }

{
\centering
\begin{table}
\centering
\includegraphics[width=\linewidth]{tabular2_ii-ccl}
\caption{
{\bf Results summary for molecule \texttt{CClCC(I)I}, for interactions: \emph{bond}, \emph{ele} and \emph{vdw}.}
}
\label{tab:5}
\end{table}
}


\end{appendices}


%\nocite{*}
% \inputencoding{latin2}
\printbibliography

\end{document}	